/usr/bin/python2.7 /home/dpetrovskyi/pycharm/pycharm-2016.3.2/helpers/pydev/pydevconsole.py 34267 57636
Python 2.7.6 (default, Jun 22 2015, 17:58:13)
Type "copyright", "credits" or "license" for more information.
IPython 5.3.0 -- An enhanced Interactive Python.
?         -> Introduction and overview of IPython's features.
%quickref -> Quick reference.
help      -> Python's own help system.
object?   -> Details about 'object', use 'object??' for extra details.
PyDev console: using IPython 5.3.0
import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/home/dpetrovskyi/PycharmProjects/quora', '/home/dpetrovskyi/PycharmProjects/grml', '/home/dpetrovskyi/PycharmProjects/cs224d', '/home/dpetrovskyi/PycharmProjects/kaggle', '/home/dpetrovskyi/PycharmProjects/fai', '/home/dpetrovskyi/PycharmProjects/courses', '/home/dpetrovskyi/PycharmProjects/grml/src'])
Python 2.7.6 (default, Jun 22 2015, 17:58:13)
[GCC 4.8.2] on linux2
from sklearn.metrics import log_loss
from preprocessing import *
import pandas as pd
import xgboost as xgb
import matplotlib.pyplot as plt
art_map = {'a':0, 'an':1, 'the':2}
inverse_art_map={0:'a', 1:'an', 2:'the'}
def is_indefinite_article(a):
    return a in {'a', 'an'}
def is_definite_article(a):
    return a == 'the'
def submit_train(df):
    train_arr = load_train_arr()
    corrections = load_resource(fp_corrections_train)
    submit_arr = create_submission_arr(df, train_arr)
    return evaluate(train_arr, corrections, submit_arr)
def evaluate(text, correct, submission):
    print len(text), len(correct), len(submission)
    # with open(text_file) as f:
    #     text = json.load(f)
    # with open(correct_file) as f:
    #     correct = json.load(f)
    # with open(submission_file) as f:
    #     submission = json.load(f)
    data = []
    for sent, cor, sub in izip_longest(text, correct, submission):
        for w, c, s in izip_longest(sent, cor, sub):
            if w in ['a', 'an', 'the']:
                tel = None if s is None else s[2]
                if s is None or s[0] == w:
                    s = ['', float('-inf')]
                # -score, ok-prediction, is_realy_error
                data.append((-s[1], s[0] == c, c is not None, tel))
                # -1, -0.8, -0.2, ... inf, inf
    print 0.02 * len(data)
    data.sort(key=lambda x: x[0])
    fp2 = 0
    fp = 0
    tp = 0
    all_mistakes = sum(x[2] for x in data)  # num of ALL incorrect
    print 'all_mistakes {}'.format(all_mistakes)
    score = 0
    acc = 0
    counter = 0
    stats = {'w': [], 'r': []}
    stop = False
    for _, c, r, tel in data:
        fp2 += not c  # wrong correction
        fp += not r  # realy errors count
        tp += c  # right correction
        if not stop:
            if not c:
                stats['w'].append(tel)
            if c:
                stats['r'].append(tel)
        acc = max(acc, 1 - (0. + fp + all_mistakes - tp) / len(data))
        if fp2 * 1. / len(data) <= 0.02:
            score = tp * 1. / all_mistakes
        else:
            stop = True
    print 'target score = %.2f %%' % (score * 100)
    print 'accuracy (just for info) = %.2f %%' % (acc * 100)
    return stats
def create_submission_arr(df, sents):
    arr = [[None] * (len(x)) for x in sents]
    cols = df.columns
    def do_work(row):
        if row[correction] is not None:
            corr = row[correction]
            conf = row[confidence]
            sent_index = row[sentence_index]
            pos = row[position]
            arr[sent_index][pos] = (corr, conf, OrderedDict([(c,row[c]) for c in cols]))
    df.apply(do_work, axis=1)
    return arr
def arr_to_df(arr):
    cols = arr[0].keys()
    m = OrderedDict((c, [None if x is None else x[c] for x in arr]) for c in cols)
    df= pd.DataFrame(m)
    df['trans'] = df[article]+'->'+df[correction].apply(str)#.apply(lambda s: inverse_art_map[s])
    # add_short_freq_cols(df)
    cols = ['trans', confidence]
    return df[cols]
def to_xgb_df(df):
    df[article]=df[article].apply(lambda s: art_map[s])
    df[correct_article]=df[correct_article].apply(lambda s: art_map[s])
    def normalized_ratio(row, col1, col2, prior):
        a = row[col1]
        b = row[col2]
        if a is None:
            return None
        return (float(prior) + a) / (float(prior) + b)
    def normalized_ratio1(row, col1, col2, prior):
        a = row[col1]
        b = row[col2]
        if a is None:
            return None
        return (float(prior) + a) / (2*float(prior) + b+a)
    def create_normalized_ratio_col(d, col1, col2, new_col, prior):
        d[new_col] = d.apply(lambda row: normalized_ratio(row, col1, col2, prior), axis=1)
    def create_normalized_ratio_col1(d, col1, col2, new_col, prior):
        d[new_col] = d.apply(lambda row: normalized_ratio1(row, col1, col2, prior), axis=1)
    col_pairs=[
        (a_bi_freq_suff, the_bi_freq_suff, 'bi_freq_suff'),
        (a_three_freq_suff, the_three_freq_suff, 'three_freq_suff'),
        (a_four_freq_suff, the_four_freq_suff, 'four_freq_suff'),
        (a_five_freq_suff, the_five_freq_suff, 'five_freq_suff'),
        (a_bi_freq_pref, the_bi_freq_pref, 'bi_freq_pref'),
        (a_three_freq_pref, the_three_freq_pref, 'three_freq_pref'),
        (a_four_freq_pref, the_four_freq_pref, 'four_freq_pref'),
        (a_five_freq_pref, the_five_freq_pref, 'five_freq_pref'),
        (a_sn_gram_freq, the_sn_gram_freq, 'sn_ngram_freq')
    ]
    frequencies_cols = [x[0] for x in col_pairs]+[x[1] for x in col_pairs]
    # frequencies_cols=[]
    for prior in [1, 10, 100]:#
        for a_col, the_col, name in col_pairs:
            new_col = 'a_the_{}_p{}'.format(name, prior)
            create_normalized_ratio_col(df, a_col, the_col, new_col, prior)
            frequencies_cols.append(new_col)
            new_col = 'the_a_{}_p{}'.format(name, prior)
            create_normalized_ratio_col(df, the_col, a_col, new_col, prior)
            frequencies_cols.append(new_col)
    # cols_to_exclude = [
    #     'the_bi_freq_pref', 'the_three_freq_pref',
    #     'the_four_freq_pref' ,'the_five_freq_pref',
    #     'a_bi_freq_pref', 'a_three_freq_pref',
    #     'a_four_freq_pref', 'a_five_freq_pref'
    # ]
    #
    # frequencies_cols = list(set(frequencies_cols).difference(set(cols_to_exclude)))
    cols = frequencies_cols + [st_with_v,article, correct_article]
    # df, new_cols = add_dummy_cols_df(df, raw_prev_token_POS, prev_token_tags)
    # cols+=new_cols
    #
    # df, new_cols = add_dummy_cols_df(df, raw_next_token_POS, next_token_tags)
    # cols+=new_cols
    return df, cols
def create_splits(df, cv, seed=42):
    s_indexes = set(df[sentence_index])
    np.random.seed(seed)
    m = {j: np.random.randint(0, cv) for j in s_indexes}
    df['fold'] = df[sentence_index].apply(lambda s: m[s])
    res = []
    for f in range(cv):
        train = df[df['fold']!=f]
        test = df[df['fold']==f]
        res.append((train, test))
    return res
def add_corrections_cols(df):
    def get_corrections(row):
        a = row['a']
        an = row['an']
        the = row['the']
        s = [(a, 'a'), (an, 'an'), (the, 'the')]
        s.sort(key=lambda s: s[0], reverse=True)
        proposed_correction = s[0][1]
        art_val = row[article]
        if proposed_correction == art_val:
            return None, None
        else:
            return proposed_correction, s[0][0]
    df[tmp] = df.apply(get_corrections, axis=1)
    df[correction] = df[tmp].apply(lambda s: s[0])
    df[confidence] = df[tmp].apply(lambda s: s[1])
def df_to_submit_array(df, sentences):
    res = [[None]*len(x) for x in sentences]
    def collect_corrections_info(row):
        correction_val = row[correction]
        confidence_val = row[confidence]
        sentence_index_val = row[sentence_index]
        position_val  = row[position]
        if correction_val is None:
            return
        res[sentence_index_val][position_val] = (correction_val, confidence_val)
    df.apply(collect_corrections_info, axis=1)
    # res = [(k,v) for k,v in res.iteritems()]
    # res.sort(key=lambda s: s[0])
    # res = [x[1] for x in res]
    return res
def submit_xgb_test():
    train_arr = load_train()
    test_arr = load_test()
    df = test_arr.copy()
    TARGET = correct_article
    df, cols = to_xgb_df(train_arr)
    df, cols = to_xgb_df(test_arr)
    train_arr = train_arr[cols]
    test_arr=test_arr[cols]
    print len(train_arr), len(test_arr)
    train_target = train_arr[TARGET]
    del train_arr[TARGET]
    test_target = test_arr[TARGET]
    del test_arr[TARGET]
    print test_target.head()
    estimator = xgb.XGBClassifier(n_estimators=180,
                                  subsample=0.8,
                                  colsample_bytree=0.8,
                                  max_depth=5,
                                  # learning_rate=learning_rate,
                                  objective='mlogloss',
                                  nthread=-1
                                  )
    print test_arr.columns.values
    estimator.fit(
        train_arr, train_target,
        verbose=True
    )
    proba = estimator.predict_proba(test_arr)
    classes = list(estimator.classes_)
    print classes
    for c in  classes:
        col = inverse_art_map[c]
        test_arr[col] =proba[:,classes.index(c)]
        df.loc[test_arr.index, col] = test_arr.loc[test_arr.index, col]
    add_corrections_cols(df)
    sentences = load_test_arr()
    res = df_to_submit_array(df, sentences)
    json.dump(res, open('test_submition.json', 'w+'))
    return res
def submit_xgb_out_of_fold_pred(df):
    # df = load_train()
    df = create_out_of_fold_xgb_predictions(df)
    add_corrections_cols(df)
    stats = submit_train(df)
    return arr_to_df(stats['w']), arr_to_df(stats['r'])
def create_out_of_fold_xgb_predictions(df):
    # df = load_train()
    TARGET = correct_article
    df_cp = df.copy()
    df, cols = to_xgb_df(df)
    losses = []
    for train_arr, test_arr in create_splits(df, 3):
        train_arr = train_arr[cols]
        test_arr=test_arr[cols]
        print len(train_arr), len(test_arr)
        train_target = train_arr[TARGET]
        del train_arr[TARGET]
        test_target = test_arr[TARGET]
        del test_arr[TARGET]
        # train_arr, test_arr = train_arr[cols], test_arr[cols]
        estimator = xgb.XGBClassifier(n_estimators=180,
                                      subsample=0.8,
                                      colsample_bytree=0.8,
                                      max_depth=5,
                                      # learning_rate=learning_rate,
                                      objective='mlogloss',
                                      nthread=-1
                                      )
        print test_arr.columns.values
        estimator.fit(
            train_arr, train_target,
            verbose=True
        )
        proba = estimator.predict_proba(test_arr)
        classes = list(estimator.classes_)
        print classes
        for c in  classes:
            col = inverse_art_map[c]
            test_arr[col] =proba[:,classes.index(c)]
            df_cp.loc[test_arr.index, col] = test_arr.loc[test_arr.index, col]
        loss = log_loss(test_target, proba)
        losses.append(loss)
        print loss
    return df_cp
def eval_target_score(arts, probs, labels):
    sz=len(arts)
    all_mistakes = sum(arts[j]!=labels[j] for j in range(sz))
    data=[]
    for j in range(sz):
        p = probs[j]#probs
        a=arts[j]#current article
        correct_a = labels[j]#correct article
        p = [(i, p[i]) for i in range(len(p))]
        p.sort(key=lambda s: s[1], reverse=True)
        s = p[0][0]
        conf = p[0][1]
        conf = float('-inf') if a==s else conf
        c = s ==correct_a
        data.append((-conf, c, correct_a!=a))
    data.sort()
    fp2 = 0
    fp = 0
    tp = 0
    score = 0
    acc = 0
    for _, c, r in data:
        fp2 += not c # wrong correction
        fp += not r#realy errors count
        tp += c#right correction
        acc = max(acc, 1 - (0. + fp + all_mistakes - tp) / len(data))
        if fp2 * 1. / len(data) <= 0.02:
            score = tp * 1. / all_mistakes
    print 'target score = %.2f %%' % (score * 100)
    print 'accuracy (just for info) = %.2f %%' % (acc * 100)
    return 'target_score'  , -score
def perform_xgboost_cv(df):
    # df = load_train()
    TARGET = correct_article
    df, cols = to_xgb_df(df)
    losses = []
    for train_arr, test_arr in create_splits(df, 3):
        train_arr = train_arr[cols]
        test_arr=test_arr[cols]
        def get_articles(labels):
            if len(labels)==len(train_arr):
                return list(train_arr[article])
            elif len(labels) == len(test_arr):
                return list(test_arr[article])
            raise
        def my_obj(preds, dtrain):
            labels = dtrain.get_label()
            arts = get_articles(labels)
            return eval_target_score(arts, preds, labels)
        print len(train_arr), len(test_arr)
        train_target = train_arr[TARGET]
        del train_arr[TARGET]
        test_target = test_arr[TARGET]
        del test_arr[TARGET]
        # train_arr, test_arr = train_arr[cols], test_arr[cols]
        estimator = xgb.XGBClassifier(n_estimators=10000,
                                      subsample=0.8,
                                      colsample_bytree=0.8,
                                      max_depth=5,
                                      # learning_rate=learning_rate,
                                      objective='mlogloss',
                                      nthread=-1
                                      )
        print test_arr.columns.values
        eval_set = [(train_arr, train_target), (test_arr, test_target)]
        estimator.fit(
            train_arr, train_target,
            eval_set=eval_set,
            eval_metric='mlogloss',
            verbose=True,
            early_stopping_rounds=50
        )
        classes = list(estimator.classes_)
        print classes
        xgb.plot_importance(estimator)
        plt.show()
        proba = estimator.predict_proba(test_arr)
        loss = log_loss(test_target, proba)
        losses.append(loss)
        print loss
cols=['a_bi_freq_suff' ,'a_three_freq_suff' ,'a_four_freq_suff', 'a_five_freq_suff',
      'a_bi_freq_pref' ,'a_three_freq_pref' ,'a_four_freq_pref' ,'a_five_freq_pref',
      'the_bi_freq_suff', 'the_three_freq_suff', 'the_four_freq_suff',
      'the_five_freq_suff', 'the_bi_freq_pref' ,'the_three_freq_pref',
      'the_four_freq_pref' ,'the_five_freq_pref' ,'a_the_bi_freq_suff_p1',
      'the_a_bi_freq_suff_p1' ,'a_the_three_freq_suff_p1',
      'the_a_three_freq_suff_p1' ,'a_the_four_freq_suff_p1',
      'the_a_four_freq_suff_p1' ,'a_the_five_freq_suff_p1',
      'the_a_five_freq_suff_p1' ,'a_the_bi_freq_pref_p1' 'the_a_bi_freq_pref_p1',
      'a_the_three_freq_pref_p1', 'the_a_three_freq_pref_p1',
      'a_the_four_freq_pref_p1', 'the_a_four_freq_pref_p1',
      'a_the_five_freq_pref_p1', 'the_a_five_freq_pref_p1',
      'a_the_bi_freq_suff_p10', 'the_a_bi_freq_suff_p10',
      'a_the_three_freq_suff_p10' ,'the_a_three_freq_suff_p10',
      'a_the_four_freq_suff_p10' ,'the_a_four_freq_suff_p10',
      'a_the_five_freq_suff_p10', 'the_a_five_freq_suff_p10',
      'a_the_bi_freq_pref_p10', 'the_a_bi_freq_pref_p10',
      'a_the_three_freq_pref_p10', 'the_a_three_freq_pref_p10',
      'a_the_four_freq_pref_p10' ,'the_a_four_freq_pref_p10',
      'a_the_five_freq_pref_p10' ,'the_a_five_freq_pref_p10',
      'a_the_bi_freq_suff_p100', 'the_a_bi_freq_suff_p100',
      'a_the_three_freq_suff_p100', 'the_a_three_freq_suff_p100',
      'a_the_four_freq_suff_p100' ,'the_a_four_freq_suff_p100',
      'a_the_five_freq_suff_p100' ,'the_a_five_freq_suff_p100',
      'a_the_bi_freq_pref_p100', 'the_a_bi_freq_pref_p100',
      'a_the_three_freq_pref_p100', 'the_a_three_freq_pref_p100',
      'a_the_four_freq_pref_p100' ,'the_a_four_freq_pref_p100',
      'a_the_five_freq_pref_p100' ,'the_a_five_freq_pref_p100' ,'st_with_v',
      'article']
# train_df = exploring_df(load_train())
# train_df = load_train()
#bad, good = process_max_ngram_freq_naive(train_df, 8, 5, 10)   63%
# bad, good = process_max_ngram_freq_naive(train_df, 8, 5, 10)
Backend Qt4Agg is interactive backend. Turning interactive mode on.
/usr/local/lib/python2.7/dist-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.
  "`IPython.html.widgets` has moved to `ipywidgets`.", ShimWarning)
[]
df = load_train()
True
perform_xgboost_cv(df.copy())
35673 17890
['a_bi_freq_suff' 'a_three_freq_suff' 'a_four_freq_suff' 'a_five_freq_suff'
 'a_bi_freq_pref' 'a_three_freq_pref' 'a_four_freq_pref' 'a_five_freq_pref'
 'a_sn_gram_freq' 'the_bi_freq_suff' 'the_three_freq_suff'
 'the_four_freq_suff' 'the_five_freq_suff' 'the_bi_freq_pref'
 'the_three_freq_pref' 'the_four_freq_pref' 'the_five_freq_pref'
 'the_sn_gram_freq' 'a_the_bi_freq_suff_p1' 'the_a_bi_freq_suff_p1'
 'a_the_three_freq_suff_p1' 'the_a_three_freq_suff_p1'
 'a_the_four_freq_suff_p1' 'the_a_four_freq_suff_p1'
 'a_the_five_freq_suff_p1' 'the_a_five_freq_suff_p1'
 'a_the_bi_freq_pref_p1' 'the_a_bi_freq_pref_p1' 'a_the_three_freq_pref_p1'
 'the_a_three_freq_pref_p1' 'a_the_four_freq_pref_p1'
 'the_a_four_freq_pref_p1' 'a_the_five_freq_pref_p1'
 'the_a_five_freq_pref_p1' 'a_the_sn_ngram_freq_p1'
 'the_a_sn_ngram_freq_p1' 'a_the_bi_freq_suff_p10' 'the_a_bi_freq_suff_p10'
 'a_the_three_freq_suff_p10' 'the_a_three_freq_suff_p10'
 'a_the_four_freq_suff_p10' 'the_a_four_freq_suff_p10'
 'a_the_five_freq_suff_p10' 'the_a_five_freq_suff_p10'
 'a_the_bi_freq_pref_p10' 'the_a_bi_freq_pref_p10'
 'a_the_three_freq_pref_p10' 'the_a_three_freq_pref_p10'
 'a_the_four_freq_pref_p10' 'the_a_four_freq_pref_p10'
 'a_the_five_freq_pref_p10' 'the_a_five_freq_pref_p10'
 'a_the_sn_ngram_freq_p10' 'the_a_sn_ngram_freq_p10'
 'a_the_bi_freq_suff_p100' 'the_a_bi_freq_suff_p100'
 'a_the_three_freq_suff_p100' 'the_a_three_freq_suff_p100'
 'a_the_four_freq_suff_p100' 'the_a_four_freq_suff_p100'
 'a_the_five_freq_suff_p100' 'the_a_five_freq_suff_p100'
 'a_the_bi_freq_pref_p100' 'the_a_bi_freq_pref_p100'
 'a_the_three_freq_pref_p100' 'the_a_three_freq_pref_p100'
 'a_the_four_freq_pref_p100' 'the_a_four_freq_pref_p100'
 'a_the_five_freq_pref_p100' 'the_a_five_freq_pref_p100'
 'a_the_sn_ngram_freq_p100' 'the_a_sn_ngram_freq_p100' 'st_with_v'
 'article']
[0]	validation_0-mlogloss:0.991127	validation_1-mlogloss:0.99207
Multiple eval metrics have been passed: 'validation_1-mlogloss' will be used for early stopping.
Will train until validation_1-mlogloss hasn't improved in 50 rounds.
[1]	validation_0-mlogloss:0.902301	validation_1-mlogloss:0.904093
[2]	validation_0-mlogloss:0.825968	validation_1-mlogloss:0.828487
[3]	validation_0-mlogloss:0.760435	validation_1-mlogloss:0.76368
[4]	validation_0-mlogloss:0.703929	validation_1-mlogloss:0.70763
[5]	validation_0-mlogloss:0.656259	validation_1-mlogloss:0.660539
[6]	validation_0-mlogloss:0.612731	validation_1-mlogloss:0.617556
[7]	validation_0-mlogloss:0.576021	validation_1-mlogloss:0.581399
[8]	validation_0-mlogloss:0.542408	validation_1-mlogloss:0.548214
[9]	validation_0-mlogloss:0.512077	validation_1-mlogloss:0.518403
[10]	validation_0-mlogloss:0.48507	validation_1-mlogloss:0.491863
[11]	validation_0-mlogloss:0.461007	validation_1-mlogloss:0.468225
[12]	validation_0-mlogloss:0.44056	validation_1-mlogloss:0.44832
[13]	validation_0-mlogloss:0.421125	validation_1-mlogloss:0.429487
[14]	validation_0-mlogloss:0.40469	validation_1-mlogloss:0.413446
[15]	validation_0-mlogloss:0.389843	validation_1-mlogloss:0.399069
[16]	validation_0-mlogloss:0.375584	validation_1-mlogloss:0.385273
[17]	validation_0-mlogloss:0.362465	validation_1-mlogloss:0.372461
[18]	validation_0-mlogloss:0.350818	validation_1-mlogloss:0.361227
[19]	validation_0-mlogloss:0.341209	validation_1-mlogloss:0.352036
[20]	validation_0-mlogloss:0.331437	validation_1-mlogloss:0.342591
[21]	validation_0-mlogloss:0.323268	validation_1-mlogloss:0.334922
[22]	validation_0-mlogloss:0.315242	validation_1-mlogloss:0.327146
[23]	validation_0-mlogloss:0.308511	validation_1-mlogloss:0.320832
[24]	validation_0-mlogloss:0.301798	validation_1-mlogloss:0.314325
[25]	validation_0-mlogloss:0.296279	validation_1-mlogloss:0.309222
[26]	validation_0-mlogloss:0.290573	validation_1-mlogloss:0.30385
[27]	validation_0-mlogloss:0.285459	validation_1-mlogloss:0.299045
[28]	validation_0-mlogloss:0.281316	validation_1-mlogloss:0.295226
[29]	validation_0-mlogloss:0.276919	validation_1-mlogloss:0.291034
[30]	validation_0-mlogloss:0.272785	validation_1-mlogloss:0.287226
[31]	validation_0-mlogloss:0.269058	validation_1-mlogloss:0.283834
[32]	validation_0-mlogloss:0.265675	validation_1-mlogloss:0.280667
[33]	validation_0-mlogloss:0.262528	validation_1-mlogloss:0.277809
[34]	validation_0-mlogloss:0.259654	validation_1-mlogloss:0.275185
[35]	validation_0-mlogloss:0.257294	validation_1-mlogloss:0.27312
[36]	validation_0-mlogloss:0.254689	validation_1-mlogloss:0.270834
[37]	validation_0-mlogloss:0.252326	validation_1-mlogloss:0.268753
[38]	validation_0-mlogloss:0.250085	validation_1-mlogloss:0.266824
[39]	validation_0-mlogloss:0.24835	validation_1-mlogloss:0.265395
[40]	validation_0-mlogloss:0.246397	validation_1-mlogloss:0.263756
[41]	validation_0-mlogloss:0.244564	validation_1-mlogloss:0.262235
[42]	validation_0-mlogloss:0.243217	validation_1-mlogloss:0.261212
[43]	validation_0-mlogloss:0.241602	validation_1-mlogloss:0.259848
[44]	validation_0-mlogloss:0.240079	validation_1-mlogloss:0.258598
[45]	validation_0-mlogloss:0.238644	validation_1-mlogloss:0.257471
[46]	validation_0-mlogloss:0.237552	validation_1-mlogloss:0.256669
[47]	validation_0-mlogloss:0.236472	validation_1-mlogloss:0.255878
[48]	validation_0-mlogloss:0.235122	validation_1-mlogloss:0.254849
[49]	validation_0-mlogloss:0.234001	validation_1-mlogloss:0.253914
[50]	validation_0-mlogloss:0.232882	validation_1-mlogloss:0.253083
[51]	validation_0-mlogloss:0.232071	validation_1-mlogloss:0.252561
[52]	validation_0-mlogloss:0.231147	validation_1-mlogloss:0.251875
[53]	validation_0-mlogloss:0.23015	validation_1-mlogloss:0.251151
[54]	validation_0-mlogloss:0.229235	validation_1-mlogloss:0.250438
[55]	validation_0-mlogloss:0.228343	validation_1-mlogloss:0.249869
[56]	validation_0-mlogloss:0.227442	validation_1-mlogloss:0.249327
[57]	validation_0-mlogloss:0.226618	validation_1-mlogloss:0.248773
[58]	validation_0-mlogloss:0.225868	validation_1-mlogloss:0.248267
[59]	validation_0-mlogloss:0.224966	validation_1-mlogloss:0.247788
[60]	validation_0-mlogloss:0.224203	validation_1-mlogloss:0.247346
[61]	validation_0-mlogloss:0.223292	validation_1-mlogloss:0.246876
[62]	validation_0-mlogloss:0.222471	validation_1-mlogloss:0.246494
[63]	validation_0-mlogloss:0.221658	validation_1-mlogloss:0.246114
[64]	validation_0-mlogloss:0.220817	validation_1-mlogloss:0.245771
[65]	validation_0-mlogloss:0.220191	validation_1-mlogloss:0.245474
[66]	validation_0-mlogloss:0.219576	validation_1-mlogloss:0.245112
[67]	validation_0-mlogloss:0.219013	validation_1-mlogloss:0.244829
[68]	validation_0-mlogloss:0.218483	validation_1-mlogloss:0.244608
[69]	validation_0-mlogloss:0.21794	validation_1-mlogloss:0.244309
[70]	validation_0-mlogloss:0.217286	validation_1-mlogloss:0.244103
[71]	validation_0-mlogloss:0.216742	validation_1-mlogloss:0.243855
[72]	validation_0-mlogloss:0.215886	validation_1-mlogloss:0.243527
[73]	validation_0-mlogloss:0.215363	validation_1-mlogloss:0.24339
[74]	validation_0-mlogloss:0.214821	validation_1-mlogloss:0.243211
[75]	validation_0-mlogloss:0.214433	validation_1-mlogloss:0.243085
[76]	validation_0-mlogloss:0.213872	validation_1-mlogloss:0.242929
[77]	validation_0-mlogloss:0.213343	validation_1-mlogloss:0.242765
[78]	validation_0-mlogloss:0.212808	validation_1-mlogloss:0.242572
[79]	validation_0-mlogloss:0.212297	validation_1-mlogloss:0.242356
[80]	validation_0-mlogloss:0.211862	validation_1-mlogloss:0.242181
[81]	validation_0-mlogloss:0.211339	validation_1-mlogloss:0.241993
[82]	validation_0-mlogloss:0.210914	validation_1-mlogloss:0.241844
[83]	validation_0-mlogloss:0.210493	validation_1-mlogloss:0.241793
[84]	validation_0-mlogloss:0.20976	validation_1-mlogloss:0.241627
[85]	validation_0-mlogloss:0.209391	validation_1-mlogloss:0.241553
[86]	validation_0-mlogloss:0.208822	validation_1-mlogloss:0.241398
[87]	validation_0-mlogloss:0.208391	validation_1-mlogloss:0.241279
[88]	validation_0-mlogloss:0.207802	validation_1-mlogloss:0.241124
[89]	validation_0-mlogloss:0.207351	validation_1-mlogloss:0.241035
[90]	validation_0-mlogloss:0.206945	validation_1-mlogloss:0.240936
[91]	validation_0-mlogloss:0.206385	validation_1-mlogloss:0.240805
[92]	validation_0-mlogloss:0.20594	validation_1-mlogloss:0.240705
[93]	validation_0-mlogloss:0.205569	validation_1-mlogloss:0.240595
[94]	validation_0-mlogloss:0.204971	validation_1-mlogloss:0.240314
[95]	validation_0-mlogloss:0.204548	validation_1-mlogloss:0.24023
[96]	validation_0-mlogloss:0.204181	validation_1-mlogloss:0.24015
[97]	validation_0-mlogloss:0.203715	validation_1-mlogloss:0.239999
[98]	validation_0-mlogloss:0.203312	validation_1-mlogloss:0.239957
[99]	validation_0-mlogloss:0.202831	validation_1-mlogloss:0.239884
[100]	validation_0-mlogloss:0.202405	validation_1-mlogloss:0.239776
[101]	validation_0-mlogloss:0.201858	validation_1-mlogloss:0.23965
[102]	validation_0-mlogloss:0.201554	validation_1-mlogloss:0.239669
[103]	validation_0-mlogloss:0.201246	validation_1-mlogloss:0.239667
[104]	validation_0-mlogloss:0.200804	validation_1-mlogloss:0.2397
[105]	validation_0-mlogloss:0.200517	validation_1-mlogloss:0.239575
[106]	validation_0-mlogloss:0.200145	validation_1-mlogloss:0.239597
[107]	validation_0-mlogloss:0.199729	validation_1-mlogloss:0.239542
[108]	validation_0-mlogloss:0.199406	validation_1-mlogloss:0.239564
[109]	validation_0-mlogloss:0.198993	validation_1-mlogloss:0.239461
[110]	validation_0-mlogloss:0.198545	validation_1-mlogloss:0.239371
[111]	validation_0-mlogloss:0.198255	validation_1-mlogloss:0.239357
[112]	validation_0-mlogloss:0.197882	validation_1-mlogloss:0.2394
[113]	validation_0-mlogloss:0.197499	validation_1-mlogloss:0.239324
[114]	validation_0-mlogloss:0.197282	validation_1-mlogloss:0.239351
[115]	validation_0-mlogloss:0.196888	validation_1-mlogloss:0.239321
[116]	validation_0-mlogloss:0.19647	validation_1-mlogloss:0.239375
[117]	validation_0-mlogloss:0.196278	validation_1-mlogloss:0.239358
[118]	validation_0-mlogloss:0.195917	validation_1-mlogloss:0.239261
[119]	validation_0-mlogloss:0.195561	validation_1-mlogloss:0.23922
[120]	validation_0-mlogloss:0.195184	validation_1-mlogloss:0.239183
[121]	validation_0-mlogloss:0.194868	validation_1-mlogloss:0.239142
[122]	validation_0-mlogloss:0.194422	validation_1-mlogloss:0.239035
[123]	validation_0-mlogloss:0.194018	validation_1-mlogloss:0.239011
[124]	validation_0-mlogloss:0.19354	validation_1-mlogloss:0.238961
[125]	validation_0-mlogloss:0.193089	validation_1-mlogloss:0.238902
[126]	validation_0-mlogloss:0.19282	validation_1-mlogloss:0.23894
[127]	validation_0-mlogloss:0.192401	validation_1-mlogloss:0.238939
[128]	validation_0-mlogloss:0.191895	validation_1-mlogloss:0.238924
[129]	validation_0-mlogloss:0.191317	validation_1-mlogloss:0.238973
[130]	validation_0-mlogloss:0.190913	validation_1-mlogloss:0.238962
[131]	validation_0-mlogloss:0.190559	validation_1-mlogloss:0.238976
[132]	validation_0-mlogloss:0.190306	validation_1-mlogloss:0.239002
[133]	validation_0-mlogloss:0.189954	validation_1-mlogloss:0.239017
[134]	validation_0-mlogloss:0.189732	validation_1-mlogloss:0.239014
[135]	validation_0-mlogloss:0.189382	validation_1-mlogloss:0.239043
[136]	validation_0-mlogloss:0.188699	validation_1-mlogloss:0.239043
[137]	validation_0-mlogloss:0.188184	validation_1-mlogloss:0.239
[138]	validation_0-mlogloss:0.18759	validation_1-mlogloss:0.238975
[139]	validation_0-mlogloss:0.187093	validation_1-mlogloss:0.239022
[140]	validation_0-mlogloss:0.186652	validation_1-mlogloss:0.238978
[141]	validation_0-mlogloss:0.186253	validation_1-mlogloss:0.238934
[142]	validation_0-mlogloss:0.185855	validation_1-mlogloss:0.238971
[143]	validation_0-mlogloss:0.18564	validation_1-mlogloss:0.23902
[144]	validation_0-mlogloss:0.18529	validation_1-mlogloss:0.23905
[145]	validation_0-mlogloss:0.185009	validation_1-mlogloss:0.23905
[146]	validation_0-mlogloss:0.184647	validation_1-mlogloss:0.239081
[147]	validation_0-mlogloss:0.184347	validation_1-mlogloss:0.239109
[148]	validation_0-mlogloss:0.184017	validation_1-mlogloss:0.239064
[149]	validation_0-mlogloss:0.183695	validation_1-mlogloss:0.239029
[150]	validation_0-mlogloss:0.18337	validation_1-mlogloss:0.239048
[151]	validation_0-mlogloss:0.183061	validation_1-mlogloss:0.239086
[152]	validation_0-mlogloss:0.182721	validation_1-mlogloss:0.239026
[153]	validation_0-mlogloss:0.182465	validation_1-mlogloss:0.238984
[154]	validation_0-mlogloss:0.182263	validation_1-mlogloss:0.239009
[155]	validation_0-mlogloss:0.181854	validation_1-mlogloss:0.239011
[156]	validation_0-mlogloss:0.181504	validation_1-mlogloss:0.23903
[157]	validation_0-mlogloss:0.181079	validation_1-mlogloss:0.239044
[158]	validation_0-mlogloss:0.18071	validation_1-mlogloss:0.238983
[159]	validation_0-mlogloss:0.18032	validation_1-mlogloss:0.238933
[160]	validation_0-mlogloss:0.18004	validation_1-mlogloss:0.238974
[161]	validation_0-mlogloss:0.17956	validation_1-mlogloss:0.238988
[162]	validation_0-mlogloss:0.179161	validation_1-mlogloss:0.238973
[163]	validation_0-mlogloss:0.178896	validation_1-mlogloss:0.238991
[164]	validation_0-mlogloss:0.178529	validation_1-mlogloss:0.238947
[165]	validation_0-mlogloss:0.178188	validation_1-mlogloss:0.238865
[166]	validation_0-mlogloss:0.17777	validation_1-mlogloss:0.2389
[167]	validation_0-mlogloss:0.177352	validation_1-mlogloss:0.238856
[168]	validation_0-mlogloss:0.176968	validation_1-mlogloss:0.238819
[169]	validation_0-mlogloss:0.176738	validation_1-mlogloss:0.238832
[170]	validation_0-mlogloss:0.176304	validation_1-mlogloss:0.238759
[171]	validation_0-mlogloss:0.175841	validation_1-mlogloss:0.238716
[172]	validation_0-mlogloss:0.175499	validation_1-mlogloss:0.238719
[173]	validation_0-mlogloss:0.175138	validation_1-mlogloss:0.238685
[174]	validation_0-mlogloss:0.17474	validation_1-mlogloss:0.238671
[175]	validation_0-mlogloss:0.174479	validation_1-mlogloss:0.238724
[176]	validation_0-mlogloss:0.174055	validation_1-mlogloss:0.238682
[177]	validation_0-mlogloss:0.17377	validation_1-mlogloss:0.238555
[178]	validation_0-mlogloss:0.173372	validation_1-mlogloss:0.238565
[179]	validation_0-mlogloss:0.173038	validation_1-mlogloss:0.238657
[180]	validation_0-mlogloss:0.172641	validation_1-mlogloss:0.238671
[181]	validation_0-mlogloss:0.172233	validation_1-mlogloss:0.238657
[182]	validation_0-mlogloss:0.171889	validation_1-mlogloss:0.238718
[183]	validation_0-mlogloss:0.171592	validation_1-mlogloss:0.23872
[184]	validation_0-mlogloss:0.171248	validation_1-mlogloss:0.238744
[185]	validation_0-mlogloss:0.170952	validation_1-mlogloss:0.238732
[186]	validation_0-mlogloss:0.170753	validation_1-mlogloss:0.238761
[187]	validation_0-mlogloss:0.17051	validation_1-mlogloss:0.238767
[188]	validation_0-mlogloss:0.170224	validation_1-mlogloss:0.238724
[189]	validation_0-mlogloss:0.169916	validation_1-mlogloss:0.238723
[190]	validation_0-mlogloss:0.169693	validation_1-mlogloss:0.238734
[191]	validation_0-mlogloss:0.169354	validation_1-mlogloss:0.238762
[192]	validation_0-mlogloss:0.168921	validation_1-mlogloss:0.23878
[193]	validation_0-mlogloss:0.168698	validation_1-mlogloss:0.238802
[194]	validation_0-mlogloss:0.168278	validation_1-mlogloss:0.238789
[195]	validation_0-mlogloss:0.167928	validation_1-mlogloss:0.238802
[196]	validation_0-mlogloss:0.167612	validation_1-mlogloss:0.23881
[197]	validation_0-mlogloss:0.167248	validation_1-mlogloss:0.238871
[198]	validation_0-mlogloss:0.166859	validation_1-mlogloss:0.238829
[199]	validation_0-mlogloss:0.166601	validation_1-mlogloss:0.238856
[200]	validation_0-mlogloss:0.166287	validation_1-mlogloss:0.23882
[201]	validation_0-mlogloss:0.166012	validation_1-mlogloss:0.238881
[202]	validation_0-mlogloss:0.165673	validation_1-mlogloss:0.238955
[203]	validation_0-mlogloss:0.165389	validation_1-mlogloss:0.239008
[204]	validation_0-mlogloss:0.165066	validation_1-mlogloss:0.239056
[205]	validation_0-mlogloss:0.164697	validation_1-mlogloss:0.239026
[206]	validation_0-mlogloss:0.164289	validation_1-mlogloss:0.239083
[207]	validation_0-mlogloss:0.163878	validation_1-mlogloss:0.239085
[208]	validation_0-mlogloss:0.163605	validation_1-mlogloss:0.239097
[209]	validation_0-mlogloss:0.163313	validation_1-mlogloss:0.239096
[210]	validation_0-mlogloss:0.16305	validation_1-mlogloss:0.239131
[211]	validation_0-mlogloss:0.162798	validation_1-mlogloss:0.239134
[212]	validation_0-mlogloss:0.162506	validation_1-mlogloss:0.239142
[213]	validation_0-mlogloss:0.16225	validation_1-mlogloss:0.239189
[214]	validation_0-mlogloss:0.161888	validation_1-mlogloss:0.239274
[215]	validation_0-mlogloss:0.161605	validation_1-mlogloss:0.239255
[216]	validation_0-mlogloss:0.161347	validation_1-mlogloss:0.239269
[217]	validation_0-mlogloss:0.160983	validation_1-mlogloss:0.239248
[218]	validation_0-mlogloss:0.160613	validation_1-mlogloss:0.239303
[219]	validation_0-mlogloss:0.160351	validation_1-mlogloss:0.239311
[220]	validation_0-mlogloss:0.160114	validation_1-mlogloss:0.239361
[221]	validation_0-mlogloss:0.159828	validation_1-mlogloss:0.239374
[222]	validation_0-mlogloss:0.159529	validation_1-mlogloss:0.239373
[223]	validation_0-mlogloss:0.159232	validation_1-mlogloss:0.239357
[224]	validation_0-mlogloss:0.158979	validation_1-mlogloss:0.239359
[225]	validation_0-mlogloss:0.158808	validation_1-mlogloss:0.239374
[226]	validation_0-mlogloss:0.158487	validation_1-mlogloss:0.239386
[227]	validation_0-mlogloss:0.158159	validation_1-mlogloss:0.239448
Stopping. Best iteration:
[177]	validation_0-mlogloss:0.17377	validation_1-mlogloss:0.238555
[0, 1, 2]
0.239448453025
35672 17891
['a_bi_freq_suff' 'a_three_freq_suff' 'a_four_freq_suff' 'a_five_freq_suff'
 'a_bi_freq_pref' 'a_three_freq_pref' 'a_four_freq_pref' 'a_five_freq_pref'
 'a_sn_gram_freq' 'the_bi_freq_suff' 'the_three_freq_suff'
 'the_four_freq_suff' 'the_five_freq_suff' 'the_bi_freq_pref'
 'the_three_freq_pref' 'the_four_freq_pref' 'the_five_freq_pref'
 'the_sn_gram_freq' 'a_the_bi_freq_suff_p1' 'the_a_bi_freq_suff_p1'
 'a_the_three_freq_suff_p1' 'the_a_three_freq_suff_p1'
 'a_the_four_freq_suff_p1' 'the_a_four_freq_suff_p1'
 'a_the_five_freq_suff_p1' 'the_a_five_freq_suff_p1'
 'a_the_bi_freq_pref_p1' 'the_a_bi_freq_pref_p1' 'a_the_three_freq_pref_p1'
 'the_a_three_freq_pref_p1' 'a_the_four_freq_pref_p1'
 'the_a_four_freq_pref_p1' 'a_the_five_freq_pref_p1'
 'the_a_five_freq_pref_p1' 'a_the_sn_ngram_freq_p1'
 'the_a_sn_ngram_freq_p1' 'a_the_bi_freq_suff_p10' 'the_a_bi_freq_suff_p10'
 'a_the_three_freq_suff_p10' 'the_a_three_freq_suff_p10'
 'a_the_four_freq_suff_p10' 'the_a_four_freq_suff_p10'
 'a_the_five_freq_suff_p10' 'the_a_five_freq_suff_p10'
 'a_the_bi_freq_pref_p10' 'the_a_bi_freq_pref_p10'
 'a_the_three_freq_pref_p10' 'the_a_three_freq_pref_p10'
 'a_the_four_freq_pref_p10' 'the_a_four_freq_pref_p10'
 'a_the_five_freq_pref_p10' 'the_a_five_freq_pref_p10'
 'a_the_sn_ngram_freq_p10' 'the_a_sn_ngram_freq_p10'
 'a_the_bi_freq_suff_p100' 'the_a_bi_freq_suff_p100'
 'a_the_three_freq_suff_p100' 'the_a_three_freq_suff_p100'
 'a_the_four_freq_suff_p100' 'the_a_four_freq_suff_p100'
 'a_the_five_freq_suff_p100' 'the_a_five_freq_suff_p100'
 'a_the_bi_freq_pref_p100' 'the_a_bi_freq_pref_p100'
 'a_the_three_freq_pref_p100' 'the_a_three_freq_pref_p100'
 'a_the_four_freq_pref_p100' 'the_a_four_freq_pref_p100'
 'a_the_five_freq_pref_p100' 'the_a_five_freq_pref_p100'
 'a_the_sn_ngram_freq_p100' 'the_a_sn_ngram_freq_p100' 'st_with_v'
 'article']
[0]	validation_0-mlogloss:0.994492	validation_1-mlogloss:0.995118
Multiple eval metrics have been passed: 'validation_1-mlogloss' will be used for early stopping.
Will train until validation_1-mlogloss hasn't improved in 50 rounds.
[1]	validation_0-mlogloss:0.904416	validation_1-mlogloss:0.905649
[2]	validation_0-mlogloss:0.830359	validation_1-mlogloss:0.832166
[3]	validation_0-mlogloss:0.766806	validation_1-mlogloss:0.769204
[4]	validation_0-mlogloss:0.709345	validation_1-mlogloss:0.712396
[5]	validation_0-mlogloss:0.661671	validation_1-mlogloss:0.665159
[6]	validation_0-mlogloss:0.617602	validation_1-mlogloss:0.621763
[7]	validation_0-mlogloss:0.580928	validation_1-mlogloss:0.58578
[8]	validation_0-mlogloss:0.546455	validation_1-mlogloss:0.551885
[9]	validation_0-mlogloss:0.515668	validation_1-mlogloss:0.521591
[10]	validation_0-mlogloss:0.488198	validation_1-mlogloss:0.494613
[11]	validation_0-mlogloss:0.463845	validation_1-mlogloss:0.470881
[12]	validation_0-mlogloss:0.442057	validation_1-mlogloss:0.449665
[13]	validation_0-mlogloss:0.42248	validation_1-mlogloss:0.430419
[14]	validation_0-mlogloss:0.404924	validation_1-mlogloss:0.413363
[15]	validation_0-mlogloss:0.389147	validation_1-mlogloss:0.398221
[16]	validation_0-mlogloss:0.374916	validation_1-mlogloss:0.384433
[17]	validation_0-mlogloss:0.361966	validation_1-mlogloss:0.371854
[18]	validation_0-mlogloss:0.351124	validation_1-mlogloss:0.36122
[19]	validation_0-mlogloss:0.340527	validation_1-mlogloss:0.350929
[20]	validation_0-mlogloss:0.330794	validation_1-mlogloss:0.341738
[21]	validation_0-mlogloss:0.322795	validation_1-mlogloss:0.334018
[22]	validation_0-mlogloss:0.31479	validation_1-mlogloss:0.326428
[23]	validation_0-mlogloss:0.308131	validation_1-mlogloss:0.320027
[24]	validation_0-mlogloss:0.301486	validation_1-mlogloss:0.313816
[25]	validation_0-mlogloss:0.295301	validation_1-mlogloss:0.308004
[26]	validation_0-mlogloss:0.2896	validation_1-mlogloss:0.302703
[27]	validation_0-mlogloss:0.284947	validation_1-mlogloss:0.298435
[28]	validation_0-mlogloss:0.280217	validation_1-mlogloss:0.294128
[29]	validation_0-mlogloss:0.276372	validation_1-mlogloss:0.290591
[30]	validation_0-mlogloss:0.272838	validation_1-mlogloss:0.287426
[31]	validation_0-mlogloss:0.269565	validation_1-mlogloss:0.284476
[32]	validation_0-mlogloss:0.26596	validation_1-mlogloss:0.281271
[33]	validation_0-mlogloss:0.262688	validation_1-mlogloss:0.278373
[34]	validation_0-mlogloss:0.259711	validation_1-mlogloss:0.275821
[35]	validation_0-mlogloss:0.256914	validation_1-mlogloss:0.273442
[36]	validation_0-mlogloss:0.254784	validation_1-mlogloss:0.271599
[37]	validation_0-mlogloss:0.252316	validation_1-mlogloss:0.26956
[38]	validation_0-mlogloss:0.250411	validation_1-mlogloss:0.268048
[39]	validation_0-mlogloss:0.24824	validation_1-mlogloss:0.266258
[40]	validation_0-mlogloss:0.246577	validation_1-mlogloss:0.264887
[41]	validation_0-mlogloss:0.244606	validation_1-mlogloss:0.263432
[42]	validation_0-mlogloss:0.242841	validation_1-mlogloss:0.26195
[43]	validation_0-mlogloss:0.241174	validation_1-mlogloss:0.260655
[44]	validation_0-mlogloss:0.239567	validation_1-mlogloss:0.259479
[45]	validation_0-mlogloss:0.238016	validation_1-mlogloss:0.258289
[46]	validation_0-mlogloss:0.236586	validation_1-mlogloss:0.257245
[47]	validation_0-mlogloss:0.235267	validation_1-mlogloss:0.25637
[48]	validation_0-mlogloss:0.234015	validation_1-mlogloss:0.255499
[49]	validation_0-mlogloss:0.232692	validation_1-mlogloss:0.254638
[50]	validation_0-mlogloss:0.231512	validation_1-mlogloss:0.253861
[51]	validation_0-mlogloss:0.230435	validation_1-mlogloss:0.2532
[52]	validation_0-mlogloss:0.22943	validation_1-mlogloss:0.252499
[53]	validation_0-mlogloss:0.228406	validation_1-mlogloss:0.251883
[54]	validation_0-mlogloss:0.227507	validation_1-mlogloss:0.251281
[55]	validation_0-mlogloss:0.226848	validation_1-mlogloss:0.250889
[56]	validation_0-mlogloss:0.225916	validation_1-mlogloss:0.250437
[57]	validation_0-mlogloss:0.225268	validation_1-mlogloss:0.250071
[58]	validation_0-mlogloss:0.224535	validation_1-mlogloss:0.249651
[59]	validation_0-mlogloss:0.224009	validation_1-mlogloss:0.249379
[60]	validation_0-mlogloss:0.223134	validation_1-mlogloss:0.248981
[61]	validation_0-mlogloss:0.222391	validation_1-mlogloss:0.248579
[62]	validation_0-mlogloss:0.22175	validation_1-mlogloss:0.24819
[63]	validation_0-mlogloss:0.221081	validation_1-mlogloss:0.247807
[64]	validation_0-mlogloss:0.220315	validation_1-mlogloss:0.247422
[65]	validation_0-mlogloss:0.219685	validation_1-mlogloss:0.247081
[66]	validation_0-mlogloss:0.21893	validation_1-mlogloss:0.246832
[67]	validation_0-mlogloss:0.218321	validation_1-mlogloss:0.246503
[68]	validation_0-mlogloss:0.217627	validation_1-mlogloss:0.246233
[69]	validation_0-mlogloss:0.217048	validation_1-mlogloss:0.245933
[70]	validation_0-mlogloss:0.216445	validation_1-mlogloss:0.245686
[71]	validation_0-mlogloss:0.215982	validation_1-mlogloss:0.245434
[72]	validation_0-mlogloss:0.215309	validation_1-mlogloss:0.245183
[73]	validation_0-mlogloss:0.214826	validation_1-mlogloss:0.244981
[74]	validation_0-mlogloss:0.214431	validation_1-mlogloss:0.244816
[75]	validation_0-mlogloss:0.213852	validation_1-mlogloss:0.244612
[76]	validation_0-mlogloss:0.213184	validation_1-mlogloss:0.244333
[77]	validation_0-mlogloss:0.212683	validation_1-mlogloss:0.244199
[78]	validation_0-mlogloss:0.212198	validation_1-mlogloss:0.244065
[79]	validation_0-mlogloss:0.21185	validation_1-mlogloss:0.243922
[80]	validation_0-mlogloss:0.211276	validation_1-mlogloss:0.243754
[81]	validation_0-mlogloss:0.21076	validation_1-mlogloss:0.243622
[82]	validation_0-mlogloss:0.20995	validation_1-mlogloss:0.243399
[83]	validation_0-mlogloss:0.209357	validation_1-mlogloss:0.243297
[84]	validation_0-mlogloss:0.208987	validation_1-mlogloss:0.243153
[85]	validation_0-mlogloss:0.208428	validation_1-mlogloss:0.243004
[86]	validation_0-mlogloss:0.208098	validation_1-mlogloss:0.242882
[87]	validation_0-mlogloss:0.207765	validation_1-mlogloss:0.242857
[88]	validation_0-mlogloss:0.207434	validation_1-mlogloss:0.242728
[89]	validation_0-mlogloss:0.206986	validation_1-mlogloss:0.242641
[90]	validation_0-mlogloss:0.206342	validation_1-mlogloss:0.242507
[91]	validation_0-mlogloss:0.205948	validation_1-mlogloss:0.242463
[92]	validation_0-mlogloss:0.205482	validation_1-mlogloss:0.242391
[93]	validation_0-mlogloss:0.205075	validation_1-mlogloss:0.242227
[94]	validation_0-mlogloss:0.20451	validation_1-mlogloss:0.242133
[95]	validation_0-mlogloss:0.204127	validation_1-mlogloss:0.242024
[96]	validation_0-mlogloss:0.203626	validation_1-mlogloss:0.241933
[97]	validation_0-mlogloss:0.203158	validation_1-mlogloss:0.241888
[98]	validation_0-mlogloss:0.202617	validation_1-mlogloss:0.241703
[99]	validation_0-mlogloss:0.202084	validation_1-mlogloss:0.241541
[100]	validation_0-mlogloss:0.201798	validation_1-mlogloss:0.241507
[101]	validation_0-mlogloss:0.201244	validation_1-mlogloss:0.241386
[102]	validation_0-mlogloss:0.200792	validation_1-mlogloss:0.241344
[103]	validation_0-mlogloss:0.200246	validation_1-mlogloss:0.241272
[104]	validation_0-mlogloss:0.199938	validation_1-mlogloss:0.241201
[105]	validation_0-mlogloss:0.199652	validation_1-mlogloss:0.241203
[106]	validation_0-mlogloss:0.19922	validation_1-mlogloss:0.241195
[107]	validation_0-mlogloss:0.19885	validation_1-mlogloss:0.241233
[108]	validation_0-mlogloss:0.198521	validation_1-mlogloss:0.24121
[109]	validation_0-mlogloss:0.198083	validation_1-mlogloss:0.241179
[110]	validation_0-mlogloss:0.197821	validation_1-mlogloss:0.241112
[111]	validation_0-mlogloss:0.197254	validation_1-mlogloss:0.241031
[112]	validation_0-mlogloss:0.196925	validation_1-mlogloss:0.240992
[113]	validation_0-mlogloss:0.196433	validation_1-mlogloss:0.240913
[114]	validation_0-mlogloss:0.196101	validation_1-mlogloss:0.240914
[115]	validation_0-mlogloss:0.195632	validation_1-mlogloss:0.240793
[116]	validation_0-mlogloss:0.195427	validation_1-mlogloss:0.240818
[117]	validation_0-mlogloss:0.194975	validation_1-mlogloss:0.240755
[118]	validation_0-mlogloss:0.194654	validation_1-mlogloss:0.240736
[119]	validation_0-mlogloss:0.194273	validation_1-mlogloss:0.240719
[120]	validation_0-mlogloss:0.193813	validation_1-mlogloss:0.240693
[121]	validation_0-mlogloss:0.193424	validation_1-mlogloss:0.240634
[122]	validation_0-mlogloss:0.193097	validation_1-mlogloss:0.240552
[123]	validation_0-mlogloss:0.192527	validation_1-mlogloss:0.240488
[124]	validation_0-mlogloss:0.191976	validation_1-mlogloss:0.240398
[125]	validation_0-mlogloss:0.191705	validation_1-mlogloss:0.24036
[126]	validation_0-mlogloss:0.191371	validation_1-mlogloss:0.240281
[127]	validation_0-mlogloss:0.1911	validation_1-mlogloss:0.240287
[128]	validation_0-mlogloss:0.190474	validation_1-mlogloss:0.240266
[129]	validation_0-mlogloss:0.190229	validation_1-mlogloss:0.240286
[130]	validation_0-mlogloss:0.189901	validation_1-mlogloss:0.240254
[131]	validation_0-mlogloss:0.189543	validation_1-mlogloss:0.240287
[132]	validation_0-mlogloss:0.189077	validation_1-mlogloss:0.240192
[133]	validation_0-mlogloss:0.188657	validation_1-mlogloss:0.24021
[134]	validation_0-mlogloss:0.1883	validation_1-mlogloss:0.240143
[135]	validation_0-mlogloss:0.187921	validation_1-mlogloss:0.24006
[136]	validation_0-mlogloss:0.18754	validation_1-mlogloss:0.240083
[137]	validation_0-mlogloss:0.187302	validation_1-mlogloss:0.240035
[138]	validation_0-mlogloss:0.186836	validation_1-mlogloss:0.23997
[139]	validation_0-mlogloss:0.186494	validation_1-mlogloss:0.239894
[140]	validation_0-mlogloss:0.186157	validation_1-mlogloss:0.239866
[141]	validation_0-mlogloss:0.185722	validation_1-mlogloss:0.239868
[142]	validation_0-mlogloss:0.185373	validation_1-mlogloss:0.23983
[143]	validation_0-mlogloss:0.184863	validation_1-mlogloss:0.239812
[144]	validation_0-mlogloss:0.184636	validation_1-mlogloss:0.239817
[145]	validation_0-mlogloss:0.184217	validation_1-mlogloss:0.239789
[146]	validation_0-mlogloss:0.183981	validation_1-mlogloss:0.239754
[147]	validation_0-mlogloss:0.183628	validation_1-mlogloss:0.239724
[148]	validation_0-mlogloss:0.183254	validation_1-mlogloss:0.239685
[149]	validation_0-mlogloss:0.182875	validation_1-mlogloss:0.239652
[150]	validation_0-mlogloss:0.182543	validation_1-mlogloss:0.239584
[151]	validation_0-mlogloss:0.182135	validation_1-mlogloss:0.239494
[152]	validation_0-mlogloss:0.181817	validation_1-mlogloss:0.239484
[153]	validation_0-mlogloss:0.181375	validation_1-mlogloss:0.239382
[154]	validation_0-mlogloss:0.180917	validation_1-mlogloss:0.239306
[155]	validation_0-mlogloss:0.180493	validation_1-mlogloss:0.239297
[156]	validation_0-mlogloss:0.180081	validation_1-mlogloss:0.239249
[157]	validation_0-mlogloss:0.179697	validation_1-mlogloss:0.239213
[158]	validation_0-mlogloss:0.179379	validation_1-mlogloss:0.239238
[159]	validation_0-mlogloss:0.17912	validation_1-mlogloss:0.239294
[160]	validation_0-mlogloss:0.178941	validation_1-mlogloss:0.239303
[161]	validation_0-mlogloss:0.178632	validation_1-mlogloss:0.239259
[162]	validation_0-mlogloss:0.178209	validation_1-mlogloss:0.239237
[163]	validation_0-mlogloss:0.177965	validation_1-mlogloss:0.239259
[164]	validation_0-mlogloss:0.17774	validation_1-mlogloss:0.239253
[165]	validation_0-mlogloss:0.177411	validation_1-mlogloss:0.239299
[166]	validation_0-mlogloss:0.177081	validation_1-mlogloss:0.239289
[167]	validation_0-mlogloss:0.176745	validation_1-mlogloss:0.239294
[168]	validation_0-mlogloss:0.176432	validation_1-mlogloss:0.239284
[169]	validation_0-mlogloss:0.17603	validation_1-mlogloss:0.239281
[170]	validation_0-mlogloss:0.175629	validation_1-mlogloss:0.239276
[171]	validation_0-mlogloss:0.175224	validation_1-mlogloss:0.239263
[172]	validation_0-mlogloss:0.174869	validation_1-mlogloss:0.239187
[173]	validation_0-mlogloss:0.17448	validation_1-mlogloss:0.239207
[174]	validation_0-mlogloss:0.174159	validation_1-mlogloss:0.239187
[175]	validation_0-mlogloss:0.173837	validation_1-mlogloss:0.239187
[176]	validation_0-mlogloss:0.173489	validation_1-mlogloss:0.239221
[177]	validation_0-mlogloss:0.173077	validation_1-mlogloss:0.239233
[178]	validation_0-mlogloss:0.172696	validation_1-mlogloss:0.23924
[179]	validation_0-mlogloss:0.17223	validation_1-mlogloss:0.239201
[180]	validation_0-mlogloss:0.171987	validation_1-mlogloss:0.239211
[181]	validation_0-mlogloss:0.171733	validation_1-mlogloss:0.239184
[182]	validation_0-mlogloss:0.171474	validation_1-mlogloss:0.239121
[183]	validation_0-mlogloss:0.171168	validation_1-mlogloss:0.239143
[184]	validation_0-mlogloss:0.170878	validation_1-mlogloss:0.239139
[185]	validation_0-mlogloss:0.170433	validation_1-mlogloss:0.239091
[186]	validation_0-mlogloss:0.170017	validation_1-mlogloss:0.239103
[187]	validation_0-mlogloss:0.1697	validation_1-mlogloss:0.239116
[188]	validation_0-mlogloss:0.169372	validation_1-mlogloss:0.239087
[189]	validation_0-mlogloss:0.169026	validation_1-mlogloss:0.239044
[190]	validation_0-mlogloss:0.168627	validation_1-mlogloss:0.23901
[191]	validation_0-mlogloss:0.168384	validation_1-mlogloss:0.239025
[192]	validation_0-mlogloss:0.16795	validation_1-mlogloss:0.238968
[193]	validation_0-mlogloss:0.167608	validation_1-mlogloss:0.238924
[194]	validation_0-mlogloss:0.167317	validation_1-mlogloss:0.23888
[195]	validation_0-mlogloss:0.167014	validation_1-mlogloss:0.23888
[196]	validation_0-mlogloss:0.166574	validation_1-mlogloss:0.238931
[197]	validation_0-mlogloss:0.166356	validation_1-mlogloss:0.238915
[198]	validation_0-mlogloss:0.166055	validation_1-mlogloss:0.238877
[199]	validation_0-mlogloss:0.165846	validation_1-mlogloss:0.238823
[200]	validation_0-mlogloss:0.165474	validation_1-mlogloss:0.238802
[201]	validation_0-mlogloss:0.165181	validation_1-mlogloss:0.238855
[202]	validation_0-mlogloss:0.164898	validation_1-mlogloss:0.238855
[203]	validation_0-mlogloss:0.164663	validation_1-mlogloss:0.238878
[204]	validation_0-mlogloss:0.164355	validation_1-mlogloss:0.238882
[205]	validation_0-mlogloss:0.164211	validation_1-mlogloss:0.238887
[206]	validation_0-mlogloss:0.163822	validation_1-mlogloss:0.238818
[207]	validation_0-mlogloss:0.16351	validation_1-mlogloss:0.238819
[208]	validation_0-mlogloss:0.163216	validation_1-mlogloss:0.238896
[209]	validation_0-mlogloss:0.162882	validation_1-mlogloss:0.238932
[210]	validation_0-mlogloss:0.162489	validation_1-mlogloss:0.238943
[211]	validation_0-mlogloss:0.162084	validation_1-mlogloss:0.238889
[212]	validation_0-mlogloss:0.161803	validation_1-mlogloss:0.238859
[213]	validation_0-mlogloss:0.161559	validation_1-mlogloss:0.238883
[214]	validation_0-mlogloss:0.161257	validation_1-mlogloss:0.238952
[215]	validation_0-mlogloss:0.161015	validation_1-mlogloss:0.23897
[216]	validation_0-mlogloss:0.160759	validation_1-mlogloss:0.238993
[217]	validation_0-mlogloss:0.160469	validation_1-mlogloss:0.239054
[218]	validation_0-mlogloss:0.160221	validation_1-mlogloss:0.239084
[219]	validation_0-mlogloss:0.159879	validation_1-mlogloss:0.23909
[220]	validation_0-mlogloss:0.159669	validation_1-mlogloss:0.239101
[221]	validation_0-mlogloss:0.159358	validation_1-mlogloss:0.239153
[222]	validation_0-mlogloss:0.159084	validation_1-mlogloss:0.239133
[223]	validation_0-mlogloss:0.158745	validation_1-mlogloss:0.239109
[224]	validation_0-mlogloss:0.158434	validation_1-mlogloss:0.239134
[225]	validation_0-mlogloss:0.158029	validation_1-mlogloss:0.239193
[226]	validation_0-mlogloss:0.15785	validation_1-mlogloss:0.239206
[227]	validation_0-mlogloss:0.157546	validation_1-mlogloss:0.239197
[228]	validation_0-mlogloss:0.157249	validation_1-mlogloss:0.239171
[229]	validation_0-mlogloss:0.157092	validation_1-mlogloss:0.239225
[230]	validation_0-mlogloss:0.156759	validation_1-mlogloss:0.239222
[231]	validation_0-mlogloss:0.156535	validation_1-mlogloss:0.239277
[232]	validation_0-mlogloss:0.156277	validation_1-mlogloss:0.239232
[233]	validation_0-mlogloss:0.156078	validation_1-mlogloss:0.239217
[234]	validation_0-mlogloss:0.155787	validation_1-mlogloss:0.239189
[235]	validation_0-mlogloss:0.155362	validation_1-mlogloss:0.239186
[236]	validation_0-mlogloss:0.155023	validation_1-mlogloss:0.239099
[237]	validation_0-mlogloss:0.15474	validation_1-mlogloss:0.239082
[238]	validation_0-mlogloss:0.154462	validation_1-mlogloss:0.239089
[239]	validation_0-mlogloss:0.154304	validation_1-mlogloss:0.239133
[240]	validation_0-mlogloss:0.154039	validation_1-mlogloss:0.239129
[241]	validation_0-mlogloss:0.153764	validation_1-mlogloss:0.239152
[242]	validation_0-mlogloss:0.153444	validation_1-mlogloss:0.239141
[243]	validation_0-mlogloss:0.153191	validation_1-mlogloss:0.239127
[244]	validation_0-mlogloss:0.153002	validation_1-mlogloss:0.239131
[245]	validation_0-mlogloss:0.152724	validation_1-mlogloss:0.239108
[246]	validation_0-mlogloss:0.152529	validation_1-mlogloss:0.239086
[247]	validation_0-mlogloss:0.152212	validation_1-mlogloss:0.239096
[248]	validation_0-mlogloss:0.151925	validation_1-mlogloss:0.239131
[249]	validation_0-mlogloss:0.151819	validation_1-mlogloss:0.239149
[250]	validation_0-mlogloss:0.151512	validation_1-mlogloss:0.239122
Stopping. Best iteration:
[200]	validation_0-mlogloss:0.165474	validation_1-mlogloss:0.238802
[0, 1, 2]
0.23912155999
35781 17782
['a_bi_freq_suff' 'a_three_freq_suff' 'a_four_freq_suff' 'a_five_freq_suff'
 'a_bi_freq_pref' 'a_three_freq_pref' 'a_four_freq_pref' 'a_five_freq_pref'
 'a_sn_gram_freq' 'the_bi_freq_suff' 'the_three_freq_suff'
 'the_four_freq_suff' 'the_five_freq_suff' 'the_bi_freq_pref'
 'the_three_freq_pref' 'the_four_freq_pref' 'the_five_freq_pref'
 'the_sn_gram_freq' 'a_the_bi_freq_suff_p1' 'the_a_bi_freq_suff_p1'
 'a_the_three_freq_suff_p1' 'the_a_three_freq_suff_p1'
 'a_the_four_freq_suff_p1' 'the_a_four_freq_suff_p1'
 'a_the_five_freq_suff_p1' 'the_a_five_freq_suff_p1'
 'a_the_bi_freq_pref_p1' 'the_a_bi_freq_pref_p1' 'a_the_three_freq_pref_p1'
 'the_a_three_freq_pref_p1' 'a_the_four_freq_pref_p1'
 'the_a_four_freq_pref_p1' 'a_the_five_freq_pref_p1'
 'the_a_five_freq_pref_p1' 'a_the_sn_ngram_freq_p1'
 'the_a_sn_ngram_freq_p1' 'a_the_bi_freq_suff_p10' 'the_a_bi_freq_suff_p10'
 'a_the_three_freq_suff_p10' 'the_a_three_freq_suff_p10'
 'a_the_four_freq_suff_p10' 'the_a_four_freq_suff_p10'
 'a_the_five_freq_suff_p10' 'the_a_five_freq_suff_p10'
 'a_the_bi_freq_pref_p10' 'the_a_bi_freq_pref_p10'
 'a_the_three_freq_pref_p10' 'the_a_three_freq_pref_p10'
 'a_the_four_freq_pref_p10' 'the_a_four_freq_pref_p10'
 'a_the_five_freq_pref_p10' 'the_a_five_freq_pref_p10'
 'a_the_sn_ngram_freq_p10' 'the_a_sn_ngram_freq_p10'
 'a_the_bi_freq_suff_p100' 'the_a_bi_freq_suff_p100'
 'a_the_three_freq_suff_p100' 'the_a_three_freq_suff_p100'
 'a_the_four_freq_suff_p100' 'the_a_four_freq_suff_p100'
 'a_the_five_freq_suff_p100' 'the_a_five_freq_suff_p100'
 'a_the_bi_freq_pref_p100' 'the_a_bi_freq_pref_p100'
 'a_the_three_freq_pref_p100' 'the_a_three_freq_pref_p100'
 'a_the_four_freq_pref_p100' 'the_a_four_freq_pref_p100'
 'a_the_five_freq_pref_p100' 'the_a_five_freq_pref_p100'
 'a_the_sn_ngram_freq_p100' 'the_a_sn_ngram_freq_p100' 'st_with_v'
 'article']
[0]	validation_0-mlogloss:0.994256	validation_1-mlogloss:0.994553
Multiple eval metrics have been passed: 'validation_1-mlogloss' will be used for early stopping.
Will train until validation_1-mlogloss hasn't improved in 50 rounds.
[1]	validation_0-mlogloss:0.905536	validation_1-mlogloss:0.906032
[2]	validation_0-mlogloss:0.829067	validation_1-mlogloss:0.829928
[3]	validation_0-mlogloss:0.763336	validation_1-mlogloss:0.764602
[4]	validation_0-mlogloss:0.706338	validation_1-mlogloss:0.707997
[5]	validation_0-mlogloss:0.656783	validation_1-mlogloss:0.658627
[6]	validation_0-mlogloss:0.61328	validation_1-mlogloss:0.615521
[7]	validation_0-mlogloss:0.574995	validation_1-mlogloss:0.57756
[8]	validation_0-mlogloss:0.542881	validation_1-mlogloss:0.545963
[9]	validation_0-mlogloss:0.514121	validation_1-mlogloss:0.51748
[10]	validation_0-mlogloss:0.488396	validation_1-mlogloss:0.49214
[11]	validation_0-mlogloss:0.464199	validation_1-mlogloss:0.468285
[12]	validation_0-mlogloss:0.442381	validation_1-mlogloss:0.446706
[13]	validation_0-mlogloss:0.422975	validation_1-mlogloss:0.427517
[14]	validation_0-mlogloss:0.405776	validation_1-mlogloss:0.410573
[15]	validation_0-mlogloss:0.390151	validation_1-mlogloss:0.395224
[16]	validation_0-mlogloss:0.375902	validation_1-mlogloss:0.381182
[17]	validation_0-mlogloss:0.363292	validation_1-mlogloss:0.368819
[18]	validation_0-mlogloss:0.3516	validation_1-mlogloss:0.357416
[19]	validation_0-mlogloss:0.341727	validation_1-mlogloss:0.347815
[20]	validation_0-mlogloss:0.332167	validation_1-mlogloss:0.338486
[21]	validation_0-mlogloss:0.323521	validation_1-mlogloss:0.330113
[22]	validation_0-mlogloss:0.316302	validation_1-mlogloss:0.323158
[23]	validation_0-mlogloss:0.308995	validation_1-mlogloss:0.316004
[24]	validation_0-mlogloss:0.30233	validation_1-mlogloss:0.309592
[25]	validation_0-mlogloss:0.296875	validation_1-mlogloss:0.304318
[26]	validation_0-mlogloss:0.291309	validation_1-mlogloss:0.298956
[27]	validation_0-mlogloss:0.286702	validation_1-mlogloss:0.294626
[28]	validation_0-mlogloss:0.282438	validation_1-mlogloss:0.290649
[29]	validation_0-mlogloss:0.27811	validation_1-mlogloss:0.286582
[30]	validation_0-mlogloss:0.274099	validation_1-mlogloss:0.282771
[31]	validation_0-mlogloss:0.27093	validation_1-mlogloss:0.27973
[32]	validation_0-mlogloss:0.267459	validation_1-mlogloss:0.276573
[33]	validation_0-mlogloss:0.26426	validation_1-mlogloss:0.27359
[34]	validation_0-mlogloss:0.261242	validation_1-mlogloss:0.270854
[35]	validation_0-mlogloss:0.258553	validation_1-mlogloss:0.268358
[36]	validation_0-mlogloss:0.255917	validation_1-mlogloss:0.266057
[37]	validation_0-mlogloss:0.253556	validation_1-mlogloss:0.263914
[38]	validation_0-mlogloss:0.251369	validation_1-mlogloss:0.26194
[39]	validation_0-mlogloss:0.24935	validation_1-mlogloss:0.260116
[40]	validation_0-mlogloss:0.247361	validation_1-mlogloss:0.258431
[41]	validation_0-mlogloss:0.245518	validation_1-mlogloss:0.256865
[42]	validation_0-mlogloss:0.243813	validation_1-mlogloss:0.255422
[43]	validation_0-mlogloss:0.242466	validation_1-mlogloss:0.254373
[44]	validation_0-mlogloss:0.240937	validation_1-mlogloss:0.253186
[45]	validation_0-mlogloss:0.239538	validation_1-mlogloss:0.251982
[46]	validation_0-mlogloss:0.238184	validation_1-mlogloss:0.250893
[47]	validation_0-mlogloss:0.236855	validation_1-mlogloss:0.249877
[48]	validation_0-mlogloss:0.23561	validation_1-mlogloss:0.248997
[49]	validation_0-mlogloss:0.234497	validation_1-mlogloss:0.248127
[50]	validation_0-mlogloss:0.233303	validation_1-mlogloss:0.247241
[51]	validation_0-mlogloss:0.232489	validation_1-mlogloss:0.24671
[52]	validation_0-mlogloss:0.231682	validation_1-mlogloss:0.246075
[53]	validation_0-mlogloss:0.23101	validation_1-mlogloss:0.245533
[54]	validation_0-mlogloss:0.23001	validation_1-mlogloss:0.24488
[55]	validation_0-mlogloss:0.228992	validation_1-mlogloss:0.244274
[56]	validation_0-mlogloss:0.228133	validation_1-mlogloss:0.243704
[57]	validation_0-mlogloss:0.227376	validation_1-mlogloss:0.243221
[58]	validation_0-mlogloss:0.226547	validation_1-mlogloss:0.242712
[59]	validation_0-mlogloss:0.225826	validation_1-mlogloss:0.242215
[60]	validation_0-mlogloss:0.225225	validation_1-mlogloss:0.241882
[61]	validation_0-mlogloss:0.224642	validation_1-mlogloss:0.24155
[62]	validation_0-mlogloss:0.223722	validation_1-mlogloss:0.241053
[63]	validation_0-mlogloss:0.222886	validation_1-mlogloss:0.240654
[64]	validation_0-mlogloss:0.222344	validation_1-mlogloss:0.240338
[65]	validation_0-mlogloss:0.221546	validation_1-mlogloss:0.239963
[66]	validation_0-mlogloss:0.22082	validation_1-mlogloss:0.239623
[67]	validation_0-mlogloss:0.220335	validation_1-mlogloss:0.239368
[68]	validation_0-mlogloss:0.21957	validation_1-mlogloss:0.239138
[69]	validation_0-mlogloss:0.218804	validation_1-mlogloss:0.238783
[70]	validation_0-mlogloss:0.218154	validation_1-mlogloss:0.238484
[71]	validation_0-mlogloss:0.217632	validation_1-mlogloss:0.238203
[72]	validation_0-mlogloss:0.217105	validation_1-mlogloss:0.23801
[73]	validation_0-mlogloss:0.216625	validation_1-mlogloss:0.237792
[74]	validation_0-mlogloss:0.216059	validation_1-mlogloss:0.237672
[75]	validation_0-mlogloss:0.215442	validation_1-mlogloss:0.237519
[76]	validation_0-mlogloss:0.214999	validation_1-mlogloss:0.237346
[77]	validation_0-mlogloss:0.214476	validation_1-mlogloss:0.237171
[78]	validation_0-mlogloss:0.213824	validation_1-mlogloss:0.23694
[79]	validation_0-mlogloss:0.213117	validation_1-mlogloss:0.236648
[80]	validation_0-mlogloss:0.212462	validation_1-mlogloss:0.236596
[81]	validation_0-mlogloss:0.212068	validation_1-mlogloss:0.236389
[82]	validation_0-mlogloss:0.211561	validation_1-mlogloss:0.236299
[83]	validation_0-mlogloss:0.211081	validation_1-mlogloss:0.236153
[84]	validation_0-mlogloss:0.210653	validation_1-mlogloss:0.236014
[85]	validation_0-mlogloss:0.210223	validation_1-mlogloss:0.235857
[86]	validation_0-mlogloss:0.209631	validation_1-mlogloss:0.235734
[87]	validation_0-mlogloss:0.208871	validation_1-mlogloss:0.235455
[88]	validation_0-mlogloss:0.208567	validation_1-mlogloss:0.235361
[89]	validation_0-mlogloss:0.208167	validation_1-mlogloss:0.23524
[90]	validation_0-mlogloss:0.207569	validation_1-mlogloss:0.235114
[91]	validation_0-mlogloss:0.207064	validation_1-mlogloss:0.235
[92]	validation_0-mlogloss:0.20664	validation_1-mlogloss:0.23485
[93]	validation_0-mlogloss:0.206171	validation_1-mlogloss:0.234785
[94]	validation_0-mlogloss:0.205866	validation_1-mlogloss:0.234808
[95]	validation_0-mlogloss:0.205519	validation_1-mlogloss:0.234752
[96]	validation_0-mlogloss:0.20514	validation_1-mlogloss:0.234732
[97]	validation_0-mlogloss:0.204621	validation_1-mlogloss:0.234706
[98]	validation_0-mlogloss:0.20421	validation_1-mlogloss:0.234624
[99]	validation_0-mlogloss:0.203735	validation_1-mlogloss:0.234566
[100]	validation_0-mlogloss:0.203336	validation_1-mlogloss:0.234519
[101]	validation_0-mlogloss:0.202932	validation_1-mlogloss:0.234476
[102]	validation_0-mlogloss:0.202584	validation_1-mlogloss:0.234385
[103]	validation_0-mlogloss:0.202132	validation_1-mlogloss:0.234347
[104]	validation_0-mlogloss:0.20183	validation_1-mlogloss:0.234325
[105]	validation_0-mlogloss:0.201559	validation_1-mlogloss:0.23425
[106]	validation_0-mlogloss:0.201109	validation_1-mlogloss:0.234221
[107]	validation_0-mlogloss:0.200718	validation_1-mlogloss:0.234174
[108]	validation_0-mlogloss:0.200391	validation_1-mlogloss:0.234118
[109]	validation_0-mlogloss:0.200163	validation_1-mlogloss:0.234033
[110]	validation_0-mlogloss:0.199825	validation_1-mlogloss:0.233967
[111]	validation_0-mlogloss:0.199325	validation_1-mlogloss:0.233831
[112]	validation_0-mlogloss:0.198933	validation_1-mlogloss:0.23378
[113]	validation_0-mlogloss:0.198538	validation_1-mlogloss:0.233757
[114]	validation_0-mlogloss:0.198312	validation_1-mlogloss:0.233746
[115]	validation_0-mlogloss:0.197859	validation_1-mlogloss:0.233737
[116]	validation_0-mlogloss:0.197338	validation_1-mlogloss:0.233667
[117]	validation_0-mlogloss:0.196902	validation_1-mlogloss:0.23362
[118]	validation_0-mlogloss:0.196491	validation_1-mlogloss:0.233582
[119]	validation_0-mlogloss:0.195949	validation_1-mlogloss:0.233495
[120]	validation_0-mlogloss:0.195444	validation_1-mlogloss:0.23342
[121]	validation_0-mlogloss:0.195165	validation_1-mlogloss:0.233328
[122]	validation_0-mlogloss:0.194583	validation_1-mlogloss:0.233265
[123]	validation_0-mlogloss:0.194095	validation_1-mlogloss:0.233219
[124]	validation_0-mlogloss:0.193651	validation_1-mlogloss:0.233216
[125]	validation_0-mlogloss:0.193142	validation_1-mlogloss:0.233197
[126]	validation_0-mlogloss:0.19272	validation_1-mlogloss:0.233156
[127]	validation_0-mlogloss:0.192277	validation_1-mlogloss:0.233076
[128]	validation_0-mlogloss:0.191821	validation_1-mlogloss:0.232997
[129]	validation_0-mlogloss:0.191608	validation_1-mlogloss:0.232998
[130]	validation_0-mlogloss:0.19125	validation_1-mlogloss:0.232972
[131]	validation_0-mlogloss:0.190894	validation_1-mlogloss:0.232972
[132]	validation_0-mlogloss:0.19055	validation_1-mlogloss:0.232951
[133]	validation_0-mlogloss:0.19011	validation_1-mlogloss:0.232926
[134]	validation_0-mlogloss:0.189839	validation_1-mlogloss:0.232934
[135]	validation_0-mlogloss:0.189332	validation_1-mlogloss:0.232856
[136]	validation_0-mlogloss:0.189168	validation_1-mlogloss:0.232862
[137]	validation_0-mlogloss:0.18884	validation_1-mlogloss:0.232867
[138]	validation_0-mlogloss:0.188503	validation_1-mlogloss:0.232848
[139]	validation_0-mlogloss:0.188284	validation_1-mlogloss:0.232842
[140]	validation_0-mlogloss:0.187915	validation_1-mlogloss:0.232827
[141]	validation_0-mlogloss:0.187538	validation_1-mlogloss:0.232786
[142]	validation_0-mlogloss:0.187192	validation_1-mlogloss:0.232797
[143]	validation_0-mlogloss:0.186734	validation_1-mlogloss:0.232862
[144]	validation_0-mlogloss:0.186412	validation_1-mlogloss:0.232899
[145]	validation_0-mlogloss:0.18605	validation_1-mlogloss:0.232905
[146]	validation_0-mlogloss:0.185511	validation_1-mlogloss:0.23292
[147]	validation_0-mlogloss:0.185206	validation_1-mlogloss:0.232833
[148]	validation_0-mlogloss:0.184854	validation_1-mlogloss:0.232862
[149]	validation_0-mlogloss:0.184585	validation_1-mlogloss:0.232831
[150]	validation_0-mlogloss:0.184099	validation_1-mlogloss:0.232777
[151]	validation_0-mlogloss:0.183658	validation_1-mlogloss:0.232771
[152]	validation_0-mlogloss:0.18334	validation_1-mlogloss:0.232815
[153]	validation_0-mlogloss:0.183038	validation_1-mlogloss:0.232797
[154]	validation_0-mlogloss:0.182609	validation_1-mlogloss:0.232786
[155]	validation_0-mlogloss:0.182363	validation_1-mlogloss:0.232815
[156]	validation_0-mlogloss:0.181948	validation_1-mlogloss:0.232803
[157]	validation_0-mlogloss:0.181649	validation_1-mlogloss:0.232796
[158]	validation_0-mlogloss:0.181425	validation_1-mlogloss:0.232793
[159]	validation_0-mlogloss:0.180959	validation_1-mlogloss:0.232813
[160]	validation_0-mlogloss:0.180522	validation_1-mlogloss:0.232764
[161]	validation_0-mlogloss:0.180051	validation_1-mlogloss:0.232694
[162]	validation_0-mlogloss:0.179766	validation_1-mlogloss:0.232697
[163]	validation_0-mlogloss:0.179467	validation_1-mlogloss:0.232666
[164]	validation_0-mlogloss:0.17915	validation_1-mlogloss:0.232636
[165]	validation_0-mlogloss:0.178765	validation_1-mlogloss:0.232592
[166]	validation_0-mlogloss:0.178521	validation_1-mlogloss:0.23259
[167]	validation_0-mlogloss:0.178132	validation_1-mlogloss:0.232566
[168]	validation_0-mlogloss:0.17781	validation_1-mlogloss:0.232561
[169]	validation_0-mlogloss:0.177522	validation_1-mlogloss:0.232559
[170]	validation_0-mlogloss:0.177256	validation_1-mlogloss:0.232547
[171]	validation_0-mlogloss:0.176862	validation_1-mlogloss:0.23261
[172]	validation_0-mlogloss:0.176646	validation_1-mlogloss:0.232563
[173]	validation_0-mlogloss:0.176219	validation_1-mlogloss:0.232511
[174]	validation_0-mlogloss:0.175787	validation_1-mlogloss:0.232563
[175]	validation_0-mlogloss:0.175515	validation_1-mlogloss:0.232633
[176]	validation_0-mlogloss:0.175104	validation_1-mlogloss:0.232664
[177]	validation_0-mlogloss:0.174789	validation_1-mlogloss:0.232603
[178]	validation_0-mlogloss:0.174419	validation_1-mlogloss:0.232607
[179]	validation_0-mlogloss:0.174095	validation_1-mlogloss:0.232619
[180]	validation_0-mlogloss:0.173839	validation_1-mlogloss:0.232634
[181]	validation_0-mlogloss:0.1736	validation_1-mlogloss:0.232628
[182]	validation_0-mlogloss:0.173313	validation_1-mlogloss:0.23265
[183]	validation_0-mlogloss:0.173028	validation_1-mlogloss:0.232677
[184]	validation_0-mlogloss:0.172658	validation_1-mlogloss:0.232683
[185]	validation_0-mlogloss:0.172297	validation_1-mlogloss:0.232619
[186]	validation_0-mlogloss:0.171924	validation_1-mlogloss:0.232663
[187]	validation_0-mlogloss:0.171583	validation_1-mlogloss:0.232638
[188]	validation_0-mlogloss:0.171199	validation_1-mlogloss:0.232574
[189]	validation_0-mlogloss:0.170948	validation_1-mlogloss:0.232552
[190]	validation_0-mlogloss:0.170603	validation_1-mlogloss:0.232553
[191]	validation_0-mlogloss:0.170257	validation_1-mlogloss:0.232518
[192]	validation_0-mlogloss:0.169925	validation_1-mlogloss:0.232474
[193]	validation_0-mlogloss:0.169592	validation_1-mlogloss:0.232432
[194]	validation_0-mlogloss:0.169286	validation_1-mlogloss:0.232378
[195]	validation_0-mlogloss:0.168901	validation_1-mlogloss:0.232354
[196]	validation_0-mlogloss:0.168537	validation_1-mlogloss:0.232322
[197]	validation_0-mlogloss:0.168179	validation_1-mlogloss:0.232325
[198]	validation_0-mlogloss:0.167974	validation_1-mlogloss:0.232348
[199]	validation_0-mlogloss:0.167687	validation_1-mlogloss:0.232362
[200]	validation_0-mlogloss:0.16737	validation_1-mlogloss:0.232422
[201]	validation_0-mlogloss:0.167059	validation_1-mlogloss:0.232433
[202]	validation_0-mlogloss:0.166852	validation_1-mlogloss:0.232475
[203]	validation_0-mlogloss:0.166629	validation_1-mlogloss:0.23245
[204]	validation_0-mlogloss:0.166348	validation_1-mlogloss:0.232511
[205]	validation_0-mlogloss:0.166106	validation_1-mlogloss:0.232501
[206]	validation_0-mlogloss:0.165929	validation_1-mlogloss:0.232525
[207]	validation_0-mlogloss:0.165577	validation_1-mlogloss:0.232558
[208]	validation_0-mlogloss:0.165226	validation_1-mlogloss:0.23258
[209]	validation_0-mlogloss:0.164927	validation_1-mlogloss:0.232568
[210]	validation_0-mlogloss:0.164551	validation_1-mlogloss:0.232543
[211]	validation_0-mlogloss:0.164249	validation_1-mlogloss:0.232537
[212]	validation_0-mlogloss:0.16394	validation_1-mlogloss:0.232506
[213]	validation_0-mlogloss:0.163627	validation_1-mlogloss:0.232511
[214]	validation_0-mlogloss:0.163291	validation_1-mlogloss:0.232504
[215]	validation_0-mlogloss:0.162931	validation_1-mlogloss:0.232476
[216]	validation_0-mlogloss:0.16264	validation_1-mlogloss:0.23254
[217]	validation_0-mlogloss:0.162397	validation_1-mlogloss:0.232566
[218]	validation_0-mlogloss:0.162163	validation_1-mlogloss:0.23253
[219]	validation_0-mlogloss:0.161929	validation_1-mlogloss:0.232551
[220]	validation_0-mlogloss:0.161503	validation_1-mlogloss:0.232554
[221]	validation_0-mlogloss:0.161299	validation_1-mlogloss:0.232577
[222]	validation_0-mlogloss:0.160919	validation_1-mlogloss:0.232625
[223]	validation_0-mlogloss:0.160663	validation_1-mlogloss:0.232616
[224]	validation_0-mlogloss:0.160409	validation_1-mlogloss:0.23262
[225]	validation_0-mlogloss:0.160175	validation_1-mlogloss:0.232595
[226]	validation_0-mlogloss:0.159872	validation_1-mlogloss:0.232643
[227]	validation_0-mlogloss:0.159561	validation_1-mlogloss:0.232614
[228]	validation_0-mlogloss:0.159336	validation_1-mlogloss:0.232646
[229]	validation_0-mlogloss:0.159121	validation_1-mlogloss:0.232673
[230]	validation_0-mlogloss:0.158885	validation_1-mlogloss:0.232673
[231]	validation_0-mlogloss:0.158667	validation_1-mlogloss:0.232682
[232]	validation_0-mlogloss:0.158303	validation_1-mlogloss:0.232724
[233]	validation_0-mlogloss:0.15812	validation_1-mlogloss:0.232669
[234]	validation_0-mlogloss:0.157841	validation_1-mlogloss:0.232669
[235]	validation_0-mlogloss:0.157661	validation_1-mlogloss:0.232652
[236]	validation_0-mlogloss:0.157282	validation_1-mlogloss:0.232718
[237]	validation_0-mlogloss:0.157031	validation_1-mlogloss:0.232735
[238]	validation_0-mlogloss:0.156664	validation_1-mlogloss:0.232694
[239]	validation_0-mlogloss:0.15643	validation_1-mlogloss:0.232707
[240]	validation_0-mlogloss:0.156193	validation_1-mlogloss:0.232786
[241]	validation_0-mlogloss:0.155842	validation_1-mlogloss:0.232741
[242]	validation_0-mlogloss:0.155689	validation_1-mlogloss:0.232751
[243]	validation_0-mlogloss:0.155426	validation_1-mlogloss:0.232776
[244]	validation_0-mlogloss:0.155151	validation_1-mlogloss:0.232778
[245]	validation_0-mlogloss:0.154823	validation_1-mlogloss:0.232809
[246]	validation_0-mlogloss:0.154544	validation_1-mlogloss:0.232777
Stopping. Best iteration:
[196]	validation_0-mlogloss:0.168537	validation_1-mlogloss:0.232322
[0, 1, 2]
0.232777242108
submit_xgb_out_of_fold_pred(df.copy())
35673 17890
['a_bi_freq_suff' 'a_three_freq_suff' 'a_four_freq_suff' 'a_five_freq_suff'
 'a_bi_freq_pref' 'a_three_freq_pref' 'a_four_freq_pref' 'a_five_freq_pref'
 'a_sn_gram_freq' 'the_bi_freq_suff' 'the_three_freq_suff'
 'the_four_freq_suff' 'the_five_freq_suff' 'the_bi_freq_pref'
 'the_three_freq_pref' 'the_four_freq_pref' 'the_five_freq_pref'
 'the_sn_gram_freq' 'a_the_bi_freq_suff_p1' 'the_a_bi_freq_suff_p1'
 'a_the_three_freq_suff_p1' 'the_a_three_freq_suff_p1'
 'a_the_four_freq_suff_p1' 'the_a_four_freq_suff_p1'
 'a_the_five_freq_suff_p1' 'the_a_five_freq_suff_p1'
 'a_the_bi_freq_pref_p1' 'the_a_bi_freq_pref_p1' 'a_the_three_freq_pref_p1'
 'the_a_three_freq_pref_p1' 'a_the_four_freq_pref_p1'
 'the_a_four_freq_pref_p1' 'a_the_five_freq_pref_p1'
 'the_a_five_freq_pref_p1' 'a_the_sn_ngram_freq_p1'
 'the_a_sn_ngram_freq_p1' 'a_the_bi_freq_suff_p10' 'the_a_bi_freq_suff_p10'
 'a_the_three_freq_suff_p10' 'the_a_three_freq_suff_p10'
 'a_the_four_freq_suff_p10' 'the_a_four_freq_suff_p10'
 'a_the_five_freq_suff_p10' 'the_a_five_freq_suff_p10'
 'a_the_bi_freq_pref_p10' 'the_a_bi_freq_pref_p10'
 'a_the_three_freq_pref_p10' 'the_a_three_freq_pref_p10'
 'a_the_four_freq_pref_p10' 'the_a_four_freq_pref_p10'
 'a_the_five_freq_pref_p10' 'the_a_five_freq_pref_p10'
 'a_the_sn_ngram_freq_p10' 'the_a_sn_ngram_freq_p10'
 'a_the_bi_freq_suff_p100' 'the_a_bi_freq_suff_p100'
 'a_the_three_freq_suff_p100' 'the_a_three_freq_suff_p100'
 'a_the_four_freq_suff_p100' 'the_a_four_freq_suff_p100'
 'a_the_five_freq_suff_p100' 'the_a_five_freq_suff_p100'
 'a_the_bi_freq_pref_p100' 'the_a_bi_freq_pref_p100'
 'a_the_three_freq_pref_p100' 'the_a_three_freq_pref_p100'
 'a_the_four_freq_pref_p100' 'the_a_four_freq_pref_p100'
 'a_the_five_freq_pref_p100' 'the_a_five_freq_pref_p100'
 'a_the_sn_ngram_freq_p100' 'the_a_sn_ngram_freq_p100' 'st_with_v'
 'article']
[0, 1, 2]
/home/dpetrovskyi/pycharm/pycharm-2016.3.2/helpers/pydev/pydevconsole.py:348: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead
See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy

0.238656636068
35672 17891
['a_bi_freq_suff' 'a_three_freq_suff' 'a_four_freq_suff' 'a_five_freq_suff'
 'a_bi_freq_pref' 'a_three_freq_pref' 'a_four_freq_pref' 'a_five_freq_pref'
 'a_sn_gram_freq' 'the_bi_freq_suff' 'the_three_freq_suff'
 'the_four_freq_suff' 'the_five_freq_suff' 'the_bi_freq_pref'
 'the_three_freq_pref' 'the_four_freq_pref' 'the_five_freq_pref'
 'the_sn_gram_freq' 'a_the_bi_freq_suff_p1' 'the_a_bi_freq_suff_p1'
 'a_the_three_freq_suff_p1' 'the_a_three_freq_suff_p1'
 'a_the_four_freq_suff_p1' 'the_a_four_freq_suff_p1'
 'a_the_five_freq_suff_p1' 'the_a_five_freq_suff_p1'
 'a_the_bi_freq_pref_p1' 'the_a_bi_freq_pref_p1' 'a_the_three_freq_pref_p1'
 'the_a_three_freq_pref_p1' 'a_the_four_freq_pref_p1'
 'the_a_four_freq_pref_p1' 'a_the_five_freq_pref_p1'
 'the_a_five_freq_pref_p1' 'a_the_sn_ngram_freq_p1'
 'the_a_sn_ngram_freq_p1' 'a_the_bi_freq_suff_p10' 'the_a_bi_freq_suff_p10'
 'a_the_three_freq_suff_p10' 'the_a_three_freq_suff_p10'
 'a_the_four_freq_suff_p10' 'the_a_four_freq_suff_p10'
 'a_the_five_freq_suff_p10' 'the_a_five_freq_suff_p10'
 'a_the_bi_freq_pref_p10' 'the_a_bi_freq_pref_p10'
 'a_the_three_freq_pref_p10' 'the_a_three_freq_pref_p10'
 'a_the_four_freq_pref_p10' 'the_a_four_freq_pref_p10'
 'a_the_five_freq_pref_p10' 'the_a_five_freq_pref_p10'
 'a_the_sn_ngram_freq_p10' 'the_a_sn_ngram_freq_p10'
 'a_the_bi_freq_suff_p100' 'the_a_bi_freq_suff_p100'
 'a_the_three_freq_suff_p100' 'the_a_three_freq_suff_p100'
 'a_the_four_freq_suff_p100' 'the_a_four_freq_suff_p100'
 'a_the_five_freq_suff_p100' 'the_a_five_freq_suff_p100'
 'a_the_bi_freq_pref_p100' 'the_a_bi_freq_pref_p100'
 'a_the_three_freq_pref_p100' 'the_a_three_freq_pref_p100'
 'a_the_four_freq_pref_p100' 'the_a_four_freq_pref_p100'
 'a_the_five_freq_pref_p100' 'the_a_five_freq_pref_p100'
 'a_the_sn_ngram_freq_p100' 'the_a_sn_ngram_freq_p100' 'st_with_v'
 'article']
[0, 1, 2]
0.239201084451
35781 17782
['a_bi_freq_suff' 'a_three_freq_suff' 'a_four_freq_suff' 'a_five_freq_suff'
 'a_bi_freq_pref' 'a_three_freq_pref' 'a_four_freq_pref' 'a_five_freq_pref'
 'a_sn_gram_freq' 'the_bi_freq_suff' 'the_three_freq_suff'
 'the_four_freq_suff' 'the_five_freq_suff' 'the_bi_freq_pref'
 'the_three_freq_pref' 'the_four_freq_pref' 'the_five_freq_pref'
 'the_sn_gram_freq' 'a_the_bi_freq_suff_p1' 'the_a_bi_freq_suff_p1'
 'a_the_three_freq_suff_p1' 'the_a_three_freq_suff_p1'
 'a_the_four_freq_suff_p1' 'the_a_four_freq_suff_p1'
 'a_the_five_freq_suff_p1' 'the_a_five_freq_suff_p1'
 'a_the_bi_freq_pref_p1' 'the_a_bi_freq_pref_p1' 'a_the_three_freq_pref_p1'
 'the_a_three_freq_pref_p1' 'a_the_four_freq_pref_p1'
 'the_a_four_freq_pref_p1' 'a_the_five_freq_pref_p1'
 'the_a_five_freq_pref_p1' 'a_the_sn_ngram_freq_p1'
 'the_a_sn_ngram_freq_p1' 'a_the_bi_freq_suff_p10' 'the_a_bi_freq_suff_p10'
 'a_the_three_freq_suff_p10' 'the_a_three_freq_suff_p10'
 'a_the_four_freq_suff_p10' 'the_a_four_freq_suff_p10'
 'a_the_five_freq_suff_p10' 'the_a_five_freq_suff_p10'
 'a_the_bi_freq_pref_p10' 'the_a_bi_freq_pref_p10'
 'a_the_three_freq_pref_p10' 'the_a_three_freq_pref_p10'
 'a_the_four_freq_pref_p10' 'the_a_four_freq_pref_p10'
 'a_the_five_freq_pref_p10' 'the_a_five_freq_pref_p10'
 'a_the_sn_ngram_freq_p10' 'the_a_sn_ngram_freq_p10'
 'a_the_bi_freq_suff_p100' 'the_a_bi_freq_suff_p100'
 'a_the_three_freq_suff_p100' 'the_a_three_freq_suff_p100'
 'a_the_four_freq_suff_p100' 'the_a_four_freq_suff_p100'
 'a_the_five_freq_suff_p100' 'the_a_five_freq_suff_p100'
 'a_the_bi_freq_pref_p100' 'the_a_bi_freq_pref_p100'
 'a_the_three_freq_pref_p100' 'the_a_three_freq_pref_p100'
 'a_the_four_freq_pref_p100' 'the_a_four_freq_pref_p100'
 'a_the_five_freq_pref_p100' 'the_a_five_freq_pref_p100'
 'a_the_sn_ngram_freq_p100' 'the_a_sn_ngram_freq_p100' 'st_with_v'
 'article']
[0, 1, 2]
0.232619411437
36000 36000 36000
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py", line 2881, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-5-0d375cecf64d>", line 1, in <module>
    submit_xgb_out_of_fold_pred(df.copy())
  File "<ipython-input-2-8c00a450a9cf>", line 300, in submit_xgb_out_of_fold_pred
    stats = submit_train(df)
  File "<ipython-input-2-8c00a450a9cf>", line 24, in submit_train
    return evaluate(train_arr, corrections, submit_arr)
  File "<ipython-input-2-8c00a450a9cf>", line 35, in evaluate
    for sent, cor, sub in izip_longest(text, correct, submission):
NameError: global name 'izip_longest' is not defined
from itertools import izip_longest
from sklearn.metrics import log_loss
from preprocessing import *
import pandas as pd
import xgboost as xgb
import matplotlib.pyplot as plt
art_map = {'a':0, 'an':1, 'the':2}
inverse_art_map={0:'a', 1:'an', 2:'the'}
def is_indefinite_article(a):
    return a in {'a', 'an'}
def is_definite_article(a):
    return a == 'the'
def submit_train(df):
    train_arr = load_train_arr()
    corrections = load_resource(fp_corrections_train)
    submit_arr = create_submission_arr(df, train_arr)
    return evaluate(train_arr, corrections, submit_arr)
def evaluate(text, correct, submission):
    print len(text), len(correct), len(submission)
    # with open(text_file) as f:
    #     text = json.load(f)
    # with open(correct_file) as f:
    #     correct = json.load(f)
    # with open(submission_file) as f:
    #     submission = json.load(f)
    data = []
    for sent, cor, sub in izip_longest(text, correct, submission):
        for w, c, s in izip_longest(sent, cor, sub):
            if w in ['a', 'an', 'the']:
                tel = None if s is None else s[2]
                if s is None or s[0] == w:
                    s = ['', float('-inf')]
                # -score, ok-prediction, is_realy_error
                data.append((-s[1], s[0] == c, c is not None, tel))
                # -1, -0.8, -0.2, ... inf, inf
    print 0.02 * len(data)
    data.sort(key=lambda x: x[0])
    fp2 = 0
    fp = 0
    tp = 0
    all_mistakes = sum(x[2] for x in data)  # num of ALL incorrect
    print 'all_mistakes {}'.format(all_mistakes)
    score = 0
    acc = 0
    counter = 0
    stats = {'w': [], 'r': []}
    stop = False
    for _, c, r, tel in data:
        fp2 += not c  # wrong correction
        fp += not r  # realy errors count
        tp += c  # right correction
        if not stop:
            if not c:
                stats['w'].append(tel)
            if c:
                stats['r'].append(tel)
        acc = max(acc, 1 - (0. + fp + all_mistakes - tp) / len(data))
        if fp2 * 1. / len(data) <= 0.02:
            score = tp * 1. / all_mistakes
        else:
            stop = True
    print 'target score = %.2f %%' % (score * 100)
    print 'accuracy (just for info) = %.2f %%' % (acc * 100)
    return stats
def create_submission_arr(df, sents):
    arr = [[None] * (len(x)) for x in sents]
    cols = df.columns
    def do_work(row):
        if row[correction] is not None:
            corr = row[correction]
            conf = row[confidence]
            sent_index = row[sentence_index]
            pos = row[position]
            arr[sent_index][pos] = (corr, conf, OrderedDict([(c,row[c]) for c in cols]))
    df.apply(do_work, axis=1)
    return arr
def arr_to_df(arr):
    cols = arr[0].keys()
    m = OrderedDict((c, [None if x is None else x[c] for x in arr]) for c in cols)
    df= pd.DataFrame(m)
    df['trans'] = df[article]+'->'+df[correction].apply(str)#.apply(lambda s: inverse_art_map[s])
    # add_short_freq_cols(df)
    cols = ['trans', confidence]
    return df[cols]
def to_xgb_df(df):
    df[article]=df[article].apply(lambda s: art_map[s])
    df[correct_article]=df[correct_article].apply(lambda s: art_map[s])
    def normalized_ratio(row, col1, col2, prior):
        a = row[col1]
        b = row[col2]
        if a is None:
            return None
        return (float(prior) + a) / (float(prior) + b)
    def normalized_ratio1(row, col1, col2, prior):
        a = row[col1]
        b = row[col2]
        if a is None:
            return None
        return (float(prior) + a) / (2*float(prior) + b+a)
    def create_normalized_ratio_col(d, col1, col2, new_col, prior):
        d[new_col] = d.apply(lambda row: normalized_ratio(row, col1, col2, prior), axis=1)
    def create_normalized_ratio_col1(d, col1, col2, new_col, prior):
        d[new_col] = d.apply(lambda row: normalized_ratio1(row, col1, col2, prior), axis=1)
    col_pairs=[
        (a_bi_freq_suff, the_bi_freq_suff, 'bi_freq_suff'),
        (a_three_freq_suff, the_three_freq_suff, 'three_freq_suff'),
        (a_four_freq_suff, the_four_freq_suff, 'four_freq_suff'),
        (a_five_freq_suff, the_five_freq_suff, 'five_freq_suff'),
        (a_bi_freq_pref, the_bi_freq_pref, 'bi_freq_pref'),
        (a_three_freq_pref, the_three_freq_pref, 'three_freq_pref'),
        (a_four_freq_pref, the_four_freq_pref, 'four_freq_pref'),
        (a_five_freq_pref, the_five_freq_pref, 'five_freq_pref'),
        (a_sn_gram_freq, the_sn_gram_freq, 'sn_ngram_freq')
    ]
    frequencies_cols = [x[0] for x in col_pairs]+[x[1] for x in col_pairs]
    # frequencies_cols=[]
    for prior in [1, 10, 100]:#
        for a_col, the_col, name in col_pairs:
            new_col = 'a_the_{}_p{}'.format(name, prior)
            create_normalized_ratio_col(df, a_col, the_col, new_col, prior)
            frequencies_cols.append(new_col)
            new_col = 'the_a_{}_p{}'.format(name, prior)
            create_normalized_ratio_col(df, the_col, a_col, new_col, prior)
            frequencies_cols.append(new_col)
    # cols_to_exclude = [
    #     'the_bi_freq_pref', 'the_three_freq_pref',
    #     'the_four_freq_pref' ,'the_five_freq_pref',
    #     'a_bi_freq_pref', 'a_three_freq_pref',
    #     'a_four_freq_pref', 'a_five_freq_pref'
    # ]
    #
    # frequencies_cols = list(set(frequencies_cols).difference(set(cols_to_exclude)))
    cols = frequencies_cols + [st_with_v,article, correct_article]
    # df, new_cols = add_dummy_cols_df(df, raw_prev_token_POS, prev_token_tags)
    # cols+=new_cols
    #
    # df, new_cols = add_dummy_cols_df(df, raw_next_token_POS, next_token_tags)
    # cols+=new_cols
    return df, cols
def create_splits(df, cv, seed=42):
    s_indexes = set(df[sentence_index])
    np.random.seed(seed)
    m = {j: np.random.randint(0, cv) for j in s_indexes}
    df['fold'] = df[sentence_index].apply(lambda s: m[s])
    res = []
    for f in range(cv):
        train = df[df['fold']!=f]
        test = df[df['fold']==f]
        res.append((train, test))
    return res
def add_corrections_cols(df):
    def get_corrections(row):
        a = row['a']
        an = row['an']
        the = row['the']
        s = [(a, 'a'), (an, 'an'), (the, 'the')]
        s.sort(key=lambda s: s[0], reverse=True)
        proposed_correction = s[0][1]
        art_val = row[article]
        if proposed_correction == art_val:
            return None, None
        else:
            return proposed_correction, s[0][0]
    df[tmp] = df.apply(get_corrections, axis=1)
    df[correction] = df[tmp].apply(lambda s: s[0])
    df[confidence] = df[tmp].apply(lambda s: s[1])
def df_to_submit_array(df, sentences):
    res = [[None]*len(x) for x in sentences]
    def collect_corrections_info(row):
        correction_val = row[correction]
        confidence_val = row[confidence]
        sentence_index_val = row[sentence_index]
        position_val  = row[position]
        if correction_val is None:
            return
        res[sentence_index_val][position_val] = (correction_val, confidence_val)
    df.apply(collect_corrections_info, axis=1)
    # res = [(k,v) for k,v in res.iteritems()]
    # res.sort(key=lambda s: s[0])
    # res = [x[1] for x in res]
    return res
def submit_xgb_test():
    train_arr = load_train()
    test_arr = load_test()
    df = test_arr.copy()
    TARGET = correct_article
    df, cols = to_xgb_df(train_arr)
    df, cols = to_xgb_df(test_arr)
    train_arr = train_arr[cols]
    test_arr=test_arr[cols]
    print len(train_arr), len(test_arr)
    train_target = train_arr[TARGET]
    del train_arr[TARGET]
    test_target = test_arr[TARGET]
    del test_arr[TARGET]
    print test_target.head()
    estimator = xgb.XGBClassifier(n_estimators=180,
                                  subsample=0.8,
                                  colsample_bytree=0.8,
                                  max_depth=5,
                                  # learning_rate=learning_rate,
                                  objective='mlogloss',
                                  nthread=-1
                                  )
    print test_arr.columns.values
    estimator.fit(
        train_arr, train_target,
        verbose=True
    )
    proba = estimator.predict_proba(test_arr)
    classes = list(estimator.classes_)
    print classes
    for c in  classes:
        col = inverse_art_map[c]
        test_arr[col] =proba[:,classes.index(c)]
        df.loc[test_arr.index, col] = test_arr.loc[test_arr.index, col]
    add_corrections_cols(df)
    sentences = load_test_arr()
    res = df_to_submit_array(df, sentences)
    json.dump(res, open('test_submition.json', 'w+'))
    return res
def submit_xgb_out_of_fold_pred(df):
    # df = load_train()
    df = create_out_of_fold_xgb_predictions(df)
    add_corrections_cols(df)
    stats = submit_train(df)
    return arr_to_df(stats['w']), arr_to_df(stats['r'])
def create_out_of_fold_xgb_predictions(df):
    # df = load_train()
    TARGET = correct_article
    df_cp = df.copy()
    df, cols = to_xgb_df(df)
    losses = []
    for train_arr, test_arr in create_splits(df, 3):
        train_arr = train_arr[cols]
        test_arr=test_arr[cols]
        print len(train_arr), len(test_arr)
        train_target = train_arr[TARGET]
        del train_arr[TARGET]
        test_target = test_arr[TARGET]
        del test_arr[TARGET]
        # train_arr, test_arr = train_arr[cols], test_arr[cols]
        estimator = xgb.XGBClassifier(n_estimators=180,
                                      subsample=0.8,
                                      colsample_bytree=0.8,
                                      max_depth=5,
                                      # learning_rate=learning_rate,
                                      objective='mlogloss',
                                      nthread=-1
                                      )
        print test_arr.columns.values
        estimator.fit(
            train_arr, train_target,
            verbose=True
        )
        proba = estimator.predict_proba(test_arr)
        classes = list(estimator.classes_)
        print classes
        for c in  classes:
            col = inverse_art_map[c]
            test_arr[col] =proba[:,classes.index(c)]
            df_cp.loc[test_arr.index, col] = test_arr.loc[test_arr.index, col]
        loss = log_loss(test_target, proba)
        losses.append(loss)
        print loss
    return df_cp
def eval_target_score(arts, probs, labels):
    sz=len(arts)
    all_mistakes = sum(arts[j]!=labels[j] for j in range(sz))
    data=[]
    for j in range(sz):
        p = probs[j]#probs
        a=arts[j]#current article
        correct_a = labels[j]#correct article
        p = [(i, p[i]) for i in range(len(p))]
        p.sort(key=lambda s: s[1], reverse=True)
        s = p[0][0]
        conf = p[0][1]
        conf = float('-inf') if a==s else conf
        c = s ==correct_a
        data.append((-conf, c, correct_a!=a))
    data.sort()
    fp2 = 0
    fp = 0
    tp = 0
    score = 0
    acc = 0
    for _, c, r in data:
        fp2 += not c # wrong correction
        fp += not r#realy errors count
        tp += c#right correction
        acc = max(acc, 1 - (0. + fp + all_mistakes - tp) / len(data))
        if fp2 * 1. / len(data) <= 0.02:
            score = tp * 1. / all_mistakes
    print 'target score = %.2f %%' % (score * 100)
    print 'accuracy (just for info) = %.2f %%' % (acc * 100)
    return 'target_score'  , -score
def perform_xgboost_cv(df):
    # df = load_train()
    TARGET = correct_article
    df, cols = to_xgb_df(df)
    losses = []
    for train_arr, test_arr in create_splits(df, 3):
        train_arr = train_arr[cols]
        test_arr=test_arr[cols]
        def get_articles(labels):
            if len(labels)==len(train_arr):
                return list(train_arr[article])
            elif len(labels) == len(test_arr):
                return list(test_arr[article])
            raise
        def my_obj(preds, dtrain):
            labels = dtrain.get_label()
            arts = get_articles(labels)
            return eval_target_score(arts, preds, labels)
        print len(train_arr), len(test_arr)
        train_target = train_arr[TARGET]
        del train_arr[TARGET]
        test_target = test_arr[TARGET]
        del test_arr[TARGET]
        # train_arr, test_arr = train_arr[cols], test_arr[cols]
        estimator = xgb.XGBClassifier(n_estimators=10000,
                                      subsample=0.8,
                                      colsample_bytree=0.8,
                                      max_depth=5,
                                      # learning_rate=learning_rate,
                                      objective='mlogloss',
                                      nthread=-1
                                      )
        print test_arr.columns.values
        eval_set = [(train_arr, train_target), (test_arr, test_target)]
        estimator.fit(
            train_arr, train_target,
            eval_set=eval_set,
            eval_metric='mlogloss',
            verbose=True,
            early_stopping_rounds=50
        )
        classes = list(estimator.classes_)
        print classes
        xgb.plot_importance(estimator)
        plt.show()
        proba = estimator.predict_proba(test_arr)
        loss = log_loss(test_target, proba)
        losses.append(loss)
        print loss
cols=['a_bi_freq_suff' ,'a_three_freq_suff' ,'a_four_freq_suff', 'a_five_freq_suff',
      'a_bi_freq_pref' ,'a_three_freq_pref' ,'a_four_freq_pref' ,'a_five_freq_pref',
      'the_bi_freq_suff', 'the_three_freq_suff', 'the_four_freq_suff',
      'the_five_freq_suff', 'the_bi_freq_pref' ,'the_three_freq_pref',
      'the_four_freq_pref' ,'the_five_freq_pref' ,'a_the_bi_freq_suff_p1',
      'the_a_bi_freq_suff_p1' ,'a_the_three_freq_suff_p1',
      'the_a_three_freq_suff_p1' ,'a_the_four_freq_suff_p1',
      'the_a_four_freq_suff_p1' ,'a_the_five_freq_suff_p1',
      'the_a_five_freq_suff_p1' ,'a_the_bi_freq_pref_p1' 'the_a_bi_freq_pref_p1',
      'a_the_three_freq_pref_p1', 'the_a_three_freq_pref_p1',
      'a_the_four_freq_pref_p1', 'the_a_four_freq_pref_p1',
      'a_the_five_freq_pref_p1', 'the_a_five_freq_pref_p1',
      'a_the_bi_freq_suff_p10', 'the_a_bi_freq_suff_p10',
      'a_the_three_freq_suff_p10' ,'the_a_three_freq_suff_p10',
      'a_the_four_freq_suff_p10' ,'the_a_four_freq_suff_p10',
      'a_the_five_freq_suff_p10', 'the_a_five_freq_suff_p10',
      'a_the_bi_freq_pref_p10', 'the_a_bi_freq_pref_p10',
      'a_the_three_freq_pref_p10', 'the_a_three_freq_pref_p10',
      'a_the_four_freq_pref_p10' ,'the_a_four_freq_pref_p10',
      'a_the_five_freq_pref_p10' ,'the_a_five_freq_pref_p10',
      'a_the_bi_freq_suff_p100', 'the_a_bi_freq_suff_p100',
      'a_the_three_freq_suff_p100', 'the_a_three_freq_suff_p100',
      'a_the_four_freq_suff_p100' ,'the_a_four_freq_suff_p100',
      'a_the_five_freq_suff_p100' ,'the_a_five_freq_suff_p100',
      'a_the_bi_freq_pref_p100', 'the_a_bi_freq_pref_p100',
      'a_the_three_freq_pref_p100', 'the_a_three_freq_pref_p100',
      'a_the_four_freq_pref_p100' ,'the_a_four_freq_pref_p100',
      'a_the_five_freq_pref_p100' ,'the_a_five_freq_pref_p100' ,'st_with_v',
      'article']
# train_df = exploring_df(load_train())
# train_df = load_train()
#bad, good = process_max_ngram_freq_naive(train_df, 8, 5, 10)   63%
# bad, good = process_max_ngram_freq_naive(train_df, 8, 5, 10)
submit_xgb_out_of_fold_pred(df.copy())
35673 17890
['a_bi_freq_suff' 'a_three_freq_suff' 'a_four_freq_suff' 'a_five_freq_suff'
 'a_bi_freq_pref' 'a_three_freq_pref' 'a_four_freq_pref' 'a_five_freq_pref'
 'a_sn_gram_freq' 'the_bi_freq_suff' 'the_three_freq_suff'
 'the_four_freq_suff' 'the_five_freq_suff' 'the_bi_freq_pref'
 'the_three_freq_pref' 'the_four_freq_pref' 'the_five_freq_pref'
 'the_sn_gram_freq' 'a_the_bi_freq_suff_p1' 'the_a_bi_freq_suff_p1'
 'a_the_three_freq_suff_p1' 'the_a_three_freq_suff_p1'
 'a_the_four_freq_suff_p1' 'the_a_four_freq_suff_p1'
 'a_the_five_freq_suff_p1' 'the_a_five_freq_suff_p1'
 'a_the_bi_freq_pref_p1' 'the_a_bi_freq_pref_p1' 'a_the_three_freq_pref_p1'
 'the_a_three_freq_pref_p1' 'a_the_four_freq_pref_p1'
 'the_a_four_freq_pref_p1' 'a_the_five_freq_pref_p1'
 'the_a_five_freq_pref_p1' 'a_the_sn_ngram_freq_p1'
 'the_a_sn_ngram_freq_p1' 'a_the_bi_freq_suff_p10' 'the_a_bi_freq_suff_p10'
 'a_the_three_freq_suff_p10' 'the_a_three_freq_suff_p10'
 'a_the_four_freq_suff_p10' 'the_a_four_freq_suff_p10'
 'a_the_five_freq_suff_p10' 'the_a_five_freq_suff_p10'
 'a_the_bi_freq_pref_p10' 'the_a_bi_freq_pref_p10'
 'a_the_three_freq_pref_p10' 'the_a_three_freq_pref_p10'
 'a_the_four_freq_pref_p10' 'the_a_four_freq_pref_p10'
 'a_the_five_freq_pref_p10' 'the_a_five_freq_pref_p10'
 'a_the_sn_ngram_freq_p10' 'the_a_sn_ngram_freq_p10'
 'a_the_bi_freq_suff_p100' 'the_a_bi_freq_suff_p100'
 'a_the_three_freq_suff_p100' 'the_a_three_freq_suff_p100'
 'a_the_four_freq_suff_p100' 'the_a_four_freq_suff_p100'
 'a_the_five_freq_suff_p100' 'the_a_five_freq_suff_p100'
 'a_the_bi_freq_pref_p100' 'the_a_bi_freq_pref_p100'
 'a_the_three_freq_pref_p100' 'the_a_three_freq_pref_p100'
 'a_the_four_freq_pref_p100' 'the_a_four_freq_pref_p100'
 'a_the_five_freq_pref_p100' 'the_a_five_freq_pref_p100'
 'a_the_sn_ngram_freq_p100' 'the_a_sn_ngram_freq_p100' 'st_with_v'
 'article']
[0, 1, 2]
/home/dpetrovskyi/pycharm/pycharm-2016.3.2/helpers/pydev/pydevconsole.py:350: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead
See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  def start_server(host, port, client_port):
0.238656636068
35672 17891
['a_bi_freq_suff' 'a_three_freq_suff' 'a_four_freq_suff' 'a_five_freq_suff'
 'a_bi_freq_pref' 'a_three_freq_pref' 'a_four_freq_pref' 'a_five_freq_pref'
 'a_sn_gram_freq' 'the_bi_freq_suff' 'the_three_freq_suff'
 'the_four_freq_suff' 'the_five_freq_suff' 'the_bi_freq_pref'
 'the_three_freq_pref' 'the_four_freq_pref' 'the_five_freq_pref'
 'the_sn_gram_freq' 'a_the_bi_freq_suff_p1' 'the_a_bi_freq_suff_p1'
 'a_the_three_freq_suff_p1' 'the_a_three_freq_suff_p1'
 'a_the_four_freq_suff_p1' 'the_a_four_freq_suff_p1'
 'a_the_five_freq_suff_p1' 'the_a_five_freq_suff_p1'
 'a_the_bi_freq_pref_p1' 'the_a_bi_freq_pref_p1' 'a_the_three_freq_pref_p1'
 'the_a_three_freq_pref_p1' 'a_the_four_freq_pref_p1'
 'the_a_four_freq_pref_p1' 'a_the_five_freq_pref_p1'
 'the_a_five_freq_pref_p1' 'a_the_sn_ngram_freq_p1'
 'the_a_sn_ngram_freq_p1' 'a_the_bi_freq_suff_p10' 'the_a_bi_freq_suff_p10'
 'a_the_three_freq_suff_p10' 'the_a_three_freq_suff_p10'
 'a_the_four_freq_suff_p10' 'the_a_four_freq_suff_p10'
 'a_the_five_freq_suff_p10' 'the_a_five_freq_suff_p10'
 'a_the_bi_freq_pref_p10' 'the_a_bi_freq_pref_p10'
 'a_the_three_freq_pref_p10' 'the_a_three_freq_pref_p10'
 'a_the_four_freq_pref_p10' 'the_a_four_freq_pref_p10'
 'a_the_five_freq_pref_p10' 'the_a_five_freq_pref_p10'
 'a_the_sn_ngram_freq_p10' 'the_a_sn_ngram_freq_p10'
 'a_the_bi_freq_suff_p100' 'the_a_bi_freq_suff_p100'
 'a_the_three_freq_suff_p100' 'the_a_three_freq_suff_p100'
 'a_the_four_freq_suff_p100' 'the_a_four_freq_suff_p100'
 'a_the_five_freq_suff_p100' 'the_a_five_freq_suff_p100'
 'a_the_bi_freq_pref_p100' 'the_a_bi_freq_pref_p100'
 'a_the_three_freq_pref_p100' 'the_a_three_freq_pref_p100'
 'a_the_four_freq_pref_p100' 'the_a_four_freq_pref_p100'
 'a_the_five_freq_pref_p100' 'the_a_five_freq_pref_p100'
 'a_the_sn_ngram_freq_p100' 'the_a_sn_ngram_freq_p100' 'st_with_v'
 'article']
[0, 1, 2]
0.239201084451
35781 17782
['a_bi_freq_suff' 'a_three_freq_suff' 'a_four_freq_suff' 'a_five_freq_suff'
 'a_bi_freq_pref' 'a_three_freq_pref' 'a_four_freq_pref' 'a_five_freq_pref'
 'a_sn_gram_freq' 'the_bi_freq_suff' 'the_three_freq_suff'
 'the_four_freq_suff' 'the_five_freq_suff' 'the_bi_freq_pref'
 'the_three_freq_pref' 'the_four_freq_pref' 'the_five_freq_pref'
 'the_sn_gram_freq' 'a_the_bi_freq_suff_p1' 'the_a_bi_freq_suff_p1'
 'a_the_three_freq_suff_p1' 'the_a_three_freq_suff_p1'
 'a_the_four_freq_suff_p1' 'the_a_four_freq_suff_p1'
 'a_the_five_freq_suff_p1' 'the_a_five_freq_suff_p1'
 'a_the_bi_freq_pref_p1' 'the_a_bi_freq_pref_p1' 'a_the_three_freq_pref_p1'
 'the_a_three_freq_pref_p1' 'a_the_four_freq_pref_p1'
 'the_a_four_freq_pref_p1' 'a_the_five_freq_pref_p1'
 'the_a_five_freq_pref_p1' 'a_the_sn_ngram_freq_p1'
 'the_a_sn_ngram_freq_p1' 'a_the_bi_freq_suff_p10' 'the_a_bi_freq_suff_p10'
 'a_the_three_freq_suff_p10' 'the_a_three_freq_suff_p10'
 'a_the_four_freq_suff_p10' 'the_a_four_freq_suff_p10'
 'a_the_five_freq_suff_p10' 'the_a_five_freq_suff_p10'
 'a_the_bi_freq_pref_p10' 'the_a_bi_freq_pref_p10'
 'a_the_three_freq_pref_p10' 'the_a_three_freq_pref_p10'
 'a_the_four_freq_pref_p10' 'the_a_four_freq_pref_p10'
 'a_the_five_freq_pref_p10' 'the_a_five_freq_pref_p10'
 'a_the_sn_ngram_freq_p10' 'the_a_sn_ngram_freq_p10'
 'a_the_bi_freq_suff_p100' 'the_a_bi_freq_suff_p100'
 'a_the_three_freq_suff_p100' 'the_a_three_freq_suff_p100'
 'a_the_four_freq_suff_p100' 'the_a_four_freq_suff_p100'
 'a_the_five_freq_suff_p100' 'the_a_five_freq_suff_p100'
 'a_the_bi_freq_pref_p100' 'the_a_bi_freq_pref_p100'
 'a_the_three_freq_pref_p100' 'the_a_three_freq_pref_p100'
 'a_the_four_freq_pref_p100' 'the_a_four_freq_pref_p100'
 'a_the_five_freq_pref_p100' 'the_a_five_freq_pref_p100'
 'a_the_sn_ngram_freq_p100' 'the_a_sn_ngram_freq_p100' 'st_with_v'
 'article']
[0, 1, 2]
0.232619411437
36000 36000 36000
1071.26
all_mistakes 26886
target score = 73.96 %
accuracy (just for info) = 90.35 %
Out[7]:
(        trans  confidence
 0     an->the    0.999777
 1     an->the    0.999712
 2      a->the    0.999564
 3      a->the    0.999328
 4      a->the    0.999295
 5     an->the    0.998839
 6       an->a    0.998759
 7     an->the    0.998453
 8      a->the    0.998234
 9     an->the    0.998137
 10    an->the    0.997872
 11    an->the    0.997728
 12     a->the    0.997599
 13    an->the    0.997542
 14     a->the    0.997455
 15      an->a    0.997204
 16     a->the    0.997189
 17     a->the    0.996891
 18    an->the    0.996824
 19     a->the    0.996771
 20     a->the    0.996303
 21     the->a    0.996300
 22     a->the    0.995996
 23     the->a    0.995435
 24    an->the    0.994911
 25    an->the    0.994811
 26    the->an    0.994797
 27    an->the    0.994663
 28     a->the    0.994643
 29     a->the    0.994042
 30    an->the    0.993965
 31    an->the    0.993832
 32     a->the    0.993810
 33     a->the    0.993766
 34    an->the    0.993463
 35    an->the    0.993185
 36    an->the    0.993034
 37     a->the    0.992635
 38    an->the    0.992596
 39     a->the    0.992392
 40     a->the    0.991755
 41    an->the    0.991679
 42    an->the    0.991562
 43    an->the    0.991520
 44     a->the    0.991230
 45    the->an    0.990804
 46      an->a    0.990782
 47    an->the    0.990676
 48     a->the    0.989815
 49    an->the    0.989476
 50     a->the    0.989379
 51      an->a    0.988966
 52     a->the    0.988929
 53      an->a    0.987821
 54      an->a    0.987704
 55    an->the    0.987464
 56    an->the    0.987189
 57     a->the    0.986928
 58    an->the    0.986423
 59    an->the    0.985695
 60     the->a    0.985409
 61     a->the    0.985198
 62     the->a    0.985110
 63    an->the    0.984956
 64    an->the    0.984786
 65    an->the    0.984765
 66     the->a    0.984257
 67     a->the    0.984248
 68     a->the    0.983709
 69      an->a    0.983596
 70     the->a    0.982765
 71      an->a    0.982267
 72      a->an    0.982258
 73      an->a    0.981614
 74    an->the    0.981230
 75     a->the    0.980972
 76     a->the    0.980922
 77     a->the    0.980861
 78     a->the    0.980823
 79     the->a    0.980763
 80     the->a    0.980122
 81    an->the    0.980028
 82     the->a    0.979756
 83    the->an    0.979243
 84     a->the    0.979052
 85    an->the    0.978900
 86      an->a    0.978753
 87     a->the    0.978553
 88     a->the    0.978291
 89      an->a    0.978034
 90     the->a    0.977809
 91     a->the    0.977423
 92     the->a    0.977202
 93     a->the    0.976866
 94     the->a    0.976202
 95    an->the    0.975896
 96    an->the    0.975541
 97     a->the    0.975537
 98     a->the    0.974967
 99    an->the    0.974870
 100    a->the    0.974665
 101   an->the    0.974254
 102    the->a    0.973938
 103    a->the    0.973760
 104     an->a    0.973705
 105    a->the    0.973677
 106     an->a    0.973487
 107   an->the    0.972802
 108   an->the    0.972734
 109     an->a    0.972369
 110    a->the    0.972064
 111    a->the    0.971914
 112   an->the    0.971785
 113    a->the    0.971784
 114    the->a    0.971581
 115     an->a    0.971495
 116   an->the    0.971368
 117    a->the    0.971311
 118    a->the    0.971262
 119    a->the    0.970691
 120    a->the    0.970467
 121     an->a    0.970306
 122    a->the    0.970157
 123     an->a    0.970039
 124    the->a    0.969724
 125   an->the    0.969652
 126    a->the    0.969547
 127     a->an    0.969099
 128   an->the    0.968943
 129   an->the    0.968602
 130   an->the    0.967857
 131   an->the    0.967547
 132    the->a    0.967459
 133   an->the    0.967253
 134    a->the    0.967178
 135     an->a    0.967127
 136    a->the    0.966920
 137     an->a    0.966867
 138     an->a    0.966759
 139    a->the    0.966675
 140   an->the    0.965765
 141    a->the    0.965539
 142     a->an    0.965520
 143   an->the    0.965022
 144   an->the    0.964709
 145    the->a    0.964497
 146   an->the    0.964287
 147    the->a    0.963819
 148   an->the    0.963735
 149   an->the    0.963711
 150    a->the    0.963431
 151     an->a    0.963396
 152    the->a    0.962957
 153    a->the    0.962743
 154     a->an    0.962671
 155    the->a    0.962490
 156    a->the    0.962313
 157    a->the    0.961892
 158    the->a    0.961547
 159   an->the    0.961165
 160   an->the    0.961083
 161    the->a    0.960813
 162     a->an    0.960739
 163    the->a    0.960521
 164    the->a    0.960512
 165   an->the    0.960206
 166   an->the    0.959744
 167   an->the    0.959733
 168    the->a    0.959238
 169    a->the    0.958944
 170    a->the    0.958906
 171   an->the    0.958773
 172     a->an    0.958759
 173    the->a    0.958630
 174   the->an    0.958166
 175   an->the    0.957518
 176   an->the    0.957160
 177    the->a    0.956884
 178    a->the    0.956862
 179     an->a    0.956331
 180     an->a    0.956316
 181     a->an    0.955542
 182     a->an    0.955128
 183   an->the    0.955066
 184    a->the    0.954814
 185    a->the    0.954726
 186   an->the    0.954536
 187   the->an    0.954397
 188   an->the    0.954387
 189    a->the    0.954288
 190     an->a    0.954115
 191    the->a    0.953641
 192    a->the    0.953579
 193   an->the    0.953407
 194    a->the    0.953315
 195   an->the    0.953237
 196    a->the    0.953151
 197   an->the    0.952950
 198   an->the    0.952602
 199    a->the    0.952581
 200     an->a    0.952529
 201     an->a    0.952170
 202   an->the    0.952020
 203    the->a    0.951932
 204     an->a    0.951393
 205    a->the    0.951371
 206   an->the    0.951316
 207     an->a    0.950373
 208    a->the    0.950084
 209    a->the    0.949723
 210    a->the    0.949520
 211   the->an    0.949063
 212    a->the    0.948932
 213    the->a    0.948826
 214    the->a    0.948793
 215    a->the    0.948618
 216     an->a    0.948119
 217   an->the    0.947811
 218    the->a    0.947749
 219   the->an    0.947658
 220    a->the    0.947490
 221   an->the    0.947416
 222   an->the    0.947337
 223     an->a    0.947218
 224   an->the    0.947037
 225     an->a    0.946522
 226   an->the    0.946506
 227   an->the    0.946052
 228     an->a    0.945389
 229    a->the    0.945088
 230   an->the    0.944652
 231   the->an    0.944598
 232    a->the    0.944072
 233   the->an    0.943937
 234   an->the    0.943720
 235     a->an    0.943627
 236   an->the    0.943322
 237   an->the    0.943256
 238   an->the    0.943136
 239    a->the    0.943058
 240     a->an    0.943008
 241    a->the    0.942952
 242    a->the    0.942498
 243     a->an    0.942023
 244    a->the    0.941921
 245   an->the    0.940987
 246    a->the    0.940901
 247     an->a    0.940773
 248    a->the    0.940595
 249   an->the    0.940245
 250     a->an    0.940220
 251    a->the    0.939963
 252    a->the    0.939728
 253    the->a    0.939719
 254   an->the    0.939324
 255   the->an    0.939272
 256    the->a    0.939225
 257     an->a    0.939174
 258     an->a    0.938264
 259   an->the    0.937384
 260    a->the    0.937364
 261     a->an    0.937331
 262    a->the    0.937270
 263    the->a    0.937232
 264    the->a    0.937124
 265    a->the    0.936877
 266   the->an    0.936654
 267   an->the    0.936495
 268    a->the    0.936281
 269   an->the    0.936004
 270    a->the    0.935917
 271   an->the    0.935823
 272   an->the    0.935495
 273   an->the    0.935476
 274    a->the    0.935356
 275    a->the    0.934899
 276    a->the    0.934851
 277   the->an    0.934793
 278    a->the    0.934446
 279    the->a    0.934416
 280    the->a    0.934403
 281   an->the    0.934298
 282    the->a    0.934063
 283     a->an    0.933395
 284     an->a    0.933345
 285    a->the    0.933130
 286   an->the    0.933028
 287   an->the    0.932962
 288    the->a    0.932928
 289   the->an    0.932878
 290   an->the    0.932858
 291   an->the    0.932596
 292     an->a    0.931350
 293    the->a    0.931335
 294    a->the    0.931294
 295    the->a    0.931290
 296     an->a    0.931217
 297     an->a    0.931031
 298    a->the    0.930728
 299    a->the    0.930644
 300   an->the    0.930459
 301   an->the    0.930389
 302   an->the    0.930308
 303    the->a    0.930247
 304    a->the    0.930176
 305    a->the    0.929955
 306    the->a    0.929843
 307   the->an    0.929741
 308     an->a    0.929656
 309   an->the    0.929516
 310    a->the    0.929412
 311   an->the    0.929094
 312   an->the    0.928980
 313     an->a    0.928625
 314   an->the    0.928618
 315   an->the    0.928355
 316     an->a    0.928340
 317   an->the    0.928070
 318    a->the    0.927986
 319    a->the    0.927605
 320    a->the    0.927303
 321     an->a    0.927296
 322    a->the    0.927132
 323    a->the    0.927119
 324   an->the    0.926634
 325    the->a    0.926532
 326    a->the    0.926405
 327     an->a    0.926374
 328     a->an    0.926034
 329   an->the    0.925979
 330    a->the    0.925611
 331   an->the    0.925584
 332    a->the    0.925355
 333    a->the    0.925337
 334    a->the    0.925301
 335     an->a    0.924913
 336   an->the    0.924827
 337   an->the    0.924735
 338     an->a    0.924465
 339    the->a    0.924215
 340    the->a    0.923935
 341     an->a    0.923877
 342   the->an    0.923239
 343   an->the    0.922297
 344   an->the    0.922271
 345   an->the    0.921920
 346    a->the    0.921794
 347   an->the    0.921648
 348     an->a    0.921647
 349    a->the    0.921513
 350    a->the    0.921485
 351    the->a    0.921390
 352   an->the    0.920876
 353     an->a    0.919960
 354    a->the    0.919837
 355   an->the    0.919770
 356     a->an    0.919416
 357    a->the    0.919316
 358     an->a    0.919067
 359    a->the    0.918285
 360   an->the    0.918243
 361    a->the    0.918163
 362    the->a    0.917921
 363   an->the    0.917267
 364    a->the    0.917205
 365     an->a    0.917204
 366     an->a    0.917076
 367     an->a    0.916145
 368   an->the    0.915690
 369     an->a    0.915418
 370   an->the    0.915391
 371    a->the    0.914925
 372     an->a    0.913901
 373    a->the    0.913708
 374    a->the    0.913413
 375    a->the    0.913058
 376   an->the    0.912994
 377    a->the    0.912930
 378    a->the    0.912836
 379     an->a    0.912826
 380     a->an    0.912758
 381     an->a    0.912701
 382   an->the    0.912701
 383    the->a    0.912639
 384    a->the    0.912530
 385    a->the    0.912289
 386   an->the    0.912257
 387    a->the    0.912186
 388   an->the    0.912012
 389    a->the    0.911988
 390   an->the    0.911842
 391     an->a    0.911133
 392    the->a    0.911087
 393     an->a    0.910805
 394    a->the    0.910776
 395    the->a    0.910013
 396   an->the    0.909687
 397     an->a    0.909633
 398    a->the    0.909533
 399     an->a    0.909527
 400   an->the    0.909458
 401   an->the    0.909233
 402    a->the    0.908972
 403    a->the    0.908972
 404    a->the    0.908967
 405    a->the    0.908931
 406    a->the    0.908875
 407    the->a    0.908767
 408     an->a    0.908622
 409    a->the    0.908608
 410    the->a    0.908523
 411     an->a    0.908425
 412    the->a    0.908340
 413     an->a    0.908308
 414   an->the    0.908253
 415     a->an    0.907998
 416     an->a    0.907813
 417     an->a    0.907732
 418    a->the    0.907352
 419    a->the    0.907292
 420   an->the    0.907188
 421   an->the    0.906894
 422    the->a    0.906758
 423    a->the    0.906673
 424    the->a    0.906158
 425     an->a    0.905977
 426   an->the    0.905922
 427    a->the    0.905864
 428     an->a    0.905850
 429   an->the    0.905656
 430   an->the    0.905547
 431   an->the    0.905547
 432    a->the    0.905180
 433    a->the    0.904949
 434     an->a    0.904763
 435     an->a    0.904682
 436   an->the    0.904670
 437    the->a    0.904593
 438   an->the    0.904423
 439    a->the    0.904422
 440    a->the    0.904192
 441   an->the    0.904155
 442    the->a    0.904044
 443   an->the    0.903922
 444    a->the    0.903727
 445   an->the    0.903670
 446    a->the    0.903665
 447     a->an    0.902928
 448    a->the    0.902847
 449   an->the    0.902675
 450    a->the    0.902326
 451   an->the    0.902226
 452    the->a    0.902218
 453    the->a    0.902171
 454   an->the    0.902169
 455     an->a    0.902082
 456    a->the    0.901965
 457   an->the    0.901763
 458    a->the    0.901755
 459   an->the    0.901474
 460    the->a    0.901463
 461     an->a    0.901367
 462   an->the    0.901058
 463     an->a    0.900967
 464   an->the    0.900933
 465    a->the    0.900929
 466   an->the    0.900849
 467    the->a    0.900770
 468   an->the    0.900605
 469    the->a    0.900592
 470    a->the    0.900583
 471   an->the    0.900572
 472     an->a    0.899281
 473   an->the    0.899280
 474   an->the    0.897773
 475    a->the    0.897504
 476   an->the    0.897040
 477    the->a    0.896949
 478   the->an    0.896900
 479    a->the    0.896448
 480     a->an    0.896355
 481    the->a    0.896159
 482    the->a    0.896019
 483     an->a    0.895861
 484   the->an    0.895795
 485    the->a    0.895322
 486   an->the    0.895290
 487   an->the    0.895216
 488   an->the    0.894882
 489    a->the    0.894742
 490   an->the    0.894448
 491   an->the    0.894184
 492     a->an    0.893415
 493     an->a    0.893350
 494    the->a    0.892616
 495   an->the    0.891847
 496     an->a    0.891750
 497    the->a    0.891635
 498     an->a    0.891527
 499    a->the    0.890923
 500    a->the    0.890757
 501     an->a    0.890611
 502    a->the    0.890515
 503    the->a    0.890452
 504    the->a    0.890334
 505    a->the    0.890330
 506     an->a    0.890302
 507     an->a    0.890280
 508    the->a    0.889834
 509    the->a    0.889714
 510   an->the    0.889552
 511    a->the    0.889530
 512    a->the    0.889217
 513     an->a    0.888938
 514     an->a    0.888498
 515    a->the    0.888496
 516    the->a    0.888412
 517    the->a    0.888219
 518    a->the    0.888010
 519   an->the    0.887934
 520    a->the    0.887808
 521     a->an    0.887680
 522    a->the    0.887057
 523    the->a    0.886840
 524    a->the    0.886813
 525     an->a    0.886650
 526    a->the    0.886558
 527     an->a    0.886418
 528    a->the    0.886218
 529   an->the    0.885828
 530    a->the    0.885740
 531    a->the    0.885717
 532   an->the    0.885572
 533    a->the    0.885497
 534    a->the    0.884050
 535    a->the    0.883646
 536     an->a    0.883252
 537    the->a    0.883137
 538     a->an    0.883132
 539    a->the    0.883097
 540   an->the    0.882491
 541   an->the    0.882297
 542     a->an    0.882282
 543   the->an    0.882244
 544     an->a    0.882200
 545   an->the    0.881805
 546    the->a    0.881667
 547   an->the    0.881238
 548   an->the    0.880700
 549    a->the    0.880296
 550    a->the    0.880005
 551   an->the    0.879810
 552    the->a    0.879790
 553    the->a    0.879739
 554    the->a    0.879506
 555   an->the    0.879318
 556     an->a    0.879003
 557    a->the    0.878172
 558   an->the    0.878162
 559   an->the    0.878013
 560    a->the    0.877879
 561   an->the    0.877585
 562   an->the    0.877433
 563     an->a    0.877114
 564    a->the    0.876904
 565     an->a    0.876829
 566   an->the    0.876740
 567    a->the    0.876604
 568    the->a    0.876567
 569   the->an    0.876443
 570     a->an    0.876287
 571    a->the    0.876256
 572    a->the    0.876067
 573   an->the    0.876026
 574   an->the    0.875518
 575    the->a    0.875302
 576     an->a    0.875150
 577   an->the    0.874668
 578    a->the    0.874312
 579    a->the    0.874126
 580   an->the    0.874050
 581     an->a    0.873518
 582   the->an    0.873409
 583   an->the    0.872764
 584     an->a    0.872735
 585   an->the    0.872531
 586    a->the    0.872487
 587    the->a    0.872415
 588    a->the    0.872359
 589     an->a    0.872272
 590   an->the    0.872249
 591     a->an    0.871985
 592     an->a    0.871926
 593   an->the    0.871830
 594     an->a    0.871286
 595   an->the    0.870953
 596     an->a    0.870697
 597   an->the    0.870475
 598    the->a    0.870454
 599    a->the    0.870396
 600   an->the    0.870335
 601   an->the    0.870238
 602    a->the    0.870214
 603     an->a    0.869926
 604   an->the    0.869679
 605   an->the    0.869496
 606   an->the    0.869466
 607    a->the    0.869439
 608    a->the    0.869280
 609    the->a    0.869211
 610    the->a    0.869182
 611    the->a    0.868963
 612     an->a    0.868704
 613    a->the    0.868696
 614    a->the    0.868653
 615   an->the    0.868242
 616   an->the    0.868198
 617    a->the    0.868183
 618   an->the    0.868031
 619   the->an    0.867730
 620    the->a    0.867391
 621    a->the    0.867339
 622    the->a    0.867061
 623    a->the    0.867044
 624     an->a    0.866885
 625    the->a    0.866731
 626     an->a    0.866705
 627    a->the    0.866580
 628    the->a    0.865960
 629     a->an    0.865723
 630   an->the    0.865672
 631    a->the    0.865622
 632     an->a    0.865419
 633    a->the    0.865302
 634    the->a    0.865221
 635   the->an    0.865121
 636    the->a    0.864502
 637     an->a    0.864455
 638    the->a    0.864112
 639     an->a    0.864040
 640    the->a    0.863623
 641   an->the    0.863587
 642     an->a    0.863553
 643     an->a    0.863508
 644    the->a    0.863136
 645     an->a    0.862880
 646   an->the    0.862762
 647    a->the    0.862384
 648    a->the    0.862347
 649    the->a    0.861614
 650    a->the    0.861166
 651     a->an    0.860990
 652   an->the    0.860861
 653    a->the    0.860726
 654   an->the    0.860654
 655   an->the    0.860419
 656    the->a    0.859981
 657   an->the    0.859980
 658    a->the    0.859207
 659   an->the    0.858800
 660   an->the    0.858724
 661   an->the    0.858500
 662    a->the    0.858351
 663   an->the    0.858185
 664   an->the    0.857673
 665   the->an    0.857510
 666     an->a    0.857134
 667   an->the    0.857124
 668   an->the    0.857087
 669    a->the    0.856772
 670     an->a    0.856637
 671    a->the    0.856616
 672    the->a    0.856323
 673    a->the    0.856322
 674    the->a    0.856110
 675     a->an    0.856032
 676    a->the    0.855816
 677   an->the    0.855572
 678     an->a    0.855194
 679    a->the    0.855193
 680    the->a    0.854850
 681   an->the    0.854719
 682    the->a    0.854475
 683     an->a    0.854217
 684    the->a    0.853866
 685   an->the    0.853842
 686    the->a    0.853811
 687    a->the    0.853601
 688   an->the    0.853597
 689    the->a    0.853104
 690    the->a    0.852462
 691   an->the    0.852280
 692    a->the    0.852065
 693    the->a    0.851709
 694     an->a    0.851632
 695    the->a    0.851453
 696    a->the    0.851416
 697   an->the    0.851016
 698   an->the    0.850699
 699     an->a    0.850606
 700   an->the    0.850518
 701    the->a    0.850423
 702     a->an    0.850382
 703    the->a    0.850358
 704   an->the    0.850262
 705    a->the    0.850254
 706     an->a    0.849793
 707    the->a    0.849613
 708    the->a    0.849562
 709    a->the    0.849313
 710   an->the    0.848985
 711    a->the    0.848762
 712   an->the    0.848572
 713    a->the    0.848555
 714   an->the    0.848131
 715   an->the    0.847958
 716   an->the    0.847822
 717    a->the    0.847499
 718    a->the    0.847108
 719   the->an    0.846630
 720    a->the    0.846597
 721   an->the    0.846419
 722     a->an    0.846074
 723     a->an    0.845955
 724    a->the    0.845047
 725    a->the    0.844509
 726     an->a    0.844181
 727    the->a    0.844105
 728   an->the    0.843545
 729     an->a    0.843522
 730     an->a    0.843517
 731    the->a    0.843393
 732    a->the    0.843320
 733    a->the    0.843149
 734   an->the    0.843144
 735    a->the    0.842499
 736   an->the    0.842169
 737    a->the    0.842047
 738     a->an    0.841870
 739     an->a    0.841815
 740    a->the    0.841739
 741     an->a    0.841697
 742   an->the    0.841253
 743     an->a    0.840552
 744    the->a    0.840321
 745    a->the    0.840314
 746   an->the    0.840298
 747     an->a    0.840238
 748    the->a    0.840134
 749    a->the    0.839915
 750    the->a    0.839747
 751   an->the    0.839483
 752    a->the    0.839354
 753    a->the    0.838883
 754    a->the    0.838833
 755    a->the    0.838307
 756    a->the    0.837528
 757    a->the    0.836505
 758    a->the    0.836504
 759     an->a    0.836445
 760     an->a    0.835941
 761    a->the    0.835834
 762   an->the    0.835783
 763     an->a    0.834933
 764     a->an    0.834904
 765     an->a    0.834799
 766     an->a    0.834785
 767   an->the    0.834672
 768   an->the    0.834308
 769    the->a    0.833976
 770    the->a    0.833964
 771    a->the    0.833844
 772   an->the    0.833791
 773   an->the    0.833756
 774    a->the    0.833655
 775    a->the    0.833612
 776   an->the    0.833565
 777    a->the    0.833523
 778   an->the    0.833350
 779    the->a    0.833040
 780   an->the    0.832756
 781    the->a    0.832354
 782     an->a    0.832205
 783    a->the    0.831986
 784    the->a    0.831829
 785     an->a    0.831715
 786    a->the    0.831650
 787    a->the    0.831533
 788     a->an    0.831355
 789    a->the    0.831077
 790    the->a    0.830960
 791    a->the    0.830960
 792    a->the    0.830670
 793   an->the    0.830558
 794    a->the    0.830430
 795     an->a    0.830417
 796    a->the    0.829948
 797     an->a    0.829877
 798     an->a    0.829758
 799   an->the    0.829280
 800   an->the    0.829267
 801     an->a    0.829212
 802    a->the    0.829189
 803   an->the    0.829113
 804     an->a    0.828983
 805    a->the    0.828767
 806    a->the    0.828767
 807    the->a    0.828474
 808   an->the    0.828040
 809    a->the    0.828021
 810   an->the    0.827675
 811    a->the    0.827586
 812    the->a    0.827199
 813     an->a    0.826821
 814    the->a    0.826672
 815   the->an    0.826524
 816    a->the    0.826436
 817    the->a    0.826187
 818     an->a    0.826152
 819    the->a    0.825877
 820     an->a    0.825157
 821     an->a    0.825129
 822     a->an    0.824708
 823    the->a    0.824633
 824     an->a    0.824585
 825     an->a    0.824495
 826   an->the    0.824290
 827    the->a    0.823695
 828   an->the    0.823642
 829   an->the    0.823325
 830    the->a    0.823300
 831    a->the    0.823247
 832    the->a    0.822841
 833     an->a    0.822828
 834   an->the    0.822366
 835     an->a    0.822364
 836   an->the    0.822347
 837    a->the    0.822200
 838    the->a    0.821933
 839   an->the    0.821846
 840   an->the    0.821443
 841   an->the    0.821396
 842    a->the    0.821163
 843    the->a    0.821097
 844    the->a    0.820810
 845    the->a    0.820587
 846     an->a    0.820386
 847    a->the    0.820380
 848    the->a    0.820353
 849    a->the    0.819564
 850    a->the    0.819238
 851   an->the    0.819077
 852    a->the    0.819004
 853     an->a    0.818950
 854     an->a    0.818799
 855     an->a    0.818754
 856    the->a    0.818536
 857   an->the    0.818504
 858     an->a    0.818373
 859    a->the    0.818302
 860   an->the    0.818165
 861     an->a    0.817967
 862    a->the    0.817895
 863    the->a    0.817823
 864    a->the    0.817573
 865     an->a    0.817242
 866    the->a    0.817006
 867    the->a    0.816325
 868     an->a    0.816300
 869   the->an    0.815930
 870    a->the    0.815854
 871    the->a    0.815689
 872    a->the    0.815558
 873     an->a    0.815487
 874     an->a    0.815328
 875    the->a    0.815245
 876    the->a    0.815174
 877   an->the    0.815009
 878   an->the    0.814969
 879     an->a    0.814088
 880     an->a    0.813238
 881   an->the    0.813237
 882   an->the    0.813230
 883   an->the    0.813225
 884   an->the    0.813192
 885   an->the    0.813136
 886     a->an    0.812965
 887     an->a    0.812921
 888    a->the    0.812851
 889    a->the    0.812465
 890   an->the    0.812313
 891     an->a    0.811886
 892   an->the    0.811627
 893    the->a    0.811265
 894    a->the    0.811198
 895   an->the    0.811019
 896   an->the    0.810826
 897    the->a    0.810666
 898    a->the    0.810654
 899   an->the    0.810654
 900     an->a    0.810168
 901    a->the    0.809886
 902    a->the    0.809701
 903   an->the    0.809699
 904   an->the    0.809542
 905   an->the    0.808838
 906    a->the    0.808639
 907    a->the    0.808380
 908     an->a    0.808320
 909     an->a    0.808054
 910     a->an    0.807961
 911    a->the    0.807932
 912   an->the    0.807929
 913     a->an    0.807887
 914     an->a    0.807633
 915    the->a    0.807454
 916    a->the    0.807356
 917   an->the    0.807190
 918    a->the    0.807103
 919    the->a    0.807098
 920    a->the    0.806427
 921     an->a    0.805665
 922     an->a    0.805577
 923    a->the    0.805528
 924    a->the    0.805243
 925   an->the    0.805163
 926     an->a    0.805079
 927    a->the    0.804941
 928    a->the    0.804634
 929     an->a    0.804582
 930     an->a    0.804575
 931     an->a    0.804391
 932    the->a    0.804386
 933    a->the    0.804325
 934    the->a    0.804281
 935   the->an    0.804223
 936   an->the    0.804080
 937   an->the    0.803972
 938     an->a    0.803799
 939     a->an    0.803505
 940    the->a    0.803442
 941     a->an    0.803093
 942   an->the    0.802980
 943     a->an    0.802681
 944    a->the    0.802670
 945    a->the    0.802012
 946    the->a    0.801981
 947   an->the    0.801675
 948   an->the    0.801389
 949    a->the    0.801099
 950     an->a    0.801059
 951    the->a    0.800996
 952    a->the    0.800962
 953   an->the    0.800863
 954    a->the    0.800838
 955    a->the    0.800682
 956   the->an    0.800659
 957   an->the    0.800481
 958     an->a    0.800442
 959   an->the    0.800432
 960   an->the    0.800224
 961    the->a    0.800136
 962   an->the    0.800123
 963   an->the    0.799927
 964     an->a    0.799876
 965   the->an    0.799736
 966    a->the    0.799720
 967   an->the    0.799661
 968   an->the    0.799523
 969    the->a    0.799359
 970   an->the    0.799233
 971     an->a    0.798788
 972    a->the    0.798647
 973    a->the    0.798583
 974   an->the    0.798534
 975    a->the    0.798461
 976     an->a    0.798450
 977    a->the    0.798434
 978   an->the    0.798427
 979    a->the    0.798177
 980    a->the    0.798095
 981    the->a    0.797906
 982    a->the    0.797695
 983    a->the    0.797563
 984    a->the    0.797545
 985   an->the    0.797180
 986     an->a    0.797135
 987    the->a    0.796888
 988   an->the    0.796400
 989    the->a    0.796279
 990   an->the    0.796020
 991   the->an    0.795999
 992     a->an    0.795904
 993   an->the    0.795790
 994    the->a    0.795784
 995     an->a    0.795519
 996    a->the    0.795450
 997     an->a    0.795355
 998    a->the    0.795149
 999   an->the    0.795012
 1000   a->the    0.794829
 1001    an->a    0.794677
 1002  the->an    0.794511
 1003  an->the    0.794480
 1004    an->a    0.794237
 1005    an->a    0.794074
 1006    an->a    0.793592
 1007    an->a    0.793432
 1008  an->the    0.793168
 1009  the->an    0.792885
 1010  an->the    0.792837
 1011  an->the    0.792557
 1012  an->the    0.791973
 1013   a->the    0.791936
 1014  an->the    0.791712
 1015  an->the    0.791014
 1016  an->the    0.790871
 1017   a->the    0.790821
 1018   the->a    0.790755
 1019   the->a    0.790582
 1020   a->the    0.790550
 1021    an->a    0.790475
 1022   a->the    0.790402
 1023   a->the    0.790199
 1024   a->the    0.790094
 1025   a->the    0.790092
 1026  an->the    0.789763
 1027   a->the    0.789710
 1028  an->the    0.789678
 1029   the->a    0.789303
 1030  an->the    0.789179
 1031   a->the    0.788999
 1032  the->an    0.788752
 1033  the->an    0.788752
 1034   the->a    0.788574
 1035  an->the    0.788469
 1036  an->the    0.788382
 1037   a->the    0.788340
 1038   the->a    0.788149
 1039   the->a    0.787801
 1040    an->a    0.787506
 1041   the->a    0.787245
 1042    an->a    0.786712
 1043  an->the    0.786395
 1044   a->the    0.786382
 1045    an->a    0.785850
 1046   the->a    0.785639
 1047   a->the    0.785571
 1048  an->the    0.785399
 1049  an->the    0.785287
 1050   a->the    0.785236
 1051  an->the    0.785153
 1052   a->the    0.784867
 1053  an->the    0.784749
 1054  an->the    0.784730
 1055  an->the    0.784654
 1056    an->a    0.784439
 1057   a->the    0.784329
 1058  an->the    0.784295
 1059   a->the    0.784037
 1060  the->an    0.783773
 1061   the->a    0.783721
 1062  the->an    0.783693
 1063    a->an    0.783652
 1064   the->a    0.783199
 1065  an->the    0.783016
 1066    an->a    0.782600
 1067  an->the    0.781807
 1068    an->a    0.781799
 1069  an->the    0.781611
 1070   a->the    0.781541
 1071   the->a    0.781534,          trans  confidence
 0      an->the    0.999965
 1      an->the    0.999961
 2      an->the    0.999959
 3      an->the    0.999958
 4      an->the    0.999958
 5      an->the    0.999957
 6      an->the    0.999957
 7       a->the    0.999957
 8      an->the    0.999956
 9      an->the    0.999956
 10     an->the    0.999956
 11     an->the    0.999956
 12      a->the    0.999955
 13      a->the    0.999952
 14      a->the    0.999952
 15      a->the    0.999951
 16      a->the    0.999950
 17     an->the    0.999949
 18      a->the    0.999948
 19      a->the    0.999948
 20     an->the    0.999947
 21      a->the    0.999947
 22      a->the    0.999947
 23     an->the    0.999945
 24     an->the    0.999945
 25      a->the    0.999945
 26      a->the    0.999944
 27      a->the    0.999943
 28      a->the    0.999943
 29     an->the    0.999942
 30      a->the    0.999941
 31     an->the    0.999941
 32     an->the    0.999941
 33     an->the    0.999941
 34      a->the    0.999940
 35      a->the    0.999940
 36      a->the    0.999939
 37     an->the    0.999939
 38     an->the    0.999938
 39     an->the    0.999938
 40     an->the    0.999938
 41     an->the    0.999937
 42     an->the    0.999937
 43     an->the    0.999937
 44      a->the    0.999937
 45     an->the    0.999936
 46      a->the    0.999935
 47      a->the    0.999934
 48      a->the    0.999934
 49      a->the    0.999934
 50      a->the    0.999934
 51     an->the    0.999934
 52     an->the    0.999933
 53      a->the    0.999933
 54     an->the    0.999932
 55      a->the    0.999932
 56     an->the    0.999932
 57     an->the    0.999931
 58      a->the    0.999931
 59      a->the    0.999930
 60      a->the    0.999930
 61     an->the    0.999930
 62     an->the    0.999930
 63     an->the    0.999929
 64      a->the    0.999929
 65     an->the    0.999928
 66     an->the    0.999928
 67     an->the    0.999928
 68      a->the    0.999928
 69      a->the    0.999928
 70     an->the    0.999928
 71      a->the    0.999926
 72     an->the    0.999924
 73      a->the    0.999924
 74      a->the    0.999924
 75      a->the    0.999924
 76      a->the    0.999924
 77      a->the    0.999923
 78      a->the    0.999923
 79     an->the    0.999923
 80     an->the    0.999923
 81      a->the    0.999922
 82      a->the    0.999922
 83     an->the    0.999922
 84      a->the    0.999922
 85     an->the    0.999921
 86      a->the    0.999921
 87     an->the    0.999921
 88      a->the    0.999920
 89      a->the    0.999920
 90     an->the    0.999919
 91      a->the    0.999919
 92      a->the    0.999919
 93     an->the    0.999919
 94     an->the    0.999919
 95     an->the    0.999918
 96     an->the    0.999918
 97     an->the    0.999918
 98      a->the    0.999918
 99     an->the    0.999918
 100    an->the    0.999918
 101     a->the    0.999917
 102    an->the    0.999917
 103     a->the    0.999917
 104    an->the    0.999917
 105     a->the    0.999916
 106    an->the    0.999916
 107     a->the    0.999916
 108    an->the    0.999916
 109     a->the    0.999916
 110    an->the    0.999916
 111    an->the    0.999915
 112    an->the    0.999915
 113    an->the    0.999915
 114     a->the    0.999915
 115     a->the    0.999915
 116    an->the    0.999915
 117    an->the    0.999915
 118    an->the    0.999915
 119     a->the    0.999914
 120    an->the    0.999914
 121     a->the    0.999913
 122     a->the    0.999913
 123    an->the    0.999913
 124     a->the    0.999913
 125     a->the    0.999913
 126     a->the    0.999913
 127     a->the    0.999913
 128    an->the    0.999912
 129     a->the    0.999912
 130    an->the    0.999912
 131    an->the    0.999912
 132    an->the    0.999912
 133    an->the    0.999912
 134     a->the    0.999911
 135    an->the    0.999911
 136    an->the    0.999911
 137     a->the    0.999911
 138     a->the    0.999911
 139    an->the    0.999911
 140     the->a    0.999910
 141     a->the    0.999910
 142    an->the    0.999910
 143    an->the    0.999909
 144     a->the    0.999909
 145    an->the    0.999909
 146     a->the    0.999909
 147    an->the    0.999909
 148    an->the    0.999909
 149    an->the    0.999909
 150    an->the    0.999909
 151     a->the    0.999909
 152    an->the    0.999908
 153     a->the    0.999908
 154     a->the    0.999907
 155     a->the    0.999907
 156     a->the    0.999907
 157     a->the    0.999907
 158      an->a    0.999907
 159      an->a    0.999907
 160     a->the    0.999907
 161     a->the    0.999906
 162     a->the    0.999906
 163    an->the    0.999906
 164     a->the    0.999906
 165     a->the    0.999906
 166     a->the    0.999905
 167     a->the    0.999905
 168    an->the    0.999905
 169    an->the    0.999905
 170    an->the    0.999905
 171    an->the    0.999905
 172     a->the    0.999905
 173    an->the    0.999905
 174     a->the    0.999905
 175     a->the    0.999905
 176    an->the    0.999904
 177    an->the    0.999904
 178    an->the    0.999904
 179    an->the    0.999904
 180     a->the    0.999904
 181    an->the    0.999904
 182    an->the    0.999904
 183    an->the    0.999904
 184    an->the    0.999904
 185     a->the    0.999904
 186    an->the    0.999904
 187     a->the    0.999904
 188     a->the    0.999903
 189     a->the    0.999903
 190     a->the    0.999903
 191     a->the    0.999903
 192    an->the    0.999903
 193     a->the    0.999903
 194    an->the    0.999903
 195    an->the    0.999903
 196    an->the    0.999902
 197     a->the    0.999902
 198    an->the    0.999902
 199    an->the    0.999901
 200    an->the    0.999901
 201    an->the    0.999901
 202     a->the    0.999901
 203    an->the    0.999901
 204    an->the    0.999901
 205    an->the    0.999901
 206    an->the    0.999900
 207     a->the    0.999900
 208     a->the    0.999900
 209    an->the    0.999900
 210     a->the    0.999899
 211     a->the    0.999899
 212     a->the    0.999899
 213    an->the    0.999899
 214     a->the    0.999899
 215    an->the    0.999899
 216    an->the    0.999899
 217    an->the    0.999898
 218    an->the    0.999898
 219     a->the    0.999898
 220    an->the    0.999898
 221    an->the    0.999898
 222      an->a    0.999898
 223     a->the    0.999897
 224    an->the    0.999897
 225    an->the    0.999897
 226    an->the    0.999896
 227     a->the    0.999896
 228     a->the    0.999896
 229     the->a    0.999896
 230    an->the    0.999895
 231    an->the    0.999895
 232     a->the    0.999895
 233      an->a    0.999895
 234     a->the    0.999895
 235     a->the    0.999895
 236     a->the    0.999895
 237     a->the    0.999894
 238     the->a    0.999894
 239    an->the    0.999894
 240    an->the    0.999894
 241     a->the    0.999894
 242    an->the    0.999894
 243     a->the    0.999894
 244     a->the    0.999894
 245    an->the    0.999894
 246     the->a    0.999894
 247     a->the    0.999894
 248    an->the    0.999894
 249    an->the    0.999894
 250    an->the    0.999893
 251    an->the    0.999893
 252    an->the    0.999893
 253     a->the    0.999893
 254    an->the    0.999893
 255    an->the    0.999892
 256     a->the    0.999892
 257     a->the    0.999892
 258     a->the    0.999892
 259     a->the    0.999892
 260     a->the    0.999891
 261    an->the    0.999891
 262     a->the    0.999891
 263    an->the    0.999891
 264    an->the    0.999891
 265     a->the    0.999891
 266    an->the    0.999891
 267     a->the    0.999891
 268      an->a    0.999891
 269     a->the    0.999891
 270     a->the    0.999890
 271    an->the    0.999890
 272    an->the    0.999890
 273    an->the    0.999890
 274     a->the    0.999890
 275    an->the    0.999890
 276    an->the    0.999890
 277     a->the    0.999890
 278    an->the    0.999890
 279    an->the    0.999889
 280     a->the    0.999889
 281     the->a    0.999889
 282     a->the    0.999889
 283     a->the    0.999889
 284     a->the    0.999889
 285     a->the    0.999888
 286    an->the    0.999888
 287    an->the    0.999888
 288    an->the    0.999888
 289     the->a    0.999888
 290     a->the    0.999888
 291    an->the    0.999888
 292    an->the    0.999888
 293     a->the    0.999888
 294      an->a    0.999888
 295     a->the    0.999888
 296    an->the    0.999888
 297      an->a    0.999888
 298    an->the    0.999888
 299     a->the    0.999887
 300     a->the    0.999887
 301     a->the    0.999887
 302    an->the    0.999887
 303     a->the    0.999887
 304     a->the    0.999887
 305    an->the    0.999887
 306     a->the    0.999886
 307     a->the    0.999886
 308    an->the    0.999886
 309     a->the    0.999886
 310     a->the    0.999886
 311     a->the    0.999885
 312    an->the    0.999885
 313    an->the    0.999885
 314     a->the    0.999885
 315     a->the    0.999885
 316     a->the    0.999885
 317    an->the    0.999885
 318    an->the    0.999885
 319     a->the    0.999885
 320     a->the    0.999885
 321     a->the    0.999885
 322    an->the    0.999885
 323     a->the    0.999884
 324    an->the    0.999884
 325     a->the    0.999884
 326     a->the    0.999884
 327     a->the    0.999884
 328    an->the    0.999884
 329     a->the    0.999884
 330    an->the    0.999883
 331     a->the    0.999883
 332     the->a    0.999883
 333     a->the    0.999883
 334    an->the    0.999882
 335    an->the    0.999882
 336    an->the    0.999882
 337    an->the    0.999882
 338    an->the    0.999882
 339    an->the    0.999882
 340    an->the    0.999882
 341    an->the    0.999882
 342    an->the    0.999882
 343     a->the    0.999882
 344     a->the    0.999882
 345    an->the    0.999882
 346    an->the    0.999881
 347    an->the    0.999881
 348    an->the    0.999881
 349     a->the    0.999881
 350    an->the    0.999881
 351    an->the    0.999881
 352     a->the    0.999880
 353     a->the    0.999880
 354     a->the    0.999880
 355    an->the    0.999880
 356    an->the    0.999880
 357    an->the    0.999880
 358     a->the    0.999880
 359     a->the    0.999880
 360    an->the    0.999880
 361     a->the    0.999879
 362     a->the    0.999879
 363    an->the    0.999879
 364    an->the    0.999879
 365    an->the    0.999879
 366    an->the    0.999879
 367    an->the    0.999878
 368     a->the    0.999878
 369    an->the    0.999878
 370     a->the    0.999878
 371     a->the    0.999878
 372     a->the    0.999878
 373    an->the    0.999878
 374    an->the    0.999878
 375     the->a    0.999878
 376    an->the    0.999878
 377    an->the    0.999877
 378    an->the    0.999877
 379    an->the    0.999877
 380    an->the    0.999877
 381    an->the    0.999877
 382     a->the    0.999877
 383     a->the    0.999877
 384     a->the    0.999877
 385     a->the    0.999877
 386     a->the    0.999877
 387     a->the    0.999877
 388     a->the    0.999877
 389     a->the    0.999877
 390    an->the    0.999877
 391     a->the    0.999877
 392    an->the    0.999876
 393     a->the    0.999876
 394    an->the    0.999876
 395     the->a    0.999876
 396     a->the    0.999876
 397     a->the    0.999876
 398     a->the    0.999876
 399    an->the    0.999875
 400     a->the    0.999875
 401     a->the    0.999875
 402    an->the    0.999875
 403    an->the    0.999875
 404     a->the    0.999874
 405     a->the    0.999874
 406     a->the    0.999874
 407    an->the    0.999874
 408     a->the    0.999873
 409    an->the    0.999873
 410    an->the    0.999873
 411     a->the    0.999873
 412    an->the    0.999873
 413     the->a    0.999872
 414     a->the    0.999872
 415    an->the    0.999872
 416     a->the    0.999872
 417    an->the    0.999871
 418    an->the    0.999871
 419    an->the    0.999871
 420    an->the    0.999871
 421    an->the    0.999871
 422    an->the    0.999871
 423    an->the    0.999871
 424      an->a    0.999871
 425    an->the    0.999870
 426      an->a    0.999870
 427    an->the    0.999870
 428    an->the    0.999870
 429     a->the    0.999870
 430     a->the    0.999870
 431     a->the    0.999870
 432    an->the    0.999870
 433     a->the    0.999870
 434     a->the    0.999869
 435    an->the    0.999869
 436    an->the    0.999869
 437    an->the    0.999869
 438     a->the    0.999869
 439     the->a    0.999869
 440    an->the    0.999869
 441    an->the    0.999869
 442     a->the    0.999869
 443     a->the    0.999868
 444    an->the    0.999868
 445    an->the    0.999868
 446     a->the    0.999868
 447    an->the    0.999868
 448    an->the    0.999868
 449     a->the    0.999868
 450    an->the    0.999868
 451     a->the    0.999867
 452    an->the    0.999867
 453     a->the    0.999867
 454    an->the    0.999866
 455     a->the    0.999866
 456     a->the    0.999866
 457     a->the    0.999865
 458    an->the    0.999865
 459     a->the    0.999865
 460    an->the    0.999865
 461     a->the    0.999865
 462    an->the    0.999865
 463    an->the    0.999864
 464    an->the    0.999864
 465    an->the    0.999864
 466     a->the    0.999864
 467     a->the    0.999864
 468    an->the    0.999864
 469     a->the    0.999864
 470     a->the    0.999863
 471     a->the    0.999863
 472     a->the    0.999863
 473    an->the    0.999863
 474     a->the    0.999863
 475     a->the    0.999863
 476    an->the    0.999863
 477     a->the    0.999863
 478     a->the    0.999862
 479    an->the    0.999862
 480    an->the    0.999862
 481    an->the    0.999862
 482    an->the    0.999862
 483     a->the    0.999862
 484    an->the    0.999861
 485      an->a    0.999861
 486     a->the    0.999861
 487     a->the    0.999861
 488     a->the    0.999861
 489    an->the    0.999861
 490    an->the    0.999861
 491    an->the    0.999861
 492    an->the    0.999861
 493    an->the    0.999861
 494    an->the    0.999860
 495     a->the    0.999860
 496    an->the    0.999860
 497     the->a    0.999860
 498     a->the    0.999860
 499     a->the    0.999860
 500    an->the    0.999860
 501     a->the    0.999860
 502    an->the    0.999860
 503    an->the    0.999860
 504    an->the    0.999859
 505     a->the    0.999859
 506    an->the    0.999859
 507     a->the    0.999859
 508    an->the    0.999858
 509     a->the    0.999858
 510    an->the    0.999858
 511     a->the    0.999858
 512    an->the    0.999857
 513     a->the    0.999857
 514    an->the    0.999857
 515    an->the    0.999857
 516    an->the    0.999857
 517      an->a    0.999857
 518    an->the    0.999856
 519    an->the    0.999856
 520     a->the    0.999856
 521     a->the    0.999856
 522    an->the    0.999856
 523    an->the    0.999856
 524     a->the    0.999856
 525    an->the    0.999856
 526    an->the    0.999856
 527     a->the    0.999855
 528    an->the    0.999855
 529    an->the    0.999855
 530     a->the    0.999855
 531     a->the    0.999855
 532    an->the    0.999854
 533     a->the    0.999854
 534    an->the    0.999854
 535     a->the    0.999854
 536     a->the    0.999854
 537    an->the    0.999853
 538    an->the    0.999853
 539     a->the    0.999853
 540    an->the    0.999853
 541    an->the    0.999853
 542     a->the    0.999853
 543    an->the    0.999853
 544    an->the    0.999852
 545     a->the    0.999852
 546    an->the    0.999851
 547     a->the    0.999851
 548    an->the    0.999851
 549    an->the    0.999851
 550    an->the    0.999851
 551     a->the    0.999850
 552    an->the    0.999850
 553    an->the    0.999850
 554     a->the    0.999850
 555     a->the    0.999850
 556     a->the    0.999850
 557    an->the    0.999850
 558    an->the    0.999849
 559    an->the    0.999849
 560     a->the    0.999849
 561     a->the    0.999849
 562     a->the    0.999849
 563     a->the    0.999849
 564    an->the    0.999849
 565     a->the    0.999849
 566    an->the    0.999848
 567     a->the    0.999848
 568    an->the    0.999848
 569     a->the    0.999848
 570    an->the    0.999848
 571    an->the    0.999848
 572    an->the    0.999847
 573     a->the    0.999847
 574    an->the    0.999847
 575    an->the    0.999847
 576    an->the    0.999847
 577    an->the    0.999846
 578    an->the    0.999846
 579    an->the    0.999846
 580    an->the    0.999846
 581     a->the    0.999846
 582    an->the    0.999846
 583     a->the    0.999846
 584     a->the    0.999846
 585     a->the    0.999845
 586    an->the    0.999845
 587    an->the    0.999845
 588    an->the    0.999845
 589    an->the    0.999845
 590    an->the    0.999845
 591     a->the    0.999844
 592     a->the    0.999844
 593     a->the    0.999844
 594    an->the    0.999844
 595    an->the    0.999844
 596     a->the    0.999844
 597    an->the    0.999843
 598    an->the    0.999843
 599     a->the    0.999843
 600    an->the    0.999843
 601     a->the    0.999843
 602    an->the    0.999843
 603     the->a    0.999843
 604    an->the    0.999843
 605    an->the    0.999843
 606    an->the    0.999842
 607    an->the    0.999842
 608     a->the    0.999842
 609     a->the    0.999842
 610    an->the    0.999841
 611    an->the    0.999841
 612    an->the    0.999841
 613    an->the    0.999841
 614     a->the    0.999841
 615    an->the    0.999841
 616     a->the    0.999841
 617     a->the    0.999841
 618    an->the    0.999841
 619    an->the    0.999841
 620    an->the    0.999841
 621     a->the    0.999841
 622    an->the    0.999841
 623    an->the    0.999840
 624     the->a    0.999840
 625      an->a    0.999840
 626    an->the    0.999839
 627     a->the    0.999839
 628     a->the    0.999839
 629    an->the    0.999839
 630    an->the    0.999839
 631     a->the    0.999839
 632    an->the    0.999838
 633     a->the    0.999838
 634    an->the    0.999838
 635      an->a    0.999838
 636    an->the    0.999837
 637    an->the    0.999837
 638     a->the    0.999837
 639    an->the    0.999837
 640     a->the    0.999837
 641     a->the    0.999837
 642    an->the    0.999837
 643    an->the    0.999837
 644    an->the    0.999837
 645     a->the    0.999837
 646    an->the    0.999837
 647    an->the    0.999837
 648     a->the    0.999836
 649     the->a    0.999836
 650     a->the    0.999836
 651    an->the    0.999836
 652    an->the    0.999836
 653     a->the    0.999836
 654     a->the    0.999836
 655     a->the    0.999836
 656     a->the    0.999836
 657    an->the    0.999835
 658     a->the    0.999835
 659     a->the    0.999835
 660    an->the    0.999835
 661    an->the    0.999835
 662     a->the    0.999835
 663     a->the    0.999835
 664     the->a    0.999834
 665    an->the    0.999834
 666     a->the    0.999834
 667    an->the    0.999834
 668    an->the    0.999833
 669     a->the    0.999833
 670     a->the    0.999833
 671     a->the    0.999833
 672     a->the    0.999833
 673    an->the    0.999833
 674     a->the    0.999833
 675    an->the    0.999833
 676    an->the    0.999832
 677     a->the    0.999832
 678     a->the    0.999832
 679    an->the    0.999832
 680    an->the    0.999832
 681    an->the    0.999832
 682     a->the    0.999832
 683    an->the    0.999832
 684    an->the    0.999832
 685    an->the    0.999832
 686     a->the    0.999831
 687    an->the    0.999831
 688    an->the    0.999831
 689      an->a    0.999831
 690     a->the    0.999831
 691    an->the    0.999831
 692      an->a    0.999831
 693     a->the    0.999831
 694     a->the    0.999830
 695     the->a    0.999830
 696    an->the    0.999830
 697    an->the    0.999830
 698     a->the    0.999830
 699     a->the    0.999830
 700      an->a    0.999830
 701     a->the    0.999830
 702     a->the    0.999829
 703    an->the    0.999829
 704    an->the    0.999829
 705     a->the    0.999829
 706     a->the    0.999829
 707     a->the    0.999829
 708    an->the    0.999829
 709     a->the    0.999828
 710    an->the    0.999828
 711    an->the    0.999828
 712     a->the    0.999828
 713     a->the    0.999828
 714    an->the    0.999828
 715     the->a    0.999828
 716    an->the    0.999828
 717    an->the    0.999827
 718    an->the    0.999827
 719    an->the    0.999827
 720     a->the    0.999827
 721    an->the    0.999827
 722     a->the    0.999827
 723     a->the    0.999827
 724     a->the    0.999827
 725     a->the    0.999826
 726    an->the    0.999826
 727     a->the    0.999826
 728    an->the    0.999826
 729    an->the    0.999826
 730    an->the    0.999825
 731    an->the    0.999825
 732    an->the    0.999825
 733    an->the    0.999825
 734    an->the    0.999825
 735     a->the    0.999825
 736    an->the    0.999825
 737     a->the    0.999825
 738     a->the    0.999825
 739     a->the    0.999825
 740     a->the    0.999825
 741     a->the    0.999825
 742     a->the    0.999825
 743    an->the    0.999825
 744    an->the    0.999825
 745    an->the    0.999824
 746    an->the    0.999824
 747      an->a    0.999824
 748    an->the    0.999824
 749     a->the    0.999824
 750     a->the    0.999824
 751     a->the    0.999824
 752     a->the    0.999823
 753    an->the    0.999823
 754     a->the    0.999823
 755      an->a    0.999823
 756    an->the    0.999822
 757     a->the    0.999822
 758    an->the    0.999822
 759     a->the    0.999822
 760    an->the    0.999821
 761     a->the    0.999821
 762      an->a    0.999821
 763     a->the    0.999821
 764     a->the    0.999821
 765    an->the    0.999821
 766    an->the    0.999820
 767    an->the    0.999820
 768    an->the    0.999820
 769     a->the    0.999820
 770     a->the    0.999820
 771     a->the    0.999820
 772    an->the    0.999820
 773    an->the    0.999820
 774     a->the    0.999820
 775     a->the    0.999819
 776    an->the    0.999819
 777     a->the    0.999819
 778    an->the    0.999819
 779     a->the    0.999819
 780    an->the    0.999819
 781    an->the    0.999818
 782     a->the    0.999818
 783    an->the    0.999818
 784     a->the    0.999818
 785     a->the    0.999818
 786     a->the    0.999817
 787     a->the    0.999816
 788     a->the    0.999816
 789     a->the    0.999816
 790     a->the    0.999816
 791    an->the    0.999816
 792    an->the    0.999815
 793     a->the    0.999815
 794    an->the    0.999815
 795     a->the    0.999815
 796     a->the    0.999815
 797     a->the    0.999814
 798     a->the    0.999814
 799    an->the    0.999814
 800    an->the    0.999814
 801    an->the    0.999814
 802     a->the    0.999814
 803     a->the    0.999814
 804     the->a    0.999814
 805    an->the    0.999814
 806    an->the    0.999814
 807     a->the    0.999814
 808    an->the    0.999813
 809    an->the    0.999813
 810     a->the    0.999813
 811    an->the    0.999813
 812    an->the    0.999813
 813     a->the    0.999813
 814     a->the    0.999813
 815    an->the    0.999813
 816    an->the    0.999813
 817    an->the    0.999812
 818    an->the    0.999812
 819     a->the    0.999812
 820    an->the    0.999812
 821     the->a    0.999812
 822    an->the    0.999811
 823     a->the    0.999811
 824    an->the    0.999811
 825    an->the    0.999811
 826    an->the    0.999811
 827    an->the    0.999811
 828    an->the    0.999810
 829     a->the    0.999810
 830    an->the    0.999810
 831     a->the    0.999810
 832    an->the    0.999810
 833    an->the    0.999810
 834    an->the    0.999810
 835    an->the    0.999810
 836    an->the    0.999810
 837    an->the    0.999809
 838    an->the    0.999809
 839    an->the    0.999809
 840    an->the    0.999809
 841     a->the    0.999809
 842    an->the    0.999808
 843    an->the    0.999808
 844    an->the    0.999808
 845    an->the    0.999808
 846    an->the    0.999808
 847     a->the    0.999808
 848     a->the    0.999808
 849     a->the    0.999808
 850     a->the    0.999807
 851     a->the    0.999807
 852     a->the    0.999807
 853    an->the    0.999807
 854     the->a    0.999807
 855    an->the    0.999807
 856    an->the    0.999807
 857    an->the    0.999807
 858     the->a    0.999807
 859     a->the    0.999806
 860     the->a    0.999806
 861    an->the    0.999806
 862     a->the    0.999806
 863    an->the    0.999806
 864    an->the    0.999806
 865    an->the    0.999806
 866     a->the    0.999806
 867    an->the    0.999806
 868    an->the    0.999805
 869    an->the    0.999805
 870     a->the    0.999805
 871    an->the    0.999805
 872     a->the    0.999805
 873     a->the    0.999805
 874    an->the    0.999805
 875     a->the    0.999804
 876    an->the    0.999804
 877     a->the    0.999804
 878      an->a    0.999804
 879    an->the    0.999804
 880     a->the    0.999804
 881     a->the    0.999803
 882     a->the    0.999803
 883     a->the    0.999803
 884    an->the    0.999803
 885    an->the    0.999803
 886     a->the    0.999803
 887    an->the    0.999803
 888    an->the    0.999803
 889     a->the    0.999803
 890     a->the    0.999803
 891     a->the    0.999801
 892     a->the    0.999801
 893    an->the    0.999801
 894     a->the    0.999801
 895     a->the    0.999800
 896     a->the    0.999800
 897     a->the    0.999800
 898    an->the    0.999800
 899     a->the    0.999800
 900    an->the    0.999800
 901    an->the    0.999800
 902     a->the    0.999799
 903     a->the    0.999799
 904     a->the    0.999798
 905      an->a    0.999798
 906    an->the    0.999798
 907    an->the    0.999797
 908    an->the    0.999797
 909     a->the    0.999797
 910    an->the    0.999797
 911    an->the    0.999797
 912    an->the    0.999797
 913    an->the    0.999797
 914    an->the    0.999796
 915    an->the    0.999796
 916     a->the    0.999796
 917    an->the    0.999796
 918     a->the    0.999796
 919     a->the    0.999796
 920    an->the    0.999796
 921     a->the    0.999795
 922     the->a    0.999795
 923    an->the    0.999795
 924     a->the    0.999795
 925    an->the    0.999795
 926    an->the    0.999795
 927     the->a    0.999794
 928     a->the    0.999794
 929    an->the    0.999794
 930    an->the    0.999794
 931     a->the    0.999794
 932     a->the    0.999794
 933    an->the    0.999793
 934     a->the    0.999793
 935    an->the    0.999793
 936    an->the    0.999793
 937     a->the    0.999793
 938     a->the    0.999793
 939     a->the    0.999793
 940     a->the    0.999792
 941     a->the    0.999792
 942     a->the    0.999792
 943    an->the    0.999792
 944     a->the    0.999792
 945     the->a    0.999792
 946    an->the    0.999792
 947    an->the    0.999792
 948      an->a    0.999792
 949     a->the    0.999792
 950    an->the    0.999792
 951     a->the    0.999792
 952    an->the    0.999791
 953    an->the    0.999791
 954     a->the    0.999791
 955    an->the    0.999791
 956    an->the    0.999791
 957     a->the    0.999791
 958    an->the    0.999791
 959     a->the    0.999791
 960    an->the    0.999791
 961     a->the    0.999790
 962    an->the    0.999790
 963     a->the    0.999790
 964     the->a    0.999790
 965     a->the    0.999790
 966    an->the    0.999790
 967    an->the    0.999790
 968    an->the    0.999790
 969    an->the    0.999790
 970    an->the    0.999789
 971      an->a    0.999789
 972    an->the    0.999789
 973     a->the    0.999789
 974    an->the    0.999789
 975     a->the    0.999789
 976     a->the    0.999788
 977     a->the    0.999788
 978     a->the    0.999788
 979     a->the    0.999788
 980    an->the    0.999788
 981     a->the    0.999788
 982    an->the    0.999788
 983    an->the    0.999787
 984    an->the    0.999787
 985     a->the    0.999787
 986    an->the    0.999787
 987     a->the    0.999787
 988     a->the    0.999786
 989    an->the    0.999786
 990    an->the    0.999786
 991    an->the    0.999785
 992     a->the    0.999785
 993    an->the    0.999785
 994    an->the    0.999785
 995    an->the    0.999785
 996     a->the    0.999785
 997     a->the    0.999785
 998    an->the    0.999785
 999     a->the    0.999785
 1000   an->the    0.999784
 1001   an->the    0.999784
 1002   an->the    0.999784
 1003   an->the    0.999784
 1004   an->the    0.999784
 1005    a->the    0.999784
 1006    a->the    0.999784
 1007    a->the    0.999784
 1008   an->the    0.999784
 1009   an->the    0.999784
 1010   an->the    0.999784
 1011   an->the    0.999784
 1012   an->the    0.999784
 1013   an->the    0.999783
 1014   an->the    0.999783
 1015   an->the    0.999783
 1016    a->the    0.999783
 1017   an->the    0.999783
 1018    a->the    0.999783
 1019   an->the    0.999783
 1020   an->the    0.999783
 1021   an->the    0.999783
 1022    the->a    0.999782
 1023    a->the    0.999782
 1024   an->the    0.999782
 1025   an->the    0.999782
 1026   an->the    0.999782
 1027    a->the    0.999782
 1028   an->the    0.999781
 1029    a->the    0.999781
 1030    a->the    0.999781
 1031   an->the    0.999781
 1032   an->the    0.999781
 1033    a->the    0.999781
 1034   an->the    0.999781
 1035   an->the    0.999781
 1036     an->a    0.999781
 1037   an->the    0.999780
 1038   an->the    0.999780
 1039    a->the    0.999780
 1040    the->a    0.999780
 1041    a->the    0.999780
 1042   an->the    0.999780
 1043   an->the    0.999780
 1044    a->the    0.999780
 1045   an->the    0.999780
 1046    a->the    0.999779
 1047    a->the    0.999779
 1048   an->the    0.999779
 1049   an->the    0.999778
 1050   an->the    0.999778
 1051    a->the    0.999778
 1052   an->the    0.999778
 1053   an->the    0.999778
 1054   an->the    0.999778
 1055   an->the    0.999778
 1056    a->the    0.999778
 1057    the->a    0.999777
 1058    a->the    0.999777
 1059   an->the    0.999777
 1060   an->the    0.999777
 1061   an->the    0.999777
 1062    a->the    0.999777
 1063   an->the    0.999777
 1064   an->the    0.999777
 1065    a->the    0.999777
 1066   an->the    0.999776
 1067    a->the    0.999776
 1068    a->the    0.999776
 1069    a->the    0.999776
 1070   an->the    0.999776
 1071   an->the    0.999776
 1072    a->the    0.999776
 1073    a->the    0.999776
 1074     an->a    0.999776
 1075   an->the    0.999775
 1076   an->the    0.999775
 1077   an->the    0.999775
 1078     an->a    0.999775
 1079    a->the    0.999775
 1080    a->the    0.999775
 1081    a->the    0.999774
 1082   an->the    0.999774
 1083   an->the    0.999774
 1084   an->the    0.999773
 1085   an->the    0.999773
 1086    a->the    0.999773
 1087    a->the    0.999773
 1088    a->the    0.999773
 1089   an->the    0.999773
 1090   an->the    0.999773
 1091    a->the    0.999773
 1092    a->the    0.999772
 1093   an->the    0.999772
 1094   an->the    0.999772
 1095    a->the    0.999772
 1096   an->the    0.999772
 1097   an->the    0.999772
 1098   an->the    0.999772
 1099   an->the    0.999772
 1100   an->the    0.999771
 1101    a->the    0.999771
 1102   an->the    0.999771
 1103    a->the    0.999771
 1104   an->the    0.999771
 1105    a->the    0.999771
 1106    a->the    0.999771
 1107   an->the    0.999770
 1108   an->the    0.999770
 1109   an->the    0.999770
 1110   an->the    0.999770
 1111   an->the    0.999770
 1112     an->a    0.999770
 1113    a->the    0.999770
 1114   an->the    0.999770
 1115    a->the    0.999770
 1116    a->the    0.999770
 1117   an->the    0.999770
 1118   an->the    0.999769
 1119    a->the    0.999769
 1120   an->the    0.999769
 1121    a->the    0.999769
 1122   an->the    0.999769
 1123   an->the    0.999769
 1124   an->the    0.999769
 1125   an->the    0.999769
 1126   an->the    0.999769
 1127   an->the    0.999769
 1128    a->the    0.999768
 1129   an->the    0.999768
 1130    a->the    0.999768
 1131   an->the    0.999768
 1132   an->the    0.999768
 1133    a->the    0.999768
 1134    a->the    0.999768
 1135   an->the    0.999767
 1136   an->the    0.999767
 1137   an->the    0.999767
 1138   an->the    0.999767
 1139   an->the    0.999767
 1140   an->the    0.999766
 1141    a->the    0.999766
 1142     an->a    0.999766
 1143    a->the    0.999766
 1144    a->the    0.999766
 1145    a->the    0.999766
 1146    a->the    0.999766
 1147   an->the    0.999766
 1148    a->the    0.999765
 1149    a->the    0.999765
 1150   an->the    0.999765
 1151   an->the    0.999765
 1152   an->the    0.999765
 1153   an->the    0.999765
 1154   an->the    0.999765
 1155    a->the    0.999765
 1156    a->the    0.999765
 1157   an->the    0.999763
 1158    a->the    0.999763
 1159    a->the    0.999763
 1160    a->the    0.999763
 1161    a->the    0.999762
 1162   an->the    0.999762
 1163   an->the    0.999762
 1164    a->the    0.999762
 1165   an->the    0.999761
 1166    the->a    0.999761
 1167   an->the    0.999761
 1168    a->the    0.999761
 1169   an->the    0.999761
 1170    a->the    0.999761
 1171   an->the    0.999760
 1172   an->the    0.999760
 1173   an->the    0.999760
 1174   an->the    0.999760
 1175   an->the    0.999759
 1176     an->a    0.999759
 1177    a->the    0.999759
 1178   an->the    0.999759
 1179   an->the    0.999759
 1180   an->the    0.999759
 1181    a->the    0.999759
 1182    a->the    0.999759
 1183   an->the    0.999759
 1184    a->the    0.999758
 1185   an->the    0.999758
 1186   an->the    0.999758
 1187    a->the    0.999758
 1188   an->the    0.999758
 1189    a->the    0.999757
 1190   an->the    0.999757
 1191    a->the    0.999757
 1192   an->the    0.999757
 1193   an->the    0.999757
 1194    the->a    0.999757
 1195    a->the    0.999757
 1196    a->the    0.999757
 1197   an->the    0.999757
 1198    a->the    0.999757
 1199   an->the    0.999757
 1200    a->the    0.999757
 1201    a->the    0.999756
 1202    a->the    0.999756
 1203    a->the    0.999756
 1204    a->the    0.999755
 1205   an->the    0.999755
 1206   an->the    0.999755
 1207    a->the    0.999755
 1208   an->the    0.999755
 1209    a->the    0.999755
 1210   an->the    0.999754
 1211    a->the    0.999754
 1212   an->the    0.999754
 1213   an->the    0.999754
 1214    a->the    0.999754
 1215   an->the    0.999754
 1216    a->the    0.999754
 1217    a->the    0.999753
 1218    the->a    0.999753
 1219   an->the    0.999753
 1220    a->the    0.999753
 1221    the->a    0.999753
 1222   an->the    0.999753
 1223   an->the    0.999753
 1224   an->the    0.999752
 1225    a->the    0.999752
 1226    a->the    0.999752
 1227    a->the    0.999752
 1228   an->the    0.999752
 1229   an->the    0.999751
 1230   an->the    0.999751
 1231    a->the    0.999751
 1232   an->the    0.999751
 1233    a->the    0.999751
 1234   an->the    0.999751
 1235   an->the    0.999751
 1236   an->the    0.999751
 1237   an->the    0.999750
 1238   an->the    0.999750
 1239    a->the    0.999750
 1240   an->the    0.999750
 1241   an->the    0.999750
 1242   an->the    0.999750
 1243    a->the    0.999750
 1244   an->the    0.999750
 1245     an->a    0.999750
 1246    a->the    0.999749
 1247   an->the    0.999749
 1248   an->the    0.999749
 1249    a->the    0.999749
 1250   an->the    0.999749
 1251   an->the    0.999749
 1252   an->the    0.999749
 1253   an->the    0.999749
 1254   an->the    0.999749
 1255    a->the    0.999749
 1256   an->the    0.999749
 1257    the->a    0.999749
 1258   an->the    0.999749
 1259    the->a    0.999749
 1260   an->the    0.999748
 1261    a->the    0.999748
 1262   an->the    0.999748
 1263   an->the    0.999748
 1264    a->the    0.999748
 1265   an->the    0.999748
 1266   an->the    0.999748
 1267   an->the    0.999748
 1268    the->a    0.999748
 1269   an->the    0.999748
 1270   an->the    0.999747
 1271    a->the    0.999747
 1272   an->the    0.999747
 1273   an->the    0.999747
 1274   an->the    0.999747
 1275    a->the    0.999747
 1276    a->the    0.999747
 1277    a->the    0.999747
 1278    a->the    0.999747
 1279   an->the    0.999746
 1280    a->the    0.999746
 1281    a->the    0.999746
 1282   an->the    0.999745
 1283   an->the    0.999745
 1284    a->the    0.999745
 1285    a->the    0.999745
 1286    a->the    0.999745
 1287   an->the    0.999745
 1288   an->the    0.999744
 1289    a->the    0.999744
 1290   an->the    0.999744
 1291    a->the    0.999744
 1292   an->the    0.999744
 1293   an->the    0.999743
 1294   an->the    0.999743
 1295   an->the    0.999743
 1296    a->the    0.999743
 1297   an->the    0.999743
 1298    a->the    0.999743
 1299   an->the    0.999743
 1300   an->the    0.999743
 1301   an->the    0.999743
 1302   an->the    0.999743
 1303    a->the    0.999742
 1304    a->the    0.999742
 1305   an->the    0.999742
 1306   an->the    0.999742
 1307    a->the    0.999742
 1308   an->the    0.999742
 1309   an->the    0.999742
 1310    a->the    0.999741
 1311     an->a    0.999741
 1312   an->the    0.999741
 1313   an->the    0.999741
 1314    a->the    0.999741
 1315   an->the    0.999741
 1316    a->the    0.999741
 1317    a->the    0.999740
 1318   an->the    0.999740
 1319   an->the    0.999740
 1320    a->the    0.999740
 1321   an->the    0.999740
 1322   an->the    0.999740
 1323   an->the    0.999740
 1324    a->the    0.999740
 1325    a->the    0.999739
 1326    a->the    0.999739
 1327   an->the    0.999739
 1328    a->the    0.999739
 1329    a->the    0.999739
 1330    a->the    0.999739
 1331   an->the    0.999739
 1332    a->the    0.999739
 1333   an->the    0.999739
 1334    a->the    0.999739
 1335   an->the    0.999738
 1336    a->the    0.999737
 1337   an->the    0.999737
 1338   an->the    0.999736
 1339    a->the    0.999736
 1340    a->the    0.999736
 1341   an->the    0.999736
 1342   an->the    0.999736
 1343   an->the    0.999736
 1344   an->the    0.999735
 1345   an->the    0.999735
 1346    a->the    0.999735
 1347    a->the    0.999734
 1348   an->the    0.999734
 1349   an->the    0.999734
 1350   an->the    0.999734
 1351    a->the    0.999734
 1352    a->the    0.999734
 1353   an->the    0.999734
 1354    a->the    0.999734
 1355   an->the    0.999733
 1356    a->the    0.999733
 1357    a->the    0.999733
 1358   an->the    0.999733
 1359   an->the    0.999733
 1360    a->the    0.999733
 1361    a->the    0.999732
 1362   an->the    0.999732
 1363    a->the    0.999732
 1364    a->the    0.999732
 1365   an->the    0.999732
 1366   an->the    0.999732
 1367    a->the    0.999732
 1368    a->the    0.999731
 1369   an->the    0.999731
 1370    a->the    0.999731
 1371   an->the    0.999731
 1372   an->the    0.999731
 1373   an->the    0.999730
 1374   an->the    0.999730
 1375    a->the    0.999730
 1376   an->the    0.999730
 1377    a->the    0.999730
 1378   an->the    0.999729
 1379     an->a    0.999729
 1380    a->the    0.999729
 1381    a->the    0.999729
 1382    a->the    0.999729
 1383   an->the    0.999728
 1384   an->the    0.999728
 1385    a->the    0.999728
 1386    a->the    0.999728
 1387   an->the    0.999727
 1388    a->the    0.999727
 1389   an->the    0.999727
 1390    a->the    0.999727
 1391    a->the    0.999727
 1392   an->the    0.999727
 1393    a->the    0.999726
 1394    a->the    0.999726
 1395   an->the    0.999726
 1396   an->the    0.999726
 1397   an->the    0.999726
 1398   an->the    0.999726
 1399   an->the    0.999725
 1400   an->the    0.999725
 1401    a->the    0.999725
 1402    a->the    0.999725
 1403   an->the    0.999725
 1404   an->the    0.999725
 1405    a->the    0.999725
 1406   an->the    0.999725
 1407    a->the    0.999724
 1408    a->the    0.999724
 1409   an->the    0.999724
 1410   an->the    0.999724
 1411    a->the    0.999723
 1412    a->the    0.999723
 1413   an->the    0.999723
 1414    a->the    0.999723
 1415   an->the    0.999723
 1416    a->the    0.999723
 1417   an->the    0.999723
 1418    a->the    0.999723
 1419   an->the    0.999723
 1420    a->the    0.999722
 1421   an->the    0.999722
 1422   an->the    0.999722
 1423   an->the    0.999722
 1424   an->the    0.999722
 1425   an->the    0.999722
 1426   an->the    0.999722
 1427    a->the    0.999721
 1428    a->the    0.999721
 1429   an->the    0.999721
 1430   an->the    0.999721
 1431   an->the    0.999720
 1432    a->the    0.999720
 1433   an->the    0.999720
 1434   an->the    0.999720
 1435    a->the    0.999719
 1436   an->the    0.999719
 1437   an->the    0.999719
 1438    a->the    0.999718
 1439   an->the    0.999718
 1440    a->the    0.999718
 1441   an->the    0.999717
 1442    a->the    0.999717
 1443   an->the    0.999717
 1444   an->the    0.999717
 1445   an->the    0.999717
 1446   an->the    0.999717
 1447    a->the    0.999717
 1448   an->the    0.999717
 1449    a->the    0.999717
 1450   an->the    0.999717
 1451   an->the    0.999717
 1452    the->a    0.999716
 1453   an->the    0.999716
 1454   an->the    0.999716
 1455   an->the    0.999716
 1456   an->the    0.999715
 1457    a->the    0.999715
 1458   an->the    0.999715
 1459    a->the    0.999715
 1460    a->the    0.999715
 1461    the->a    0.999715
 1462    a->the    0.999714
 1463    a->the    0.999714
 1464   an->the    0.999714
 1465   an->the    0.999714
 1466   an->the    0.999714
 1467   an->the    0.999714
 1468   an->the    0.999713
 1469   an->the    0.999713
 1470   an->the    0.999713
 1471    a->the    0.999713
 1472   an->the    0.999713
 1473   an->the    0.999713
 1474    a->the    0.999713
 1475   an->the    0.999713
 1476    a->the    0.999712
 1477   an->the    0.999712
 1478   an->the    0.999712
 1479    a->the    0.999712
 1480    a->the    0.999712
 1481     an->a    0.999711
 1482   an->the    0.999711
 1483    a->the    0.999711
 1484    a->the    0.999711
 1485    a->the    0.999711
 1486   an->the    0.999711
 1487    a->the    0.999711
 1488   an->the    0.999710
 1489   an->the    0.999710
 1490   an->the    0.999710
 1491    a->the    0.999709
 1492    a->the    0.999709
 1493    a->the    0.999709
 1494    a->the    0.999709
 1495    a->the    0.999709
 1496    a->the    0.999709
 1497    a->the    0.999709
 1498   an->the    0.999708
 1499   an->the    0.999708
 1500   an->the    0.999708
 1501    a->the    0.999708
 1502    a->the    0.999708
 1503   an->the    0.999707
 1504    a->the    0.999707
 1505    a->the    0.999707
 1506     an->a    0.999706
 1507    a->the    0.999706
 1508   an->the    0.999706
 1509   an->the    0.999706
 1510    a->the    0.999706
 1511   an->the    0.999705
 1512   an->the    0.999705
 1513    a->the    0.999705
 1514   an->the    0.999705
 1515    a->the    0.999705
 1516    a->the    0.999705
 1517   an->the    0.999705
 1518    a->the    0.999704
 1519   an->the    0.999704
 1520     an->a    0.999704
 1521    a->the    0.999704
 1522    a->the    0.999704
 1523    a->the    0.999704
 1524    a->the    0.999704
 1525   an->the    0.999704
 1526   an->the    0.999703
 1527    a->the    0.999703
 1528    a->the    0.999703
 1529   an->the    0.999703
 1530    a->the    0.999703
 1531    a->the    0.999703
 1532   an->the    0.999702
 1533   an->the    0.999702
 1534    a->the    0.999702
 1535   an->the    0.999702
 1536    a->the    0.999701
 1537   an->the    0.999701
 1538    a->the    0.999701
 1539    a->the    0.999701
 1540    a->the    0.999701
 1541   an->the    0.999701
 1542   an->the    0.999701
 1543    a->the    0.999701
 1544   an->the    0.999700
 1545   an->the    0.999700
 1546   an->the    0.999700
 1547    a->the    0.999700
 1548   an->the    0.999699
 1549   an->the    0.999699
 1550     an->a    0.999699
 1551    a->the    0.999699
 1552    the->a    0.999699
 1553    a->the    0.999699
 1554    a->the    0.999699
 1555    the->a    0.999699
 1556    a->the    0.999699
 1557   an->the    0.999699
 1558    a->the    0.999698
 1559     an->a    0.999698
 1560   an->the    0.999698
 1561    a->the    0.999698
 1562    a->the    0.999698
 1563   an->the    0.999698
 1564    a->the    0.999697
 1565   an->the    0.999697
 1566   an->the    0.999697
 1567   an->the    0.999697
 1568    a->the    0.999697
 1569   an->the    0.999697
 1570   an->the    0.999696
 1571    a->the    0.999696
 1572    the->a    0.999696
 1573    a->the    0.999696
 1574   an->the    0.999696
 1575    a->the    0.999696
 1576   an->the    0.999696
 1577    a->the    0.999696
 1578   an->the    0.999696
 1579    a->the    0.999695
 1580   an->the    0.999695
 1581   an->the    0.999695
 1582   an->the    0.999694
 1583    a->the    0.999694
 1584    a->the    0.999694
 1585   an->the    0.999694
 1586    a->the    0.999694
 1587    a->the    0.999694
 1588   an->the    0.999694
 1589    a->the    0.999694
 1590   an->the    0.999694
 1591     an->a    0.999694
 1592    a->the    0.999694
 1593    a->the    0.999694
 1594   an->the    0.999694
 1595   an->the    0.999693
 1596    a->the    0.999693
 1597   an->the    0.999693
 1598   an->the    0.999693
 1599    a->the    0.999692
 1600    a->the    0.999692
 1601   an->the    0.999691
 1602   an->the    0.999691
 1603    a->the    0.999691
 1604   an->the    0.999691
 1605   an->the    0.999690
 1606    a->the    0.999690
 1607    a->the    0.999690
 1608   an->the    0.999690
 1609     an->a    0.999690
 1610    the->a    0.999690
 1611   an->the    0.999690
 1612    a->the    0.999690
 1613    a->the    0.999690
 1614     an->a    0.999690
 1615   an->the    0.999690
 1616   an->the    0.999689
 1617    a->the    0.999689
 1618   an->the    0.999689
 1619   an->the    0.999689
 1620   an->the    0.999689
 1621    a->the    0.999689
 1622   an->the    0.999688
 1623   an->the    0.999688
 1624   an->the    0.999688
 1625   an->the    0.999688
 1626    a->the    0.999688
 1627   an->the    0.999688
 1628    a->the    0.999688
 1629   an->the    0.999687
 1630    a->the    0.999687
 1631   an->the    0.999687
 1632   an->the    0.999687
 1633   an->the    0.999687
 1634   an->the    0.999687
 1635   an->the    0.999687
 1636   an->the    0.999687
 1637   an->the    0.999687
 1638   an->the    0.999686
 1639   an->the    0.999686
 1640    a->the    0.999686
 1641   an->the    0.999686
 1642    a->the    0.999686
 1643    a->the    0.999686
 1644   an->the    0.999686
 1645   an->the    0.999685
 1646    a->the    0.999685
 1647   an->the    0.999685
 1648   an->the    0.999685
 1649    a->the    0.999685
 1650    a->the    0.999684
 1651   an->the    0.999684
 1652   an->the    0.999684
 1653   an->the    0.999684
 1654    a->the    0.999684
 1655   an->the    0.999684
 1656    the->a    0.999684
 1657   an->the    0.999684
 1658   an->the    0.999684
 1659    a->the    0.999684
 1660    a->the    0.999684
 1661    a->the    0.999683
 1662   an->the    0.999683
 1663   an->the    0.999683
 1664    a->the    0.999683
 1665    a->the    0.999683
 1666   an->the    0.999683
 1667    a->the    0.999682
 1668   an->the    0.999682
 1669   an->the    0.999682
 1670   an->the    0.999682
 1671    a->the    0.999681
 1672   an->the    0.999681
 1673    a->the    0.999681
 1674   an->the    0.999681
 1675   an->the    0.999681
 1676    a->the    0.999681
 1677   an->the    0.999680
 1678   an->the    0.999680
 1679   an->the    0.999680
 1680   an->the    0.999680
 1681   an->the    0.999680
 1682    a->the    0.999679
 1683    a->the    0.999679
 1684   an->the    0.999679
 1685   an->the    0.999679
 1686   an->the    0.999679
 1687   an->the    0.999679
 1688   an->the    0.999678
 1689    a->the    0.999678
 1690    a->the    0.999678
 1691    a->the    0.999678
 1692    a->the    0.999678
 1693   an->the    0.999678
 1694   an->the    0.999678
 1695    a->the    0.999677
 1696    a->the    0.999677
 1697   an->the    0.999677
 1698   an->the    0.999677
 1699   an->the    0.999677
 1700    a->the    0.999677
 1701   an->the    0.999676
 1702    the->a    0.999676
 1703    a->the    0.999676
 1704    a->the    0.999676
 1705   an->the    0.999676
 1706    a->the    0.999676
 1707   an->the    0.999676
 1708    a->the    0.999676
 1709   an->the    0.999676
 1710   an->the    0.999675
 1711   an->the    0.999675
 1712   an->the    0.999675
 1713   an->the    0.999675
 1714    a->the    0.999674
 1715   an->the    0.999674
 1716    the->a    0.999674
 1717    a->the    0.999673
 1718   an->the    0.999673
 1719   an->the    0.999673
 1720   an->the    0.999673
 1721    a->the    0.999672
 1722    a->the    0.999672
 1723   an->the    0.999672
 1724   an->the    0.999672
 1725     an->a    0.999672
 1726   an->the    0.999671
 1727   an->the    0.999670
 1728   an->the    0.999670
 1729    a->the    0.999669
 1730   an->the    0.999669
 1731   an->the    0.999669
 1732   an->the    0.999669
 1733    a->the    0.999668
 1734   an->the    0.999668
 1735    a->the    0.999668
 1736   an->the    0.999668
 1737    a->the    0.999668
 1738    a->the    0.999668
 1739   an->the    0.999668
 1740   an->the    0.999668
 1741   an->the    0.999667
 1742   an->the    0.999667
 1743    a->the    0.999667
 1744    a->the    0.999666
 1745    a->the    0.999666
 1746   an->the    0.999666
 1747   an->the    0.999666
 1748   an->the    0.999665
 1749   an->the    0.999665
 1750    a->the    0.999665
 1751    a->the    0.999664
 1752   an->the    0.999664
 1753    a->the    0.999664
 1754    a->the    0.999664
 1755    a->the    0.999664
 1756    a->the    0.999664
 1757    a->the    0.999663
 1758   an->the    0.999663
 1759    a->the    0.999663
 1760    a->the    0.999663
 1761    a->the    0.999663
 1762   an->the    0.999663
 1763    a->the    0.999662
 1764   an->the    0.999662
 1765   an->the    0.999662
 1766   an->the    0.999662
 1767   an->the    0.999662
 1768    a->the    0.999661
 1769   an->the    0.999660
 1770    a->the    0.999659
 1771   an->the    0.999659
 1772    a->the    0.999659
 1773   an->the    0.999659
 1774    a->the    0.999658
 1775    a->the    0.999658
 1776     an->a    0.999658
 1777    a->the    0.999658
 1778   an->the    0.999658
 1779   an->the    0.999658
 1780   an->the    0.999658
 1781   an->the    0.999658
 1782   an->the    0.999657
 1783   an->the    0.999657
 1784   an->the    0.999657
 1785    a->the    0.999657
 1786   an->the    0.999657
 1787    a->the    0.999657
 1788    a->the    0.999657
 1789   an->the    0.999657
 1790    a->the    0.999656
 1791    a->the    0.999656
 1792   an->the    0.999656
 1793    a->the    0.999656
 1794   an->the    0.999656
 1795   an->the    0.999655
 1796    a->the    0.999655
 1797     an->a    0.999655
 1798    a->the    0.999655
 1799   an->the    0.999655
 1800   an->the    0.999655
 1801   an->the    0.999655
 1802    a->the    0.999655
 1803   an->the    0.999654
 1804   an->the    0.999654
 1805    a->the    0.999654
 1806    a->the    0.999654
 1807   an->the    0.999654
 1808   an->the    0.999654
 1809   an->the    0.999654
 1810   an->the    0.999654
 1811   an->the    0.999654
 1812    a->the    0.999653
 1813   an->the    0.999653
 1814    a->the    0.999653
 1815   an->the    0.999653
 1816     an->a    0.999653
 1817   an->the    0.999652
 1818    the->a    0.999652
 1819   an->the    0.999652
 1820   an->the    0.999652
 1821    a->the    0.999651
 1822    a->the    0.999651
 1823    a->the    0.999651
 1824    a->the    0.999651
 1825   an->the    0.999650
 1826   an->the    0.999650
 1827   an->the    0.999650
 1828    the->a    0.999650
 1829   an->the    0.999650
 1830   an->the    0.999650
 1831   an->the    0.999650
 1832   an->the    0.999649
 1833   an->the    0.999649
 1834   an->the    0.999649
 1835   an->the    0.999649
 1836    a->the    0.999649
 1837   an->the    0.999649
 1838    a->the    0.999649
 1839   an->the    0.999648
 1840    a->the    0.999648
 1841    a->the    0.999647
 1842    a->the    0.999647
 1843    a->the    0.999647
 1844    a->the    0.999647
 1845    a->the    0.999646
 1846    a->the    0.999646
 1847   an->the    0.999646
 1848   an->the    0.999646
 1849   an->the    0.999646
 1850   an->the    0.999646
 1851    a->the    0.999645
 1852   an->the    0.999645
 1853    a->the    0.999645
 1854    the->a    0.999645
 1855   an->the    0.999645
 1856    a->the    0.999644
 1857   an->the    0.999643
 1858    a->the    0.999642
 1859    the->a    0.999642
 1860   an->the    0.999642
 1861   an->the    0.999642
 1862   an->the    0.999641
 1863    a->the    0.999641
 1864    a->the    0.999641
 1865   an->the    0.999640
 1866   an->the    0.999640
 1867    a->the    0.999640
 1868    a->the    0.999640
 1869   an->the    0.999640
 1870   an->the    0.999640
 1871    a->the    0.999639
 1872    a->the    0.999639
 1873   an->the    0.999638
 1874   an->the    0.999638
 1875    a->the    0.999638
 1876   an->the    0.999638
 1877   an->the    0.999638
 1878    a->the    0.999637
 1879    a->the    0.999637
 1880   an->the    0.999637
 1881    a->the    0.999637
 1882    a->the    0.999637
 1883   an->the    0.999637
 1884    a->the    0.999637
 1885   an->the    0.999637
 1886   an->the    0.999636
 1887    a->the    0.999636
 1888   an->the    0.999636
 1889     an->a    0.999635
 1890   an->the    0.999635
 1891   an->the    0.999635
 1892   an->the    0.999635
 1893   an->the    0.999635
 1894    a->the    0.999634
 1895    a->the    0.999634
 1896    a->the    0.999634
 1897    the->a    0.999634
 1898    a->the    0.999633
 1899    a->the    0.999633
 1900    a->the    0.999633
 1901   an->the    0.999633
 1902    a->the    0.999632
 1903    a->the    0.999632
 1904    a->the    0.999632
 1905    a->the    0.999632
 1906   an->the    0.999632
 1907   an->the    0.999631
 1908     an->a    0.999631
 1909   an->the    0.999631
 1910    a->the    0.999631
 1911     an->a    0.999630
 1912    a->the    0.999630
 1913    a->the    0.999629
 1914    a->the    0.999629
 1915   an->the    0.999629
 1916    a->the    0.999629
 1917   an->the    0.999629
 1918    a->the    0.999629
 1919    a->the    0.999629
 1920   an->the    0.999628
 1921    a->the    0.999628
 1922    a->the    0.999628
 1923    the->a    0.999628
 1924   an->the    0.999628
 1925   an->the    0.999628
 1926    a->the    0.999627
 1927   an->the    0.999627
 1928    a->the    0.999627
 1929    a->the    0.999627
 1930   an->the    0.999627
 1931   an->the    0.999626
 1932   an->the    0.999626
 1933   an->the    0.999626
 1934    a->the    0.999626
 1935    a->the    0.999626
 1936    a->the    0.999626
 1937   an->the    0.999626
 1938   an->the    0.999626
 1939   an->the    0.999626
 1940   an->the    0.999625
 1941   an->the    0.999625
 1942    a->the    0.999625
 1943   an->the    0.999625
 1944   an->the    0.999624
 1945    a->the    0.999624
 1946    a->the    0.999624
 1947   an->the    0.999624
 1948   an->the    0.999624
 1949   an->the    0.999624
 1950   an->the    0.999623
 1951   an->the    0.999623
 1952   an->the    0.999623
 1953   an->the    0.999623
 1954    a->the    0.999623
 1955    a->the    0.999623
 1956    a->the    0.999623
 1957   an->the    0.999623
 1958    a->the    0.999622
 1959    a->the    0.999622
 1960   an->the    0.999622
 1961   an->the    0.999622
 1962   an->the    0.999621
 1963    a->the    0.999621
 1964    a->the    0.999620
 1965    a->the    0.999620
 1966    the->a    0.999620
 1967   an->the    0.999620
 1968   an->the    0.999620
 1969    a->the    0.999620
 1970    a->the    0.999620
 1971    a->the    0.999620
 1972   an->the    0.999619
 1973     an->a    0.999619
 1974    a->the    0.999619
 1975    a->the    0.999619
 1976   an->the    0.999618
 1977    a->the    0.999617
 1978   an->the    0.999617
 1979   an->the    0.999617
 1980    a->the    0.999617
 1981   an->the    0.999617
 1982   an->the    0.999617
 1983   an->the    0.999617
 1984   an->the    0.999617
 1985    a->the    0.999616
 1986   an->the    0.999616
 1987    a->the    0.999616
 1988   an->the    0.999616
 1989    a->the    0.999616
 1990   an->the    0.999615
 1991    a->the    0.999615
 1992    a->the    0.999615
 1993    a->the    0.999615
 1994   an->the    0.999615
 1995   an->the    0.999614
 1996   an->the    0.999614
 1997   an->the    0.999614
 1998    a->the    0.999614
 1999    the->a    0.999613
 2000    a->the    0.999613
 2001   an->the    0.999613
 2002    a->the    0.999612
 2003   an->the    0.999612
 2004   an->the    0.999612
 2005   an->the    0.999612
 2006   an->the    0.999612
 2007    a->the    0.999612
 2008   an->the    0.999612
 2009   an->the    0.999612
 2010    the->a    0.999612
 2011    a->the    0.999612
 2012    a->the    0.999611
 2013   an->the    0.999611
 2014    the->a    0.999611
 2015   an->the    0.999610
 2016    a->the    0.999610
 2017    a->the    0.999610
 2018   an->the    0.999610
 2019    a->the    0.999610
 2020    the->a    0.999610
 2021    a->the    0.999610
 2022   an->the    0.999610
 2023   an->the    0.999609
 2024   an->the    0.999609
 2025   an->the    0.999609
 2026    a->the    0.999608
 2027    a->the    0.999608
 2028   an->the    0.999608
 2029    a->the    0.999608
 2030    a->the    0.999608
 2031    a->the    0.999607
 2032   an->the    0.999607
 2033   an->the    0.999607
 2034     an->a    0.999607
 2035   an->the    0.999607
 2036   an->the    0.999607
 2037   an->the    0.999607
 2038    a->the    0.999606
 2039   an->the    0.999606
 2040   an->the    0.999606
 2041    a->the    0.999606
 2042    a->the    0.999606
 2043   an->the    0.999605
 2044   an->the    0.999605
 2045    a->the    0.999605
 2046    a->the    0.999605
 2047   an->the    0.999605
 2048   an->the    0.999604
 2049   an->the    0.999604
 2050    a->the    0.999604
 2051     an->a    0.999604
 2052    a->the    0.999604
 2053    a->the    0.999604
 2054   an->the    0.999604
 2055   an->the    0.999604
 2056    a->the    0.999604
 2057    a->the    0.999603
 2058    a->the    0.999603
 2059    a->the    0.999603
 2060    a->the    0.999603
 2061   an->the    0.999603
 2062    a->the    0.999601
 2063   an->the    0.999601
 2064   an->the    0.999601
 2065   an->the    0.999601
 2066    a->the    0.999601
 2067    a->the    0.999601
 2068    a->the    0.999601
 2069   an->the    0.999600
 2070   an->the    0.999600
 2071    the->a    0.999600
 2072   an->the    0.999600
 2073    a->the    0.999600
 2074   an->the    0.999600
 2075   an->the    0.999599
 2076    a->the    0.999599
 2077    a->the    0.999598
 2078    a->the    0.999598
 2079    a->the    0.999598
 2080   an->the    0.999598
 2081   an->the    0.999598
 2082    a->the    0.999598
 2083   an->the    0.999598
 2084   an->the    0.999598
 2085    a->the    0.999597
 2086   an->the    0.999597
 2087   an->the    0.999597
 2088    a->the    0.999597
 2089   an->the    0.999596
 2090    a->the    0.999596
 2091    a->the    0.999596
 2092   an->the    0.999595
 2093   an->the    0.999595
 2094   an->the    0.999595
 2095   an->the    0.999595
 2096    a->the    0.999595
 2097   an->the    0.999594
 2098    a->the    0.999594
 2099   an->the    0.999593
 2100     an->a    0.999593
 2101    a->the    0.999593
 2102    a->the    0.999592
 2103   an->the    0.999592
 2104    the->a    0.999592
 2105   an->the    0.999592
 2106    a->the    0.999592
 2107   an->the    0.999592
 2108    a->the    0.999592
 2109   an->the    0.999591
 2110   an->the    0.999591
 2111   an->the    0.999591
 2112   an->the    0.999591
 2113    a->the    0.999591
 2114     an->a    0.999591
 2115    a->the    0.999591
 2116   an->the    0.999591
 2117    a->the    0.999591
 2118   an->the    0.999591
 2119   an->the    0.999591
 2120   an->the    0.999591
 2121    a->the    0.999590
 2122    a->the    0.999590
 2123   an->the    0.999590
 2124   an->the    0.999590
 2125    a->the    0.999590
 2126   an->the    0.999590
 2127   an->the    0.999589
 2128   an->the    0.999589
 2129    a->the    0.999589
 2130    a->the    0.999589
 2131   an->the    0.999589
 2132    the->a    0.999589
 2133   an->the    0.999588
 2134   an->the    0.999588
 2135   an->the    0.999588
 2136    the->a    0.999588
 2137    a->the    0.999588
 2138   an->the    0.999587
 2139    a->the    0.999587
 2140   an->the    0.999587
 2141    a->the    0.999587
 2142    a->the    0.999587
 2143    a->the    0.999587
 2144   an->the    0.999587
 2145    a->the    0.999587
 2146   an->the    0.999587
 2147   an->the    0.999586
 2148   an->the    0.999586
 2149   an->the    0.999586
 2150    a->the    0.999586
 2151   an->the    0.999586
 2152    a->the    0.999585
 2153   an->the    0.999585
 2154   an->the    0.999585
 2155    a->the    0.999585
 2156    a->the    0.999584
 2157    a->the    0.999584
 2158   an->the    0.999584
 2159   an->the    0.999584
 2160   an->the    0.999584
 2161     an->a    0.999584
 2162   an->the    0.999583
 2163    a->the    0.999583
 2164   an->the    0.999583
 2165   an->the    0.999583
 2166   an->the    0.999583
 2167   an->the    0.999583
 2168   an->the    0.999582
 2169    a->the    0.999582
 2170     an->a    0.999582
 2171    a->the    0.999582
 2172    a->the    0.999582
 2173   an->the    0.999581
 2174   an->the    0.999581
 2175   an->the    0.999580
 2176   an->the    0.999580
 2177    the->a    0.999580
 2178    a->the    0.999580
 2179    a->the    0.999580
 2180    a->the    0.999580
 2181   an->the    0.999580
 2182    a->the    0.999579
 2183   an->the    0.999579
 2184   an->the    0.999578
 2185   an->the    0.999578
 2186    a->the    0.999578
 2187    a->the    0.999578
 2188   an->the    0.999577
 2189    a->the    0.999577
 2190   an->the    0.999577
 2191    the->a    0.999577
 2192   an->the    0.999577
 2193   an->the    0.999577
 2194     an->a    0.999576
 2195   an->the    0.999576
 2196   an->the    0.999576
 2197   an->the    0.999575
 2198    a->the    0.999575
 2199   an->the    0.999575
 2200    the->a    0.999575
 2201   an->the    0.999574
 2202    a->the    0.999574
 2203    a->the    0.999574
 2204    a->the    0.999574
 2205   an->the    0.999574
 2206   an->the    0.999574
 2207    a->the    0.999574
 2208     an->a    0.999573
 2209    a->the    0.999573
 2210   an->the    0.999573
 2211    a->the    0.999573
 2212   an->the    0.999572
 2213    a->the    0.999572
 2214   an->the    0.999571
 2215    a->the    0.999571
 2216    a->the    0.999570
 2217   an->the    0.999570
 2218    a->the    0.999570
 2219   an->the    0.999570
 2220   an->the    0.999570
 2221   an->the    0.999569
 2222   an->the    0.999569
 2223   an->the    0.999569
 2224   an->the    0.999569
 2225   an->the    0.999568
 2226    a->the    0.999568
 2227    a->the    0.999568
 2228    a->the    0.999568
 2229   an->the    0.999568
 2230    a->the    0.999568
 2231    a->the    0.999568
 2232    a->the    0.999568
 2233    a->the    0.999567
 2234   an->the    0.999567
 2235   an->the    0.999567
 2236    a->the    0.999566
 2237    a->the    0.999566
 2238   an->the    0.999566
 2239   an->the    0.999566
 2240    a->the    0.999566
 2241   an->the    0.999566
 2242    a->the    0.999565
 2243   an->the    0.999565
 2244   an->the    0.999565
 2245   an->the    0.999564
 2246    a->the    0.999564
 2247   an->the    0.999564
 2248    a->the    0.999564
 2249   an->the    0.999564
 2250   an->the    0.999564
 2251     an->a    0.999564
 2252   an->the    0.999563
 2253    a->the    0.999563
 2254   an->the    0.999563
 2255    a->the    0.999563
 2256   an->the    0.999562
 2257    a->the    0.999561
 2258    the->a    0.999561
 2259   an->the    0.999561
 2260   an->the    0.999561
 2261   an->the    0.999561
 2262     an->a    0.999560
 2263    a->the    0.999560
 2264   an->the    0.999560
 2265   an->the    0.999560
 2266   an->the    0.999560
 2267    a->the    0.999559
 2268    a->the    0.999559
 2269   an->the    0.999559
 2270    a->the    0.999558
 2271    a->the    0.999558
 2272   an->the    0.999558
 2273   an->the    0.999558
 2274    a->the    0.999558
 2275   an->the    0.999557
 2276   an->the    0.999557
 2277    the->a    0.999557
 2278    a->the    0.999557
 2279    a->the    0.999557
 2280    a->the    0.999557
 2281    a->the    0.999557
 2282     an->a    0.999557
 2283    a->the    0.999556
 2284   an->the    0.999556
 2285   an->the    0.999555
 2286     an->a    0.999555
 2287   an->the    0.999555
 2288    a->the    0.999555
 2289    the->a    0.999555
 2290    a->the    0.999554
 2291    a->the    0.999554
 2292    a->the    0.999554
 2293   an->the    0.999554
 2294   an->the    0.999553
 2295    a->the    0.999553
 2296    a->the    0.999553
 2297   an->the    0.999552
 2298   an->the    0.999552
 2299    a->the    0.999551
 2300    a->the    0.999551
 2301   an->the    0.999551
 2302   an->the    0.999550
 2303    a->the    0.999550
 2304     an->a    0.999550
 2305    a->the    0.999550
 2306   an->the    0.999549
 2307   an->the    0.999549
 2308   an->the    0.999549
 2309   an->the    0.999548
 2310    a->the    0.999548
 2311   an->the    0.999548
 2312   an->the    0.999548
 2313   an->the    0.999548
 2314   an->the    0.999548
 2315    a->the    0.999547
 2316    a->the    0.999547
 2317   an->the    0.999547
 2318   an->the    0.999546
 2319    a->the    0.999546
 2320    a->the    0.999546
 2321    a->the    0.999545
 2322    a->the    0.999545
 2323    a->the    0.999545
 2324    a->the    0.999545
 2325    a->the    0.999545
 2326   an->the    0.999544
 2327    a->the    0.999544
 2328    a->the    0.999544
 2329    a->the    0.999544
 2330    a->the    0.999544
 2331    a->the    0.999543
 2332   an->the    0.999543
 2333   an->the    0.999543
 2334   an->the    0.999542
 2335    a->the    0.999542
 2336    a->the    0.999542
 2337    a->the    0.999542
 2338   an->the    0.999542
 2339   an->the    0.999542
 2340   an->the    0.999541
 2341    a->the    0.999541
 2342     an->a    0.999541
 2343    a->the    0.999541
 2344   an->the    0.999541
 2345    a->the    0.999541
 2346    a->the    0.999540
 2347   an->the    0.999540
 2348    a->the    0.999540
 2349    a->the    0.999540
 2350    a->the    0.999540
 2351    a->the    0.999540
 2352   an->the    0.999540
 2353   an->the    0.999539
 2354     an->a    0.999538
 2355   an->the    0.999538
 2356    the->a    0.999538
 2357     an->a    0.999538
 2358   an->the    0.999538
 2359   an->the    0.999538
 2360   an->the    0.999537
 2361    a->the    0.999537
 2362   an->the    0.999537
 2363   an->the    0.999537
 2364   an->the    0.999537
 2365   an->the    0.999537
 2366    a->the    0.999537
 2367    a->the    0.999537
 2368    a->the    0.999536
 2369   an->the    0.999536
 2370     an->a    0.999536
 2371    a->the    0.999536
 2372   an->the    0.999536
 2373    a->the    0.999536
 2374   an->the    0.999536
 2375   an->the    0.999536
 2376    a->the    0.999535
 2377    a->the    0.999535
 2378   an->the    0.999535
 2379   an->the    0.999535
 2380    a->the    0.999534
 2381   an->the    0.999533
 2382    a->the    0.999533
 2383    a->the    0.999532
 2384    a->the    0.999532
 2385     an->a    0.999532
 2386    a->the    0.999532
 2387   an->the    0.999532
 2388    a->the    0.999532
 2389    a->the    0.999532
 2390   an->the    0.999532
 2391   an->the    0.999531
 2392     an->a    0.999531
 2393    the->a    0.999531
 2394    a->the    0.999531
 2395   an->the    0.999531
 2396    a->the    0.999531
 2397    a->the    0.999531
 2398    a->the    0.999530
 2399     an->a    0.999530
 2400   an->the    0.999530
 2401   an->the    0.999530
 2402   an->the    0.999529
 2403    a->the    0.999529
 2404   an->the    0.999528
 2405   an->the    0.999528
 2406   an->the    0.999528
 2407   an->the    0.999528
 2408    a->the    0.999527
 2409    a->the    0.999527
 2410    the->a    0.999527
 2411   an->the    0.999527
 2412   an->the    0.999527
 2413    a->the    0.999527
 2414   an->the    0.999526
 2415    a->the    0.999526
 2416   an->the    0.999526
 2417    a->the    0.999526
 2418   an->the    0.999526
 2419    a->the    0.999525
 2420    a->the    0.999525
 2421   an->the    0.999525
 2422    a->the    0.999525
 2423     an->a    0.999525
 2424   an->the    0.999524
 2425    a->the    0.999524
 2426    the->a    0.999524
 2427   an->the    0.999524
 2428   an->the    0.999524
 2429    a->the    0.999523
 2430    a->the    0.999523
 2431   an->the    0.999523
 2432   an->the    0.999523
 2433   an->the    0.999522
 2434   an->the    0.999522
 2435   an->the    0.999522
 2436   an->the    0.999521
 2437   an->the    0.999521
 2438   an->the    0.999521
 2439    a->the    0.999521
 2440    a->the    0.999521
 2441    a->the    0.999520
 2442    a->the    0.999520
 2443    a->the    0.999520
 2444   an->the    0.999520
 2445    a->the    0.999520
 2446    the->a    0.999520
 2447    a->the    0.999519
 2448   an->the    0.999519
 2449    a->the    0.999519
 2450    a->the    0.999518
 2451   an->the    0.999518
 2452    a->the    0.999517
 2453     an->a    0.999517
 2454    a->the    0.999517
 2455    a->the    0.999516
 2456    a->the    0.999516
 2457   an->the    0.999516
 2458    the->a    0.999516
 2459    a->the    0.999515
 2460    a->the    0.999515
 2461   an->the    0.999515
 2462    a->the    0.999515
 2463    a->the    0.999515
 2464   an->the    0.999514
 2465    a->the    0.999514
 2466    a->the    0.999514
 2467    a->the    0.999514
 2468   an->the    0.999514
 2469    a->the    0.999513
 2470   an->the    0.999513
 2471   an->the    0.999513
 2472    a->the    0.999513
 2473   an->the    0.999513
 2474    a->the    0.999512
 2475   an->the    0.999511
 2476    a->the    0.999511
 2477   an->the    0.999511
 2478    a->the    0.999511
 2479   an->the    0.999510
 2480   an->the    0.999510
 2481   an->the    0.999510
 2482   an->the    0.999510
 2483    a->the    0.999510
 2484   an->the    0.999509
 2485    a->the    0.999509
 2486    a->the    0.999509
 2487   an->the    0.999509
 2488   an->the    0.999509
 2489    a->the    0.999509
 2490    a->the    0.999508
 2491    a->the    0.999508
 2492   an->the    0.999508
 2493   an->the    0.999508
 2494    a->the    0.999508
 2495    the->a    0.999507
 2496   an->the    0.999507
 2497    a->the    0.999507
 2498   an->the    0.999507
 2499    a->the    0.999506
 ...        ...         ...
 17386    an->a    0.883317
 17387  an->the    0.883192
 17388  an->the    0.883159
 17389   a->the    0.883157
 17390  an->the    0.883096
 17391    an->a    0.883088
 17392   the->a    0.883086
 17393   the->a    0.883038
 17394   the->a    0.883026
 17395   the->a    0.882984
 17396  an->the    0.882957
 17397   a->the    0.882942
 17398   a->the    0.882937
 17399   a->the    0.882924
 17400   a->the    0.882906
 17401    a->an    0.882890
 17402  an->the    0.882863
 17403   the->a    0.882834
 17404  an->the    0.882828
 17405    an->a    0.882760
 17406   a->the    0.882666
 17407    an->a    0.882654
 17408   the->a    0.882652
 17409   the->a    0.882623
 17410   a->the    0.882563
 17411    an->a    0.882534
 17412  an->the    0.882522
 17413    an->a    0.882518
 17414   a->the    0.882513
 17415   the->a    0.882452
 17416   the->a    0.882449
 17417  an->the    0.882421
 17418    an->a    0.882388
 17419    an->a    0.882379
 17420   a->the    0.882350
 17421    an->a    0.882330
 17422   a->the    0.882303
 17423   the->a    0.882263
 17424  an->the    0.882240
 17425  an->the    0.882193
 17426   a->the    0.882191
 17427  an->the    0.882165
 17428    an->a    0.882162
 17429   the->a    0.882155
 17430   a->the    0.882153
 17431  an->the    0.882142
 17432    an->a    0.882066
 17433    a->an    0.882041
 17434   the->a    0.882033
 17435   a->the    0.882002
 17436   a->the    0.881945
 17437    an->a    0.881856
 17438   a->the    0.881787
 17439   a->the    0.881724
 17440    an->a    0.881673
 17441    an->a    0.881659
 17442   the->a    0.881636
 17443   a->the    0.881597
 17444    a->an    0.881563
 17445  an->the    0.881549
 17446    a->an    0.881515
 17447  an->the    0.881504
 17448    an->a    0.881447
 17449   the->a    0.881442
 17450  an->the    0.881429
 17451    an->a    0.881375
 17452   a->the    0.881368
 17453   a->the    0.881366
 17454   the->a    0.881346
 17455   a->the    0.881337
 17456  an->the    0.881307
 17457    an->a    0.881297
 17458    an->a    0.881260
 17459  an->the    0.881212
 17460   a->the    0.881211
 17461  an->the    0.881191
 17462  an->the    0.881180
 17463   the->a    0.881166
 17464   a->the    0.881134
 17465   a->the    0.881134
 17466   a->the    0.881134
 17467   a->the    0.881134
 17468   a->the    0.881043
 17469    an->a    0.881025
 17470    an->a    0.881012
 17471   a->the    0.880962
 17472   the->a    0.880917
 17473    an->a    0.880915
 17474   a->the    0.880895
 17475   a->the    0.880887
 17476  an->the    0.880884
 17477   a->the    0.880819
 17478   a->the    0.880810
 17479   a->the    0.880734
 17480    a->an    0.880733
 17481    an->a    0.880717
 17482   a->the    0.880699
 17483   a->the    0.880673
 17484   the->a    0.880672
 17485  an->the    0.880659
 17486  an->the    0.880641
 17487  an->the    0.880630
 17488   a->the    0.880596
 17489   a->the    0.880579
 17490   the->a    0.880563
 17491   the->a    0.880552
 17492  an->the    0.880508
 17493  an->the    0.880503
 17494   a->the    0.880470
 17495   a->the    0.880468
 17496   a->the    0.880466
 17497   the->a    0.880359
 17498   a->the    0.880346
 17499   a->the    0.880326
 17500  an->the    0.880323
 17501   the->a    0.880283
 17502  an->the    0.880271
 17503  an->the    0.880155
 17504  the->an    0.880147
 17505   the->a    0.880124
 17506    an->a    0.880113
 17507   a->the    0.880082
 17508    an->a    0.880080
 17509  an->the    0.880036
 17510    an->a    0.880030
 17511   a->the    0.880016
 17512  an->the    0.880004
 17513  an->the    0.879977
 17514  an->the    0.879931
 17515   the->a    0.879928
 17516    an->a    0.879911
 17517  an->the    0.879878
 17518  an->the    0.879870
 17519  an->the    0.879806
 17520   a->the    0.879797
 17521    a->an    0.879782
 17522    an->a    0.879764
 17523    an->a    0.879753
 17524  an->the    0.879721
 17525   the->a    0.879715
 17526   a->the    0.879707
 17527   a->the    0.879694
 17528   a->the    0.879666
 17529  an->the    0.879654
 17530    an->a    0.879641
 17531  the->an    0.879638
 17532   the->a    0.879579
 17533  an->the    0.879575
 17534    a->an    0.879530
 17535   a->the    0.879499
 17536  an->the    0.879457
 17537  the->an    0.879445
 17538    a->an    0.879439
 17539  an->the    0.879396
 17540   a->the    0.879350
 17541  an->the    0.879343
 17542    an->a    0.879337
 17543   a->the    0.879332
 17544   the->a    0.879322
 17545   a->the    0.879275
 17546  an->the    0.879271
 17547    an->a    0.879214
 17548   a->the    0.879202
 17549  an->the    0.879194
 17550    an->a    0.879175
 17551   a->the    0.879174
 17552  an->the    0.879172
 17553    a->an    0.879145
 17554   a->the    0.879061
 17555   a->the    0.879013
 17556   a->the    0.879009
 17557   a->the    0.878989
 17558  an->the    0.878948
 17559   a->the    0.878821
 17560  an->the    0.878782
 17561  an->the    0.878770
 17562  an->the    0.878747
 17563    an->a    0.878675
 17564   the->a    0.878658
 17565  an->the    0.878651
 17566  an->the    0.878642
 17567   a->the    0.878617
 17568  an->the    0.878600
 17569   the->a    0.878564
 17570   the->a    0.878560
 17571   a->the    0.878545
 17572   a->the    0.878499
 17573  an->the    0.878450
 17574    an->a    0.878443
 17575    an->a    0.878437
 17576   the->a    0.878399
 17577    an->a    0.878350
 17578   a->the    0.878337
 17579   a->the    0.878309
 17580   a->the    0.878270
 17581  the->an    0.878259
 17582   a->the    0.878258
 17583    an->a    0.878120
 17584    a->an    0.878111
 17585   the->a    0.878099
 17586   a->the    0.878095
 17587   a->the    0.878091
 17588   a->the    0.878078
 17589  an->the    0.878076
 17590    an->a    0.878049
 17591   a->the    0.877973
 17592   a->the    0.877735
 17593    an->a    0.877732
 17594   the->a    0.877717
 17595    an->a    0.877712
 17596   the->a    0.877695
 17597    an->a    0.877631
 17598   a->the    0.877607
 17599   the->a    0.877578
 17600   the->a    0.877522
 17601  an->the    0.877467
 17602    an->a    0.877452
 17603    an->a    0.877434
 17604   a->the    0.877422
 17605   a->the    0.877413
 17606   a->the    0.877378
 17607   a->the    0.877322
 17608   a->the    0.877310
 17609   a->the    0.877250
 17610    an->a    0.877235
 17611   the->a    0.877169
 17612   a->the    0.877132
 17613  the->an    0.877080
 17614    an->a    0.877062
 17615  an->the    0.877035
 17616   a->the    0.876980
 17617   a->the    0.876931
 17618   a->the    0.876928
 17619   the->a    0.876914
 17620  an->the    0.876888
 17621    an->a    0.876854
 17622   the->a    0.876827
 17623  an->the    0.876632
 17624   a->the    0.876599
 17625    an->a    0.876585
 17626  an->the    0.876577
 17627   a->the    0.876531
 17628   the->a    0.876512
 17629    an->a    0.876498
 17630    an->a    0.876497
 17631   the->a    0.876496
 17632   the->a    0.876485
 17633  an->the    0.876464
 17634  an->the    0.876463
 17635   a->the    0.876431
 17636  an->the    0.876423
 17637  an->the    0.876369
 17638   the->a    0.876353
 17639   a->the    0.876339
 17640   a->the    0.876200
 17641  an->the    0.876173
 17642  an->the    0.876118
 17643  an->the    0.876118
 17644    an->a    0.876086
 17645    an->a    0.875955
 17646   a->the    0.875916
 17647    an->a    0.875854
 17648   a->the    0.875812
 17649  an->the    0.875776
 17650    an->a    0.875770
 17651  an->the    0.875728
 17652   the->a    0.875623
 17653   a->the    0.875593
 17654   a->the    0.875539
 17655   the->a    0.875496
 17656   the->a    0.875450
 17657   the->a    0.875425
 17658   a->the    0.875424
 17659    an->a    0.875420
 17660    an->a    0.875405
 17661   a->the    0.875404
 17662   a->the    0.875348
 17663   the->a    0.875348
 17664    an->a    0.875298
 17665   a->the    0.875271
 17666   the->a    0.875213
 17667  an->the    0.875148
 17668  an->the    0.875139
 17669  an->the    0.875137
 17670    an->a    0.875118
 17671    an->a    0.875071
 17672  an->the    0.875059
 17673  an->the    0.875048
 17674   the->a    0.875022
 17675  an->the    0.874978
 17676   a->the    0.874926
 17677    an->a    0.874919
 17678   a->the    0.874863
 17679   the->a    0.874851
 17680  an->the    0.874840
 17681    a->an    0.874810
 17682    an->a    0.874794
 17683  an->the    0.874757
 17684    an->a    0.874735
 17685    an->a    0.874733
 17686  an->the    0.874712
 17687   a->the    0.874697
 17688   the->a    0.874692
 17689   a->the    0.874570
 17690   the->a    0.874556
 17691  an->the    0.874532
 17692   the->a    0.874332
 17693    an->a    0.874235
 17694   a->the    0.874213
 17695   the->a    0.874179
 17696   a->the    0.874119
 17697  an->the    0.874080
 17698  an->the    0.874015
 17699    an->a    0.873996
 17700  an->the    0.873950
 17701   a->the    0.873902
 17702   the->a    0.873850
 17703   a->the    0.873822
 17704   the->a    0.873801
 17705   the->a    0.873765
 17706   the->a    0.873716
 17707   a->the    0.873675
 17708  an->the    0.873653
 17709  an->the    0.873637
 17710   a->the    0.873494
 17711   a->the    0.873461
 17712   a->the    0.873441
 17713   the->a    0.873393
 17714   a->the    0.873345
 17715   the->a    0.873208
 17716   a->the    0.873205
 17717  an->the    0.873134
 17718    an->a    0.873126
 17719  an->the    0.873099
 17720   the->a    0.872957
 17721    an->a    0.872956
 17722  an->the    0.872934
 17723   the->a    0.872894
 17724   the->a    0.872880
 17725  an->the    0.872840
 17726  an->the    0.872814
 17727  an->the    0.872801
 17728  an->the    0.872759
 17729   the->a    0.872756
 17730    an->a    0.872739
 17731  an->the    0.872700
 17732   a->the    0.872575
 17733   the->a    0.872574
 17734  an->the    0.872572
 17735  an->the    0.872354
 17736   a->the    0.872299
 17737    an->a    0.872274
 17738   the->a    0.872245
 17739    a->an    0.872220
 17740   a->the    0.872201
 17741    a->an    0.872162
 17742   a->the    0.872161
 17743   the->a    0.872149
 17744   the->a    0.872104
 17745  the->an    0.872104
 17746   a->the    0.872086
 17747   a->the    0.872011
 17748    an->a    0.871989
 17749    an->a    0.871944
 17750    an->a    0.871939
 17751   the->a    0.871923
 17752   the->a    0.871826
 17753   the->a    0.871824
 17754    an->a    0.871790
 17755  an->the    0.871741
 17756  an->the    0.871741
 17757   the->a    0.871698
 17758   a->the    0.871669
 17759    a->an    0.871640
 17760   a->the    0.871548
 17761    an->a    0.871521
 17762  an->the    0.871476
 17763   the->a    0.871458
 17764   a->the    0.871447
 17765   a->the    0.871345
 17766    an->a    0.871230
 17767    an->a    0.871157
 17768   a->the    0.871081
 17769   a->the    0.871053
 17770   the->a    0.871009
 17771   the->a    0.870977
 17772  an->the    0.870963
 17773  an->the    0.870859
 17774    a->an    0.870803
 17775  an->the    0.870729
 17776   a->the    0.870715
 17777  an->the    0.870712
 17778   a->the    0.870648
 17779  an->the    0.870614
 17780  an->the    0.870608
 17781  an->the    0.870593
 17782  an->the    0.870592
 17783  an->the    0.870566
 17784    an->a    0.870566
 17785   the->a    0.870556
 17786  an->the    0.870547
 17787    a->an    0.870493
 17788   a->the    0.870440
 17789   the->a    0.870431
 17790    an->a    0.870402
 17791  an->the    0.870398
 17792    an->a    0.870398
 17793    an->a    0.870389
 17794   the->a    0.870386
 17795    an->a    0.870375
 17796  an->the    0.870350
 17797  an->the    0.870303
 17798   a->the    0.870285
 17799  an->the    0.870283
 17800  an->the    0.870280
 17801  an->the    0.870259
 17802    an->a    0.870209
 17803   the->a    0.870194
 17804  an->the    0.870154
 17805  the->an    0.870136
 17806    an->a    0.870032
 17807    a->an    0.870022
 17808    a->an    0.869932
 17809  an->the    0.869931
 17810    an->a    0.869910
 17811   the->a    0.869896
 17812  an->the    0.869889
 17813    an->a    0.869879
 17814   the->a    0.869850
 17815   the->a    0.869820
 17816  an->the    0.869787
 17817   a->the    0.869725
 17818  an->the    0.869724
 17819   the->a    0.869704
 17820    an->a    0.869702
 17821   the->a    0.869695
 17822   the->a    0.869690
 17823   the->a    0.869688
 17824   a->the    0.869685
 17825   a->the    0.869646
 17826   a->the    0.869632
 17827    an->a    0.869618
 17828    a->an    0.869614
 17829  an->the    0.869492
 17830   the->a    0.869470
 17831   a->the    0.869462
 17832  an->the    0.869442
 17833   a->the    0.869334
 17834  an->the    0.869305
 17835    an->a    0.869304
 17836   a->the    0.869293
 17837   a->the    0.869227
 17838   a->the    0.869198
 17839  an->the    0.869150
 17840   a->the    0.869134
 17841   the->a    0.869121
 17842   a->the    0.869117
 17843    an->a    0.869010
 17844   the->a    0.869000
 17845   a->the    0.868987
 17846  an->the    0.868985
 17847   the->a    0.868980
 17848    an->a    0.868976
 17849   the->a    0.868938
 17850    an->a    0.868860
 17851    an->a    0.868825
 17852   the->a    0.868790
 17853   a->the    0.868776
 17854    an->a    0.868716
 17855  an->the    0.868687
 17856   a->the    0.868686
 17857   a->the    0.868684
 17858  an->the    0.868680
 17859    a->an    0.868629
 17860  an->the    0.868490
 17861  an->the    0.868467
 17862  an->the    0.868298
 17863  an->the    0.868247
 17864  an->the    0.868223
 17865  the->an    0.868219
 17866  the->an    0.868200
 17867   the->a    0.868191
 17868   the->a    0.868182
 17869   the->a    0.868157
 17870  an->the    0.868088
 17871  an->the    0.868064
 17872   the->a    0.868057
 17873   a->the    0.868027
 17874    a->an    0.867995
 17875   a->the    0.867954
 17876   a->the    0.867950
 17877    a->an    0.867940
 17878   a->the    0.867918
 17879   a->the    0.867842
 17880   a->the    0.867815
 17881    an->a    0.867811
 17882  an->the    0.867807
 17883   a->the    0.867796
 17884    an->a    0.867757
 17885    a->an    0.867753
 17886   a->the    0.867712
 17887  an->the    0.867695
 17888  an->the    0.867680
 17889  an->the    0.867655
 17890  an->the    0.867594
 17891   a->the    0.867551
 17892    an->a    0.867479
 17893    an->a    0.867461
 17894  the->an    0.867445
 17895  an->the    0.867433
 17896    an->a    0.867405
 17897   a->the    0.867361
 17898   a->the    0.867353
 17899   the->a    0.867346
 17900   a->the    0.867324
 17901   a->the    0.867270
 17902    an->a    0.867218
 17903  an->the    0.867202
 17904   a->the    0.867118
 17905    an->a    0.867101
 17906   a->the    0.867048
 17907   a->the    0.866992
 17908    a->an    0.866987
 17909  an->the    0.866982
 17910   the->a    0.866771
 17911  an->the    0.866755
 17912   the->a    0.866743
 17913    a->an    0.866731
 17914    an->a    0.866690
 17915   a->the    0.866655
 17916   the->a    0.866639
 17917   the->a    0.866637
 17918  an->the    0.866580
 17919  an->the    0.866548
 17920   a->the    0.866474
 17921  an->the    0.866470
 17922   the->a    0.866433
 17923  an->the    0.866390
 17924    an->a    0.866374
 17925   a->the    0.866351
 17926    an->a    0.866280
 17927  an->the    0.866110
 17928  the->an    0.866076
 17929    an->a    0.866063
 17930   the->a    0.866047
 17931   the->a    0.866039
 17932   a->the    0.866021
 17933    an->a    0.866016
 17934  an->the    0.865997
 17935   the->a    0.865995
 17936    an->a    0.865940
 17937  an->the    0.865923
 17938  an->the    0.865838
 17939   a->the    0.865831
 17940    an->a    0.865824
 17941   a->the    0.865819
 17942   a->the    0.865766
 17943   the->a    0.865737
 17944    an->a    0.865678
 17945    an->a    0.865633
 17946  an->the    0.865631
 17947    an->a    0.865619
 17948  an->the    0.865601
 17949   the->a    0.865567
 17950   a->the    0.865506
 17951   a->the    0.865506
 17952   a->the    0.865506
 17953    an->a    0.865479
 17954  an->the    0.865467
 17955    an->a    0.865435
 17956  an->the    0.865434
 17957  an->the    0.865434
 17958  an->the    0.865393
 17959   a->the    0.865377
 17960  an->the    0.865355
 17961   the->a    0.865272
 17962    a->an    0.865247
 17963   a->the    0.865198
 17964    a->an    0.865167
 17965   the->a    0.865082
 17966   a->the    0.864897
 17967   the->a    0.864841
 17968    a->an    0.864817
 17969   the->a    0.864800
 17970   the->a    0.864788
 17971   the->a    0.864757
 17972    an->a    0.864747
 17973  an->the    0.864708
 17974  an->the    0.864706
 17975  an->the    0.864694
 17976    a->an    0.864683
 17977   the->a    0.864656
 17978    an->a    0.864591
 17979    an->a    0.864532
 17980  an->the    0.864513
 17981   a->the    0.864501
 17982   the->a    0.864490
 17983    an->a    0.864469
 17984  an->the    0.864416
 17985   a->the    0.864409
 17986    a->an    0.864407
 17987    an->a    0.864383
 17988    an->a    0.864355
 17989   a->the    0.864355
 17990   a->the    0.864170
 17991    an->a    0.864092
 17992    an->a    0.864092
 17993  an->the    0.864071
 17994   the->a    0.864023
 17995   the->a    0.864023
 17996  an->the    0.863996
 17997  an->the    0.863881
 17998    a->an    0.863880
 17999  the->an    0.863848
 18000    an->a    0.863832
 18001  an->the    0.863774
 18002   a->the    0.863756
 18003  an->the    0.863734
 18004    an->a    0.863734
 18005   a->the    0.863711
 18006    an->a    0.863692
 18007   the->a    0.863685
 18008   the->a    0.863674
 18009    an->a    0.863659
 18010   a->the    0.863627
 18011    an->a    0.863553
 18012    an->a    0.863522
 18013   a->the    0.863519
 18014   the->a    0.863454
 18015    an->a    0.863441
 18016   the->a    0.863408
 18017   a->the    0.863382
 18018   a->the    0.863379
 18019    an->a    0.863375
 18020  an->the    0.863267
 18021    an->a    0.863244
 18022    an->a    0.863168
 18023  an->the    0.863145
 18024   a->the    0.863143
 18025   a->the    0.863131
 18026  an->the    0.863095
 18027   the->a    0.863053
 18028   a->the    0.863044
 18029  an->the    0.863032
 18030   the->a    0.862988
 18031  an->the    0.862926
 18032   the->a    0.862916
 18033  an->the    0.862839
 18034   a->the    0.862775
 18035   the->a    0.862758
 18036   the->a    0.862715
 18037   the->a    0.862690
 18038   a->the    0.862659
 18039    an->a    0.862645
 18040  an->the    0.862629
 18041   a->the    0.862604
 18042  an->the    0.862603
 18043  an->the    0.862586
 18044   the->a    0.862551
 18045   the->a    0.862464
 18046  an->the    0.862445
 18047   the->a    0.862445
 18048   the->a    0.862330
 18049   a->the    0.862327
 18050  an->the    0.862245
 18051   a->the    0.862192
 18052   a->the    0.862148
 18053   a->the    0.862139
 18054   the->a    0.862123
 18055   the->a    0.862121
 18056    an->a    0.862096
 18057   the->a    0.862095
 18058    an->a    0.862033
 18059    an->a    0.862029
 18060   the->a    0.862017
 18061  an->the    0.862004
 18062   a->the    0.861993
 18063   a->the    0.861968
 18064  an->the    0.861952
 18065    an->a    0.861944
 18066  an->the    0.861926
 18067    an->a    0.861847
 18068  an->the    0.861819
 18069    an->a    0.861767
 18070  an->the    0.861765
 18071   a->the    0.861732
 18072   a->the    0.861718
 18073  an->the    0.861712
 18074    a->an    0.861670
 18075  an->the    0.861666
 18076  an->the    0.861663
 18077  an->the    0.861625
 18078   a->the    0.861592
 18079  an->the    0.861574
 18080   a->the    0.861573
 18081   the->a    0.861461
 18082  an->the    0.861407
 18083    an->a    0.861405
 18084  an->the    0.861338
 18085    an->a    0.861336
 18086   a->the    0.861306
 18087    an->a    0.861269
 18088   a->the    0.861238
 18089   a->the    0.861238
 18090   the->a    0.861237
 18091  an->the    0.861175
 18092  an->the    0.861143
 18093    an->a    0.861117
 18094   a->the    0.861107
 18095   the->a    0.861099
 18096  an->the    0.861060
 18097    an->a    0.860997
 18098  an->the    0.860938
 18099  an->the    0.860904
 18100  an->the    0.860883
 18101    an->a    0.860854
 18102   a->the    0.860827
 18103   the->a    0.860805
 18104   a->the    0.860613
 18105   a->the    0.860587
 18106  an->the    0.860521
 18107   a->the    0.860443
 18108   a->the    0.860413
 18109   the->a    0.860403
 18110  an->the    0.860244
 18111    a->an    0.860242
 18112    an->a    0.860220
 18113   a->the    0.860197
 18114   the->a    0.860128
 18115  an->the    0.860125
 18116   a->the    0.860114
 18117  an->the    0.860109
 18118   a->the    0.860094
 18119   a->the    0.860008
 18120   a->the    0.859996
 18121  an->the    0.859977
 18122   a->the    0.859975
 18123   a->the    0.859975
 18124  an->the    0.859972
 18125    an->a    0.859898
 18126    an->a    0.859896
 18127  an->the    0.859889
 18128  an->the    0.859770
 18129  the->an    0.859769
 18130    a->an    0.859673
 18131  an->the    0.859628
 18132   a->the    0.859570
 18133   the->a    0.859517
 18134    a->an    0.859500
 18135  an->the    0.859491
 18136  an->the    0.859491
 18137   the->a    0.859368
 18138   the->a    0.859357
 18139    an->a    0.859348
 18140  an->the    0.859331
 18141    an->a    0.859322
 18142   a->the    0.859315
 18143    an->a    0.859278
 18144  an->the    0.859224
 18145  an->the    0.859203
 18146   a->the    0.859197
 18147    an->a    0.859192
 18148   a->the    0.859000
 18149   the->a    0.858912
 18150    an->a    0.858859
 18151   a->the    0.858779
 18152  an->the    0.858695
 18153    an->a    0.858681
 18154   a->the    0.858677
 18155   a->the    0.858637
 18156  an->the    0.858620
 18157   a->the    0.858606
 18158   a->the    0.858585
 18159    an->a    0.858577
 18160    an->a    0.858562
 18161   a->the    0.858517
 18162    an->a    0.858506
 18163   a->the    0.858470
 18164  an->the    0.858415
 18165    an->a    0.858398
 18166  an->the    0.858373
 18167   a->the    0.858334
 18168   a->the    0.858311
 18169   a->the    0.858284
 18170  an->the    0.858259
 18171   the->a    0.858241
 18172   the->a    0.858174
 18173    an->a    0.858164
 18174   the->a    0.858101
 18175   a->the    0.858091
 18176    an->a    0.858078
 18177   the->a    0.857967
 18178   the->a    0.857889
 18179    an->a    0.857886
 18180    an->a    0.857882
 18181   a->the    0.857856
 18182   a->the    0.857840
 18183    an->a    0.857820
 18184   a->the    0.857792
 18185   the->a    0.857791
 18186    an->a    0.857751
 18187    a->an    0.857665
 18188   a->the    0.857643
 18189   a->the    0.857585
 18190   a->the    0.857585
 18191  an->the    0.857555
 18192    an->a    0.857514
 18193  an->the    0.857443
 18194  an->the    0.857443
 18195    an->a    0.857371
 18196    a->an    0.857370
 18197    an->a    0.857359
 18198    an->a    0.857339
 18199   a->the    0.857332
 18200   a->the    0.857314
 18201   the->a    0.857296
 18202  an->the    0.857274
 18203   a->the    0.857251
 18204   a->the    0.857228
 18205   a->the    0.857192
 18206    an->a    0.857170
 18207   a->the    0.857167
 18208  an->the    0.857152
 18209    an->a    0.857105
 18210   a->the    0.857056
 18211   a->the    0.857016
 18212  an->the    0.856960
 18213   a->the    0.856881
 18214  an->the    0.856879
 18215  an->the    0.856824
 18216   a->the    0.856773
 18217    an->a    0.856729
 18218   a->the    0.856710
 18219   the->a    0.856676
 18220   a->the    0.856649
 18221   a->the    0.856627
 18222   a->the    0.856583
 18223   a->the    0.856570
 18224  an->the    0.856566
 18225  an->the    0.856554
 18226    an->a    0.856544
 18227  an->the    0.856490
 18228   a->the    0.856365
 18229   the->a    0.856337
 18230  an->the    0.856267
 18231  an->the    0.856267
 18232  an->the    0.856266
 18233   the->a    0.856232
 18234    an->a    0.856221
 18235   a->the    0.856207
 18236   a->the    0.856200
 18237  an->the    0.856172
 18238    an->a    0.856169
 18239   a->the    0.856160
 18240   a->the    0.856156
 18241   a->the    0.856151
 18242   the->a    0.856143
 18243  an->the    0.856081
 18244   the->a    0.856080
 18245   the->a    0.856016
 18246    an->a    0.855922
 18247    an->a    0.855895
 18248  an->the    0.855830
 18249   the->a    0.855780
 18250   a->the    0.855757
 18251  an->the    0.855684
 18252  an->the    0.855677
 18253  the->an    0.855659
 18254    a->an    0.855632
 18255   a->the    0.855560
 18256   a->the    0.855555
 18257  an->the    0.855428
 18258  an->the    0.855423
 18259   a->the    0.855399
 18260   the->a    0.855361
 18261    a->an    0.855348
 18262    an->a    0.855344
 18263  an->the    0.855326
 18264   the->a    0.855287
 18265    an->a    0.855275
 18266  the->an    0.855198
 18267   the->a    0.855153
 18268    an->a    0.855076
 18269    an->a    0.855024
 18270   the->a    0.855011
 18271   a->the    0.854941
 18272   a->the    0.854853
 18273   a->the    0.854824
 18274   a->the    0.854795
 18275  an->the    0.854764
 18276  an->the    0.854759
 18277   the->a    0.854673
 18278    an->a    0.854657
 18279    an->a    0.854645
 18280   a->the    0.854644
 18281   the->a    0.854585
 18282  the->an    0.854571
 18283   a->the    0.854501
 18284  an->the    0.854497
 18285   a->the    0.854461
 18286  an->the    0.854454
 18287   the->a    0.854441
 18288    an->a    0.854414
 18289   the->a    0.854318
 18290   a->the    0.854311
 18291   the->a    0.854259
 18292   a->the    0.854223
 18293  an->the    0.854148
 18294    an->a    0.854134
 18295   the->a    0.854077
 18296   the->a    0.853943
 18297   the->a    0.853827
 18298  an->the    0.853738
 18299    an->a    0.853664
 18300  an->the    0.853646
 18301  an->the    0.853597
 18302  an->the    0.853584
 18303    an->a    0.853543
 18304   a->the    0.853533
 18305   a->the    0.853526
 18306   the->a    0.853519
 18307  the->an    0.853452
 18308    an->a    0.853447
 18309   a->the    0.853406
 18310   the->a    0.853402
 18311    an->a    0.853356
 18312   a->the    0.853332
 18313   the->a    0.853320
 18314  an->the    0.853298
 18315   a->the    0.853270
 18316    an->a    0.853247
 18317  an->the    0.853225
 18318  an->the    0.853075
 18319    an->a    0.853073
 18320   a->the    0.853005
 18321    an->a    0.853004
 18322    an->a    0.852986
 18323  the->an    0.852968
 18324   a->the    0.852899
 18325  an->the    0.852868
 18326    an->a    0.852800
 18327   the->a    0.852775
 18328    an->a    0.852754
 18329   a->the    0.852741
 18330    an->a    0.852715
 18331   a->the    0.852665
 18332   a->the    0.852648
 18333    an->a    0.852508
 18334   the->a    0.852449
 18335   the->a    0.852383
 18336   the->a    0.852336
 18337  an->the    0.852312
 18338    an->a    0.852310
 18339   a->the    0.852291
 18340  an->the    0.852269
 18341   a->the    0.852264
 18342    a->an    0.852242
 18343   a->the    0.852232
 18344  an->the    0.852204
 18345   a->the    0.852173
 18346   the->a    0.852162
 18347  the->an    0.852155
 18348  an->the    0.852127
 18349    an->a    0.852083
 18350  an->the    0.852074
 18351   a->the    0.851909
 18352  an->the    0.851882
 18353   the->a    0.851872
 18354  an->the    0.851862
 18355   the->a    0.851857
 18356   the->a    0.851696
 18357   a->the    0.851599
 18358    an->a    0.851597
 18359   a->the    0.851511
 18360  an->the    0.851357
 18361  an->the    0.851354
 18362    an->a    0.851242
 18363    an->a    0.851229
 18364   the->a    0.851222
 18365   a->the    0.851197
 18366  an->the    0.851164
 18367    an->a    0.851145
 18368  an->the    0.851117
 18369   a->the    0.851099
 18370    an->a    0.851007
 18371   a->the    0.850999
 18372  an->the    0.850986
 18373    a->an    0.850942
 18374  an->the    0.850893
 18375   a->the    0.850869
 18376   a->the    0.850865
 18377   the->a    0.850862
 18378  an->the    0.850807
 18379   the->a    0.850768
 18380    an->a    0.850757
 18381   a->the    0.850588
 18382  an->the    0.850525
 18383  an->the    0.850521
 18384  an->the    0.850501
 18385  an->the    0.850457
 18386    a->an    0.850450
 18387  an->the    0.850359
 18388   a->the    0.850346
 18389  an->the    0.850345
 18390   the->a    0.850256
 18391  an->the    0.850219
 18392  the->an    0.850159
 18393   the->a    0.850153
 18394   the->a    0.850143
 18395   the->a    0.850140
 18396   the->a    0.850135
 18397  an->the    0.850117
 18398  an->the    0.850062
 18399   the->a    0.850033
 18400    an->a    0.850028
 18401    an->a    0.850028
 18402    an->a    0.850028
 18403    a->an    0.849976
 18404   a->the    0.849958
 18405    an->a    0.849825
 18406  an->the    0.849816
 18407   a->the    0.849793
 18408   the->a    0.849772
 18409    a->an    0.849749
 18410   the->a    0.849719
 18411  an->the    0.849715
 18412   a->the    0.849654
 18413   the->a    0.849553
 18414   a->the    0.849551
 18415    an->a    0.849512
 18416    an->a    0.849432
 18417   a->the    0.849274
 18418   the->a    0.849273
 18419  an->the    0.849248
 18420    an->a    0.849177
 18421   a->the    0.849147
 18422  an->the    0.849041
 18423    an->a    0.849036
 18424   the->a    0.848981
 18425   a->the    0.848967
 18426  an->the    0.848940
 18427  an->the    0.848937
 18428   a->the    0.848907
 18429   a->the    0.848892
 18430   the->a    0.848887
 18431   a->the    0.848862
 18432   a->the    0.848847
 18433  an->the    0.848767
 18434  an->the    0.848763
 18435   the->a    0.848656
 18436  an->the    0.848621
 18437   a->the    0.848601
 18438   the->a    0.848578
 18439  an->the    0.848572
 18440    a->an    0.848553
 18441   a->the    0.848500
 18442   a->the    0.848426
 18443    a->an    0.848424
 18444  an->the    0.848353
 18445   the->a    0.848290
 18446   a->the    0.848281
 18447   a->the    0.848185
 18448   a->the    0.848161
 18449    an->a    0.848150
 18450    a->an    0.848128
 18451   a->the    0.848114
 18452   the->a    0.848089
 18453    an->a    0.848062
 18454    an->a    0.848056
 18455    an->a    0.848056
 18456   a->the    0.848015
 18457   a->the    0.847995
 18458   a->the    0.847984
 18459   the->a    0.847975
 18460  an->the    0.847948
 18461  an->the    0.847893
 18462   a->the    0.847849
 18463   a->the    0.847817
 18464  the->an    0.847745
 18465    an->a    0.847714
 18466    an->a    0.847617
 18467   the->a    0.847591
 18468   the->a    0.847545
 18469    an->a    0.847507
 18470   the->a    0.847466
 18471   the->a    0.847446
 18472  an->the    0.847421
 18473   the->a    0.847417
 18474   a->the    0.847400
 18475    an->a    0.847399
 18476   a->the    0.847343
 18477  the->an    0.847327
 18478   a->the    0.847316
 18479    an->a    0.847282
 18480  an->the    0.847258
 18481  an->the    0.847251
 18482    an->a    0.847202
 18483  an->the    0.847198
 18484   the->a    0.847148
 18485    an->a    0.847074
 18486    an->a    0.847043
 18487    an->a    0.847010
 18488  an->the    0.847008
 18489   a->the    0.847000
 18490  an->the    0.846919
 18491  an->the    0.846912
 18492    an->a    0.846890
 18493  an->the    0.846876
 18494   a->the    0.846860
 18495   a->the    0.846847
 18496    a->an    0.846846
 18497  an->the    0.846816
 18498   the->a    0.846761
 18499  an->the    0.846669
 18500   a->the    0.846651
 18501   a->the    0.846565
 18502  an->the    0.846556
 18503   the->a    0.846527
 18504    an->a    0.846497
 18505   the->a    0.846458
 18506   a->the    0.846401
 18507   the->a    0.846386
 18508  an->the    0.846378
 18509  an->the    0.846325
 18510    an->a    0.846316
 18511   a->the    0.846270
 18512   a->the    0.846113
 18513   a->the    0.846074
 18514   the->a    0.845987
 18515   the->a    0.845889
 18516  an->the    0.845864
 18517   the->a    0.845844
 18518    an->a    0.845838
 18519   a->the    0.845737
 18520  an->the    0.845670
 18521   a->the    0.845609
 18522   the->a    0.845599
 18523  an->the    0.845598
 18524    an->a    0.845507
 18525   a->the    0.845484
 18526   a->the    0.845448
 18527   a->the    0.845280
 18528   the->a    0.845215
 18529   a->the    0.845059
 18530   a->the    0.845026
 18531    a->an    0.845010
 18532   the->a    0.844993
 18533   a->the    0.844940
 18534   a->the    0.844937
 18535   a->the    0.844840
 18536  the->an    0.844730
 18537   the->a    0.844701
 18538  an->the    0.844600
 18539   the->a    0.844583
 18540   the->a    0.844582
 18541   a->the    0.844473
 18542   a->the    0.844437
 18543    a->an    0.844412
 18544  an->the    0.844333
 18545   a->the    0.844272
 18546  an->the    0.844269
 18547   the->a    0.844258
 18548   the->a    0.844248
 18549   a->the    0.844207
 18550  an->the    0.844167
 18551   a->the    0.844128
 18552    an->a    0.844119
 18553  an->the    0.844118
 18554  an->the    0.844117
 18555   the->a    0.844084
 18556   the->a    0.844074
 18557   a->the    0.844020
 18558  an->the    0.844009
 18559  an->the    0.843986
 18560  an->the    0.843948
 18561   a->the    0.843916
 18562    an->a    0.843884
 18563    an->a    0.843878
 18564  an->the    0.843840
 18565    an->a    0.843810
 18566   a->the    0.843808
 18567   the->a    0.843780
 18568  an->the    0.843776
 18569  an->the    0.843752
 18570   a->the    0.843724
 18571  an->the    0.843720
 18572  an->the    0.843717
 18573   a->the    0.843696
 18574   a->the    0.843685
 18575   the->a    0.843605
 18576   a->the    0.843489
 18577    a->an    0.843449
 18578  an->the    0.843436
 18579   a->the    0.843402
 18580   the->a    0.843382
 18581   a->the    0.843352
 18582   the->a    0.843327
 18583    a->an    0.843298
 18584   a->the    0.843283
 18585  the->an    0.843172
 18586    an->a    0.843170
 18587  an->the    0.843151
 18588   the->a    0.843062
 18589   a->the    0.843001
 18590    an->a    0.842949
 18591   the->a    0.842923
 18592  an->the    0.842883
 18593  the->an    0.842840
 18594   the->a    0.842802
 18595  an->the    0.842772
 18596    an->a    0.842730
 18597   the->a    0.842714
 18598   a->the    0.842698
 18599    a->an    0.842647
 18600   a->the    0.842420
 18601   the->a    0.842324
 18602   the->a    0.842312
 18603  the->an    0.842293
 18604   a->the    0.842241
 18605    an->a    0.842241
 18606   a->the    0.842203
 18607   a->the    0.842203
 18608   a->the    0.842189
 18609   the->a    0.842167
 18610   a->the    0.842141
 18611   the->a    0.842138
 18612    an->a    0.842134
 18613    a->an    0.842109
 18614    an->a    0.842074
 18615   the->a    0.842029
 18616   the->a    0.841943
 18617  an->the    0.841782
 18618   a->the    0.841576
 18619  an->the    0.841543
 18620   a->the    0.841542
 18621  an->the    0.841539
 18622    an->a    0.841538
 18623  an->the    0.841530
 18624  an->the    0.841514
 18625   a->the    0.841511
 18626  an->the    0.841510
 18627   a->the    0.841499
 18628   the->a    0.841415
 18629   the->a    0.841379
 18630  an->the    0.841376
 18631    an->a    0.841256
 18632   the->a    0.841209
 18633    an->a    0.841163
 18634    an->a    0.841117
 18635  an->the    0.841060
 18636   a->the    0.841018
 18637  an->the    0.841017
 18638   a->the    0.840904
 18639   the->a    0.840886
 18640    an->a    0.840878
 18641   a->the    0.840833
 18642   a->the    0.840828
 18643   a->the    0.840818
 18644  an->the    0.840774
 18645  an->the    0.840744
 18646    an->a    0.840680
 18647   a->the    0.840633
 18648   a->the    0.840596
 18649    an->a    0.840570
 18650    an->a    0.840525
 18651    an->a    0.840524
 18652    an->a    0.840524
 18653   a->the    0.840453
 18654   a->the    0.840445
 18655    an->a    0.840432
 18656   a->the    0.840324
 18657   a->the    0.840299
 18658  an->the    0.840285
 18659   a->the    0.840244
 18660    a->an    0.840211
 18661    an->a    0.840183
 18662  the->an    0.840175
 18663   a->the    0.840127
 18664   a->the    0.840115
 18665   the->a    0.840110
 18666   a->the    0.840093
 18667    an->a    0.840084
 18668   a->the    0.840065
 18669   a->the    0.840041
 18670  an->the    0.840040
 18671   a->the    0.840003
 18672    an->a    0.839961
 18673   a->the    0.839956
 18674   a->the    0.839932
 18675   a->the    0.839926
 18676   a->the    0.839918
 18677   the->a    0.839916
 18678    an->a    0.839886
 18679  an->the    0.839761
 18680   a->the    0.839708
 18681   the->a    0.839697
 18682  an->the    0.839644
 18683   the->a    0.839612
 18684  an->the    0.839610
 18685   a->the    0.839574
 18686    an->a    0.839504
 18687   a->the    0.839486
 18688   the->a    0.839454
 18689    an->a    0.839442
 18690    an->a    0.839440
 18691   the->a    0.839427
 18692   the->a    0.839412
 18693   the->a    0.839409
 18694   a->the    0.839364
 18695   a->the    0.839346
 18696    a->an    0.839222
 18697   the->a    0.839190
 18698  an->the    0.839153
 18699    a->an    0.839136
 18700    a->an    0.839124
 18701    an->a    0.839119
 18702  an->the    0.839080
 18703    an->a    0.839072
 18704   a->the    0.839021
 18705    an->a    0.838882
 18706    an->a    0.838793
 18707    an->a    0.838745
 18708  the->an    0.838668
 18709   a->the    0.838595
 18710   a->the    0.838522
 18711    a->an    0.838504
 18712   the->a    0.838477
 18713  an->the    0.838460
 18714  an->the    0.838449
 18715   a->the    0.838442
 18716   a->the    0.838374
 18717    an->a    0.838332
 18718  an->the    0.838298
 18719   a->the    0.838267
 18720   the->a    0.838223
 18721   the->a    0.838221
 18722   the->a    0.838138
 18723    an->a    0.838120
 18724   a->the    0.838008
 18725   a->the    0.837981
 18726   a->the    0.837951
 18727    an->a    0.837925
 18728    an->a    0.837909
 18729  the->an    0.837875
 18730   a->the    0.837719
 18731   the->a    0.837684
 18732   the->a    0.837680
 18733   a->the    0.837604
 18734  an->the    0.837407
 18735    an->a    0.837379
 18736    a->an    0.837351
 18737    an->a    0.837268
 18738   the->a    0.837266
 18739    an->a    0.837248
 18740    a->an    0.837225
 18741    an->a    0.837138
 18742  the->an    0.837137
 18743   a->the    0.837103
 18744   the->a    0.837032
 18745    an->a    0.837003
 18746   a->the    0.836990
 18747   the->a    0.836986
 18748   the->a    0.836959
 18749  an->the    0.836959
 18750    an->a    0.836946
 18751   a->the    0.836861
 18752   a->the    0.836688
 18753  an->the    0.836681
 18754   a->the    0.836653
 18755   a->the    0.836640
 18756  the->an    0.836545
 18757   a->the    0.836505
 18758  an->the    0.836480
 18759  an->the    0.836370
 18760   a->the    0.836355
 18761   a->the    0.836295
 18762    an->a    0.836241
 18763    an->a    0.836234
 18764   a->the    0.836226
 18765  an->the    0.836025
 18766    an->a    0.835976
 18767   a->the    0.835957
 18768   a->the    0.835925
 18769  an->the    0.835815
 18770    an->a    0.835717
 18771   a->the    0.835646
 18772  an->the    0.835618
 18773  an->the    0.835610
 18774   a->the    0.835584
 18775  an->the    0.835533
 18776   the->a    0.835497
 18777   the->a    0.835454
 18778   the->a    0.835412
 18779    an->a    0.835250
 18780    an->a    0.835212
 18781   the->a    0.835180
 18782   the->a    0.835141
 18783    an->a    0.835023
 18784  an->the    0.834987
 18785   a->the    0.834962
 18786   a->the    0.834946
 18787  an->the    0.834928
 18788  an->the    0.834872
 18789   the->a    0.834825
 18790  the->an    0.834735
 18791   the->a    0.834721
 18792    an->a    0.834718
 18793   the->a    0.834640
 18794  an->the    0.834594
 18795   a->the    0.834550
 18796   the->a    0.834529
 18797   a->the    0.834468
 18798  an->the    0.834453
 18799   a->the    0.834426
 18800   a->the    0.834388
 18801  an->the    0.834374
 18802    an->a    0.834367
 18803   a->the    0.834254
 18804  an->the    0.834221
 18805  an->the    0.834131
 18806   the->a    0.834102
 18807   a->the    0.834087
 18808   the->a    0.834034
 18809   a->the    0.834007
 18810    an->a    0.833943
 18811  an->the    0.833885
 18812   the->a    0.833857
 18813   the->a    0.833830
 18814  an->the    0.833814
 18815  an->the    0.833802
 18816    an->a    0.833737
 18817    an->a    0.833656
 18818   a->the    0.833629
 18819   a->the    0.833603
 18820   the->a    0.833517
 18821   a->the    0.833469
 18822  an->the    0.833453
 18823   the->a    0.833449
 18824   a->the    0.833407
 18825   a->the    0.833376
 18826    an->a    0.833317
 18827   the->a    0.833257
 18828  an->the    0.833100
 18829    an->a    0.832996
 18830   the->a    0.832910
 18831  an->the    0.832874
 18832   a->the    0.832868
 18833   the->a    0.832861
 18834    an->a    0.832858
 18835   the->a    0.832796
 18836   a->the    0.832739
 18837   the->a    0.832610
 18838  an->the    0.832519
 18839    an->a    0.832475
 18840   a->the    0.832351
 18841   a->the    0.832333
 18842  the->an    0.832274
 18843   a->the    0.832256
 18844  an->the    0.832238
 18845    an->a    0.832230
 18846   a->the    0.832199
 18847   a->the    0.832194
 18848  an->the    0.832174
 18849   a->the    0.831998
 18850   the->a    0.831985
 18851   the->a    0.831941
 18852  an->the    0.831916
 18853   a->the    0.831877
 18854  an->the    0.831876
 18855  an->the    0.831861
 18856    a->an    0.831857
 18857   a->the    0.831851
 18858   a->the    0.831613
 18859    a->an    0.831589
 18860   the->a    0.831564
 18861    an->a    0.831408
 18862  an->the    0.831406
 18863   a->the    0.831306
 18864  an->the    0.831263
 18865   a->the    0.831168
 18866  an->the    0.831161
 18867   the->a    0.831033
 18868    an->a    0.830978
 18869   the->a    0.830917
 18870  an->the    0.830916
 18871   the->a    0.830893
 18872   the->a    0.830847
 18873  an->the    0.830761
 18874    an->a    0.830746
 18875   a->the    0.830693
 18876  an->the    0.830604
 18877  an->the    0.830486
 18878   a->the    0.830476
 18879   the->a    0.830472
 18880   a->the    0.830454
 18881   the->a    0.830245
 18882  an->the    0.830225
 18883    an->a    0.830218
 18884   a->the    0.830213
 18885   a->the    0.830200
 18886  an->the    0.830167
 18887   the->a    0.830140
 18888   a->the    0.830115
 18889   a->the    0.830061
 18890  an->the    0.830007
 18891   the->a    0.830000
 18892  an->the    0.829983
 18893  an->the    0.829981
 18894  an->the    0.829879
 18895    a->an    0.829877
 18896   a->the    0.829858
 18897    an->a    0.829832
 18898  an->the    0.829831
 18899  an->the    0.829818
 18900   a->the    0.829815
 18901   the->a    0.829776
 18902   a->the    0.829761
 18903   a->the    0.829744
 18904    an->a    0.829691
 18905   a->the    0.829670
 18906   a->the    0.829617
 18907   a->the    0.829555
 18908   the->a    0.829399
 18909  an->the    0.829357
 18910  an->the    0.829346
 18911   a->the    0.829282
 18912  an->the    0.829281
 18913   the->a    0.829229
 18914  an->the    0.829201
 18915    an->a    0.829081
 18916   a->the    0.829040
 18917  the->an    0.829034
 18918    an->a    0.829001
 18919   a->the    0.828981
 18920  an->the    0.828980
 18921  an->the    0.828973
 18922   the->a    0.828971
 18923    an->a    0.828965
 18924    an->a    0.828842
 18925   the->a    0.828709
 18926   a->the    0.828692
 18927    an->a    0.828666
 18928  an->the    0.828566
 18929   a->the    0.828511
 18930   a->the    0.828457
 18931  an->the    0.828409
 18932  an->the    0.828262
 18933  an->the    0.828249
 18934  an->the    0.828219
 18935  an->the    0.828142
 18936    a->an    0.828063
 18937   a->the    0.827952
 18938  an->the    0.827937
 18939  an->the    0.827795
 18940  an->the    0.827724
 18941   a->the    0.827685
 18942  an->the    0.827673
 18943   a->the    0.827576
 18944  an->the    0.827557
 18945   a->the    0.827453
 18946  an->the    0.827426
 18947    an->a    0.827385
 18948    an->a    0.827369
 18949   a->the    0.827324
 18950   the->a    0.827324
 18951  an->the    0.827178
 18952   a->the    0.827149
 18953   the->a    0.827141
 18954    an->a    0.827102
 18955   the->a    0.827075
 18956  the->an    0.826913
 18957    an->a    0.826902
 18958  an->the    0.826870
 18959  an->the    0.826816
 18960  an->the    0.826802
 18961   the->a    0.826708
 18962  an->the    0.826677
 18963   a->the    0.826642
 18964  an->the    0.826595
 18965  an->the    0.826580
 18966    an->a    0.826569
 18967    a->an    0.826557
 18968  an->the    0.826538
 18969   the->a    0.826414
 18970  an->the    0.826334
 18971   the->a    0.826111
 18972  an->the    0.826096
 18973  an->the    0.826062
 18974   a->the    0.826011
 18975   a->the    0.825979
 18976  an->the    0.825887
 18977   the->a    0.825866
 18978  an->the    0.825862
 18979  an->the    0.825777
 18980    a->an    0.825777
 18981    an->a    0.825748
 18982   the->a    0.825732
 18983  the->an    0.825629
 18984  an->the    0.825623
 18985   a->the    0.825526
 18986   the->a    0.825487
 18987   a->the    0.825446
 18988  an->the    0.825398
 18989   a->the    0.825350
 18990   a->the    0.825325
 18991    an->a    0.825306
 18992  an->the    0.825269
 18993  an->the    0.825217
 18994  an->the    0.825202
 18995   the->a    0.825189
 18996    an->a    0.825181
 18997   a->the    0.825176
 18998   a->the    0.825174
 18999    an->a    0.825119
 19000  an->the    0.825077
 19001    an->a    0.825015
 19002  an->the    0.824915
 19003    an->a    0.824835
 19004  an->the    0.824761
 19005   the->a    0.824707
 19006  an->the    0.824616
 19007    an->a    0.824609
 19008   a->the    0.824552
 19009    an->a    0.824487
 19010    an->a    0.824467
 19011    an->a    0.824401
 19012    an->a    0.824297
 19013   a->the    0.824264
 19014   a->the    0.824260
 19015  an->the    0.824224
 19016    an->a    0.824204
 19017   a->the    0.824088
 19018  an->the    0.824072
 19019    an->a    0.824045
 19020   a->the    0.824010
 19021   a->the    0.823992
 19022  an->the    0.823974
 19023   a->the    0.823935
 19024   a->the    0.823910
 19025  an->the    0.823894
 19026   a->the    0.823871
 19027  an->the    0.823871
 19028    an->a    0.823708
 19029   the->a    0.823669
 19030   the->a    0.823641
 19031    an->a    0.823580
 19032    an->a    0.823549
 19033    an->a    0.823528
 19034    an->a    0.823525
 19035  an->the    0.823500
 19036  an->the    0.823465
 19037   the->a    0.823446
 19038   a->the    0.823383
 19039   a->the    0.823380
 19040   the->a    0.823353
 19041   a->the    0.823290
 19042   a->the    0.823261
 19043    an->a    0.823219
 19044    an->a    0.823209
 19045    an->a    0.823187
 19046  an->the    0.823145
 19047  an->the    0.823112
 19048   the->a    0.822999
 19049   a->the    0.822986
 19050  the->an    0.822953
 19051    an->a    0.822812
 19052   the->a    0.822807
 19053  an->the    0.822716
 19054   a->the    0.822677
 19055  an->the    0.822667
 19056    an->a    0.822666
 19057   the->a    0.822611
 19058   a->the    0.822568
 19059    an->a    0.822377
 19060   a->the    0.822357
 19061   a->the    0.822274
 19062    an->a    0.822263
 19063   the->a    0.822225
 19064  an->the    0.822223
 19065   a->the    0.822191
 19066    an->a    0.822177
 19067   the->a    0.822141
 19068   the->a    0.822135
 19069   a->the    0.822131
 19070    an->a    0.822121
 19071    an->a    0.822024
 19072   the->a    0.822016
 19073   the->a    0.822000
 19074   the->a    0.821879
 19075    an->a    0.821872
 19076  an->the    0.821811
 19077    an->a    0.821686
 19078    an->a    0.821670
 19079   a->the    0.821664
 19080   a->the    0.821403
 19081   a->the    0.821340
 19082  an->the    0.821339
 19083  an->the    0.821329
 19084  an->the    0.821296
 19085  an->the    0.821265
 19086   a->the    0.821169
 19087   a->the    0.821126
 19088   a->the    0.821119
 19089   a->the    0.821078
 19090   the->a    0.821057
 19091  an->the    0.821055
 19092    an->a    0.821046
 19093    an->a    0.821018
 19094   the->a    0.820739
 19095   a->the    0.820689
 19096   a->the    0.820569
 19097  an->the    0.820526
 19098    an->a    0.820497
 19099   the->a    0.820494
 19100   the->a    0.820487
 19101    an->a    0.820465
 19102   a->the    0.820436
 19103   a->the    0.820431
 19104    an->a    0.820404
 19105    an->a    0.820288
 19106   a->the    0.820277
 19107   the->a    0.820202
 19108   the->a    0.820202
 19109  an->the    0.820190
 19110  an->the    0.820039
 19111    an->a    0.820015
 19112   the->a    0.820012
 19113   the->a    0.819928
 19114   the->a    0.819853
 19115  an->the    0.819845
 19116    an->a    0.819772
 19117   the->a    0.819705
 19118  an->the    0.819654
 19119    a->an    0.819613
 19120  an->the    0.819611
 19121   a->the    0.819605
 19122   a->the    0.819531
 19123   a->the    0.819494
 19124  an->the    0.819464
 19125    an->a    0.819375
 19126   the->a    0.819329
 19127  an->the    0.819290
 19128   the->a    0.819261
 19129    an->a    0.819206
 19130   a->the    0.819198
 19131   a->the    0.819182
 19132   a->the    0.819180
 19133    an->a    0.819077
 19134  an->the    0.819076
 19135  an->the    0.819061
 19136   a->the    0.818996
 19137   a->the    0.818947
 19138   the->a    0.818865
 19139   the->a    0.818854
 19140   the->a    0.818782
 19141   a->the    0.818665
 19142    an->a    0.818617
 19143   a->the    0.818494
 19144   the->a    0.818480
 19145   a->the    0.818292
 19146  an->the    0.818267
 19147    an->a    0.818243
 19148    an->a    0.818218
 19149   a->the    0.818193
 19150  an->the    0.818185
 19151   the->a    0.818141
 19152   the->a    0.818135
 19153    an->a    0.818114
 19154   a->the    0.818095
 19155    an->a    0.818082
 19156  an->the    0.818003
 19157   the->a    0.817968
 19158    an->a    0.817951
 19159    an->a    0.817936
 19160   a->the    0.817926
 19161  an->the    0.817833
 19162   a->the    0.817744
 19163  an->the    0.817714
 19164  an->the    0.817664
 19165    an->a    0.817607
 19166   a->the    0.817596
 19167  an->the    0.817479
 19168   the->a    0.817443
 19169    an->a    0.817417
 19170    an->a    0.817371
 19171  an->the    0.817247
 19172    an->a    0.817240
 19173  an->the    0.817096
 19174   a->the    0.817079
 19175   a->the    0.816985
 19176   a->the    0.816984
 19177   the->a    0.816974
 19178    an->a    0.816966
 19179   a->the    0.816795
 19180  an->the    0.816787
 19181  an->the    0.816751
 19182   a->the    0.816718
 19183  an->the    0.816696
 19184  an->the    0.816691
 19185   the->a    0.816600
 19186  the->an    0.816528
 19187   a->the    0.816508
 19188  an->the    0.816477
 19189    an->a    0.816423
 19190    an->a    0.816418
 19191   a->the    0.816417
 19192   a->the    0.816414
 19193    an->a    0.816376
 19194   a->the    0.816329
 19195   the->a    0.816323
 19196    an->a    0.816233
 19197    a->an    0.816186
 19198  an->the    0.816184
 19199   a->the    0.816163
 19200    an->a    0.815987
 19201   a->the    0.815934
 19202  an->the    0.815860
 19203   a->the    0.815842
 19204   a->the    0.815842
 19205  an->the    0.815703
 19206    an->a    0.815689
 19207    an->a    0.815647
 19208    an->a    0.815627
 19209  an->the    0.815572
 19210    an->a    0.815562
 19211    an->a    0.815539
 19212    a->an    0.815515
 19213   a->the    0.815470
 19214   the->a    0.815310
 19215   a->the    0.815183
 19216  an->the    0.815164
 19217    an->a    0.815154
 19218    an->a    0.815080
 19219   a->the    0.815080
 19220   a->the    0.814925
 19221   a->the    0.814810
 19222  an->the    0.814784
 19223   a->the    0.814737
 19224    an->a    0.814734
 19225  an->the    0.814701
 19226  an->the    0.814689
 19227   the->a    0.814613
 19228   the->a    0.814531
 19229    an->a    0.814509
 19230    an->a    0.814494
 19231   a->the    0.814475
 19232   a->the    0.814310
 19233   a->the    0.814195
 19234    an->a    0.814193
 19235    a->an    0.814050
 19236    an->a    0.814011
 19237   the->a    0.813973
 19238    a->an    0.813970
 19239    an->a    0.813949
 19240  an->the    0.813941
 19241   the->a    0.813874
 19242  an->the    0.813861
 19243    an->a    0.813848
 19244   a->the    0.813804
 19245    a->an    0.813715
 19246   a->the    0.813616
 19247    an->a    0.813408
 19248   the->a    0.813376
 19249    an->a    0.813323
 19250   the->a    0.813281
 19251   a->the    0.813185
 19252   a->the    0.813164
 19253   the->a    0.813163
 19254  an->the    0.813119
 19255    an->a    0.813074
 19256  an->the    0.813047
 19257   the->a    0.813029
 19258    an->a    0.812960
 19259  an->the    0.812951
 19260    an->a    0.812947
 19261  an->the    0.812899
 19262  an->the    0.812732
 19263    an->a    0.812649
 19264    an->a    0.812534
 19265  the->an    0.812490
 19266   the->a    0.812345
 19267    an->a    0.812188
 19268    an->a    0.812151
 19269   the->a    0.812104
 19270   the->a    0.812062
 19271   a->the    0.811992
 19272  an->the    0.811975
 19273   a->the    0.811939
 19274  an->the    0.811930
 19275  an->the    0.811849
 19276   the->a    0.811764
 19277   a->the    0.811757
 19278  an->the    0.811751
 19279  an->the    0.811736
 19280  an->the    0.811714
 19281  an->the    0.811698
 19282  an->the    0.811670
 19283    an->a    0.811592
 19284    an->a    0.811586
 19285   the->a    0.811539
 19286   a->the    0.811535
 19287    an->a    0.811488
 19288    an->a    0.811420
 19289    an->a    0.811401
 19290   the->a    0.811396
 19291  an->the    0.811396
 19292  an->the    0.811250
 19293  an->the    0.811201
 19294   the->a    0.811180
 19295   the->a    0.811120
 19296  an->the    0.811078
 19297   a->the    0.811053
 19298   a->the    0.811048
 19299   a->the    0.811027
 19300   the->a    0.811018
 19301   a->the    0.811012
 19302  an->the    0.810994
 19303   a->the    0.810882
 19304   a->the    0.810812
 19305  an->the    0.810809
 19306    an->a    0.810784
 19307    an->a    0.810763
 19308   the->a    0.810612
 19309   a->the    0.810580
 19310  an->the    0.810580
 19311    an->a    0.810530
 19312   a->the    0.810489
 19313   the->a    0.810474
 19314   a->the    0.810462
 19315    an->a    0.810441
 19316   a->the    0.810184
 19317    an->a    0.810153
 19318   a->the    0.810098
 19319    an->a    0.810061
 19320   the->a    0.810037
 19321    a->an    0.810024
 19322   a->the    0.809999
 19323  the->an    0.809989
 19324   the->a    0.809960
 19325   a->the    0.809943
 19326    a->an    0.809929
 19327    a->an    0.809854
 19328  an->the    0.809848
 19329  an->the    0.809841
 19330    an->a    0.809800
 19331   the->a    0.809692
 19332   the->a    0.809670
 19333  an->the    0.809652
 19334    an->a    0.809620
 19335    an->a    0.809597
 19336   the->a    0.809578
 19337   a->the    0.809436
 19338  an->the    0.809390
 19339   the->a    0.809383
 19340    an->a    0.809381
 19341   the->a    0.809372
 19342  an->the    0.809339
 19343    an->a    0.809237
 19344   the->a    0.809230
 19345    an->a    0.809218
 19346   the->a    0.809157
 19347  an->the    0.809153
 19348  the->an    0.809110
 19349   a->the    0.809080
 19350    an->a    0.809037
 19351  an->the    0.808881
 19352  an->the    0.808801
 19353    an->a    0.808769
 19354    an->a    0.808763
 19355   a->the    0.808755
 19356  an->the    0.808683
 19357    a->an    0.808664
 19358  an->the    0.808656
 19359    a->an    0.808644
 19360    an->a    0.808614
 19361  an->the    0.808604
 19362   a->the    0.808569
 19363  an->the    0.808489
 19364   a->the    0.808437
 19365    an->a    0.808389
 19366   a->the    0.808353
 19367   a->the    0.808314
 19368  an->the    0.808313
 19369    an->a    0.808194
 19370    an->a    0.807917
 19371  an->the    0.807835
 19372    an->a    0.807809
 19373    an->a    0.807797
 19374  an->the    0.807762
 19375   the->a    0.807689
 19376  an->the    0.807660
 19377    an->a    0.807602
 19378  an->the    0.807602
 19379    an->a    0.807475
 19380  an->the    0.807467
 19381   a->the    0.807454
 19382   a->the    0.807426
 19383   a->the    0.807407
 19384   the->a    0.807391
 19385   a->the    0.807367
 19386    a->an    0.807304
 19387   the->a    0.807096
 19388  an->the    0.807090
 19389   a->the    0.806884
 19390    an->a    0.806802
 19391   the->a    0.806596
 19392  an->the    0.806585
 19393    an->a    0.806511
 19394   the->a    0.806489
 19395  an->the    0.806447
 19396   a->the    0.806321
 19397   a->the    0.806289
 19398  an->the    0.806272
 19399   a->the    0.806228
 19400  an->the    0.806166
 19401   a->the    0.806141
 19402   a->the    0.805972
 19403    an->a    0.805895
 19404   the->a    0.805833
 19405   a->the    0.805786
 19406  an->the    0.805730
 19407   the->a    0.805691
 19408  an->the    0.805638
 19409   a->the    0.805612
 19410    an->a    0.805609
 19411  an->the    0.805594
 19412  an->the    0.805591
 19413   the->a    0.805562
 19414    an->a    0.805545
 19415   a->the    0.805491
 19416  an->the    0.805482
 19417  an->the    0.805445
 19418   the->a    0.805440
 19419  the->an    0.805383
 19420    an->a    0.805341
 19421  an->the    0.805319
 19422   a->the    0.805306
 19423  an->the    0.805181
 19424   a->the    0.805162
 19425  an->the    0.805102
 19426  an->the    0.805044
 19427  an->the    0.804904
 19428    an->a    0.804869
 19429  an->the    0.804855
 19430   the->a    0.804811
 19431  an->the    0.804763
 19432   a->the    0.804744
 19433   a->the    0.804743
 19434   the->a    0.804652
 19435   a->the    0.804608
 19436   the->a    0.804550
 19437   a->the    0.804549
 19438    a->an    0.804506
 19439  an->the    0.804441
 19440   the->a    0.804389
 19441   a->the    0.804385
 19442   the->a    0.804287
 19443    an->a    0.804276
 19444   a->the    0.804194
 19445   a->the    0.804160
 19446    an->a    0.804120
 19447   the->a    0.804037
 19448   the->a    0.804012
 19449   a->the    0.804001
 19450    an->a    0.803999
 19451   a->the    0.803954
 19452   the->a    0.803951
 19453   a->the    0.803909
 19454  an->the    0.803886
 19455  an->the    0.803863
 19456    an->a    0.803763
 19457    an->a    0.803740
 19458    an->a    0.803506
 19459   the->a    0.803479
 19460   a->the    0.803452
 19461  an->the    0.803417
 19462   the->a    0.803401
 19463   a->the    0.803326
 19464   the->a    0.803255
 19465   a->the    0.803182
 19466  the->an    0.803181
 19467   a->the    0.803180
 19468  an->the    0.803079
 19469   the->a    0.803054
 19470   a->the    0.803051
 19471    an->a    0.803046
 19472  the->an    0.802946
 19473   a->the    0.802900
 19474    an->a    0.802855
 19475   the->a    0.802828
 19476   the->a    0.802804
 19477   a->the    0.802762
 19478  an->the    0.802744
 19479   the->a    0.802692
 19480   a->the    0.802691
 19481  an->the    0.802688
 19482   the->a    0.802662
 19483   a->the    0.802658
 19484    an->a    0.802478
 19485   the->a    0.802410
 19486    an->a    0.802246
 19487   a->the    0.802217
 19488  an->the    0.802017
 19489  an->the    0.802017
 19490    a->an    0.801897
 19491   a->the    0.801868
 19492   a->the    0.801868
 19493  the->an    0.801798
 19494   a->the    0.801702
 19495  an->the    0.801672
 19496  an->the    0.801633
 19497    an->a    0.801629
 19498    an->a    0.801605
 19499    an->a    0.801582
 19500   the->a    0.801542
 19501   a->the    0.801516
 19502   the->a    0.801515
 19503  an->the    0.801509
 19504   a->the    0.801484
 19505   a->the    0.801460
 19506   a->the    0.801445
 19507    an->a    0.801412
 19508   a->the    0.801370
 19509   a->the    0.801364
 19510   the->a    0.801313
 19511   a->the    0.801280
 19512    an->a    0.801237
 19513  the->an    0.801237
 19514  an->the    0.801152
 19515  an->the    0.801128
 19516    an->a    0.801127
 19517   a->the    0.801106
 19518   the->a    0.801062
 19519   a->the    0.801000
 19520    an->a    0.800973
 19521  an->the    0.800928
 19522    an->a    0.800895
 19523   a->the    0.800792
 19524   the->a    0.800772
 19525   the->a    0.800720
 19526   a->the    0.800679
 19527  an->the    0.800641
 19528   a->the    0.800633
 19529    an->a    0.800581
 19530  an->the    0.800539
 19531  an->the    0.800539
 19532    a->an    0.800356
 19533  the->an    0.800351
 19534    an->a    0.800338
 19535    an->a    0.800173
 19536  the->an    0.800041
 19537    an->a    0.799989
 19538  an->the    0.799907
 19539  the->an    0.799868
 19540  the->an    0.799802
 19541   the->a    0.799799
 19542  an->the    0.799722
 19543  an->the    0.799686
 19544  an->the    0.799599
 19545   the->a    0.799513
 19546   a->the    0.799510
 19547    an->a    0.799490
 19548   the->a    0.799434
 19549  an->the    0.799216
 19550  an->the    0.799165
 19551  an->the    0.799108
 19552   a->the    0.799105
 19553  the->an    0.799097
 19554   a->the    0.799080
 19555   a->the    0.798943
 19556  an->the    0.798925
 19557   the->a    0.798894
 19558    an->a    0.798879
 19559  an->the    0.798757
 19560   a->the    0.798752
 19561   a->the    0.798730
 19562  an->the    0.798607
 19563  an->the    0.798553
 19564  an->the    0.798486
 19565    an->a    0.798437
 19566    an->a    0.798386
 19567   a->the    0.798362
 19568    an->a    0.798314
 19569  an->the    0.798127
 19570    an->a    0.797913
 19571   a->the    0.797843
 19572  an->the    0.797659
 19573   a->the    0.797646
 19574   a->the    0.797641
 19575    an->a    0.797567
 19576   the->a    0.797557
 19577    an->a    0.797462
 19578  the->an    0.797456
 19579   the->a    0.797441
 19580    a->an    0.797422
 19581   a->the    0.797394
 19582   a->the    0.797314
 19583    an->a    0.797222
 19584   the->a    0.797210
 19585   a->the    0.797110
 19586   the->a    0.797096
 19587  an->the    0.797091
 19588   a->the    0.797090
 19589   the->a    0.797052
 19590    a->an    0.796926
 19591  the->an    0.796872
 19592    an->a    0.796861
 19593   a->the    0.796795
 19594    an->a    0.796671
 19595   a->the    0.796557
 19596   the->a    0.796535
 19597   a->the    0.796526
 19598    a->an    0.796461
 19599    a->an    0.796459
 19600   the->a    0.796377
 19601    an->a    0.796193
 19602   a->the    0.796155
 19603  an->the    0.796132
 19604   the->a    0.796075
 19605  an->the    0.796034
 19606   the->a    0.795982
 19607   the->a    0.795950
 19608   a->the    0.795783
 19609   a->the    0.795652
 19610   a->the    0.795649
 19611  an->the    0.795649
 19612   a->the    0.795543
 19613   a->the    0.795503
 19614  an->the    0.795501
 19615   the->a    0.795475
 19616    an->a    0.795460
 19617   a->the    0.795428
 19618    a->an    0.795425
 19619   a->the    0.795413
 19620  an->the    0.795391
 19621   a->the    0.795327
 19622    an->a    0.795326
 19623   a->the    0.795301
 19624    a->an    0.795291
 19625   a->the    0.795274
 19626  an->the    0.795265
 19627    a->an    0.795187
 19628   a->the    0.795121
 19629   a->the    0.794950
 19630    an->a    0.794861
 19631  an->the    0.794831
 19632    an->a    0.794752
 19633  an->the    0.794700
 19634   the->a    0.794695
 19635   a->the    0.794689
 19636   the->a    0.794671
 19637   a->the    0.794654
 19638   a->the    0.794472
 19639  an->the    0.794391
 19640    an->a    0.794388
 19641  the->an    0.794362
 19642  an->the    0.794275
 19643  an->the    0.794267
 19644    an->a    0.794146
 19645  an->the    0.794085
 19646    an->a    0.793953
 19647   a->the    0.793924
 19648   a->the    0.793905
 19649  an->the    0.793837
 19650    a->an    0.793809
 19651    an->a    0.793764
 19652    an->a    0.793682
 19653  an->the    0.793672
 19654  an->the    0.793664
 19655   a->the    0.793582
 19656   the->a    0.793580
 19657   the->a    0.793519
 19658  an->the    0.793481
 19659  the->an    0.793476
 19660   the->a    0.793457
 19661    an->a    0.793440
 19662  an->the    0.793429
 19663   a->the    0.793373
 19664   a->the    0.793363
 19665   the->a    0.793329
 19666  an->the    0.793323
 19667  an->the    0.793306
 19668    an->a    0.793221
 19669   a->the    0.793220
 19670   the->a    0.793218
 19671  an->the    0.793215
 19672   a->the    0.793170
 19673   the->a    0.793152
 19674   a->the    0.792976
 19675    an->a    0.792950
 19676   a->the    0.792756
 19677  an->the    0.792590
 19678    an->a    0.792588
 19679   a->the    0.792546
 19680  an->the    0.792326
 19681   the->a    0.792311
 19682   the->a    0.792270
 19683    an->a    0.792179
 19684    a->an    0.792166
 19685   the->a    0.792132
 19686  the->an    0.791941
 19687    an->a    0.791925
 19688  an->the    0.791848
 19689    an->a    0.791835
 19690  an->the    0.791825
 19691  an->the    0.791749
 19692    an->a    0.791706
 19693   a->the    0.791691
 19694   a->the    0.791619
 19695  an->the    0.791546
 19696  an->the    0.791494
 19697  the->an    0.791490
 19698   a->the    0.791409
 19699  an->the    0.791399
 19700    an->a    0.791363
 19701   the->a    0.791292
 19702    an->a    0.791261
 19703   a->the    0.791255
 19704  an->the    0.791216
 19705   a->the    0.791194
 19706  an->the    0.791064
 19707  an->the    0.791052
 19708   a->the    0.790994
 19709   a->the    0.790977
 19710    an->a    0.790974
 19711    an->a    0.790884
 19712  the->an    0.790863
 19713   the->a    0.790816
 19714    an->a    0.790726
 19715   a->the    0.790690
 19716   a->the    0.790681
 19717   a->the    0.790652
 19718  the->an    0.790590
 19719    an->a    0.790562
 19720   a->the    0.790376
 19721   the->a    0.790299
 19722   a->the    0.790287
 19723  an->the    0.790217
 19724    a->an    0.790160
 19725  an->the    0.790160
 19726   the->a    0.790112
 19727    an->a    0.790103
 19728  an->the    0.790057
 19729    a->an    0.790042
 19730    an->a    0.790007
 19731    a->an    0.789969
 19732    an->a    0.789934
 19733  the->an    0.789883
 19734  an->the    0.789865
 19735    a->an    0.789786
 19736   the->a    0.789741
 19737   a->the    0.789613
 19738  an->the    0.789613
 19739    an->a    0.789595
 19740  the->an    0.789568
 19741   a->the    0.789493
 19742   the->a    0.789438
 19743    an->a    0.789421
 19744  an->the    0.789393
 19745   the->a    0.789277
 19746   a->the    0.789233
 19747  an->the    0.789176
 19748   a->the    0.789164
 19749   a->the    0.789052
 19750  an->the    0.789041
 19751    an->a    0.789023
 19752   a->the    0.788948
 19753   the->a    0.788836
 19754    an->a    0.788701
 19755   a->the    0.788687
 19756   a->the    0.788613
 19757    a->an    0.788596
 19758   a->the    0.788528
 19759   the->a    0.788431
 19760   a->the    0.788422
 19761    a->an    0.788404
 19762   a->the    0.788375
 19763   a->the    0.788299
 19764   a->the    0.788252
 19765    an->a    0.788250
 19766    an->a    0.788241
 19767  an->the    0.788204
 19768   a->the    0.788129
 19769   the->a    0.788074
 19770   a->the    0.788048
 19771   a->the    0.787982
 19772    an->a    0.787819
 19773   the->a    0.787789
 19774   a->the    0.787757
 19775    an->a    0.787743
 19776   a->the    0.787732
 19777   a->the    0.787721
 19778  the->an    0.787702
 19779    an->a    0.787670
 19780    an->a    0.787666
 19781   the->a    0.787641
 19782   a->the    0.787517
 19783  an->the    0.787498
 19784  an->the    0.787486
 19785   a->the    0.787480
 19786   a->the    0.787440
 19787   a->the    0.787309
 19788  an->the    0.787215
 19789   a->the    0.787201
 19790  an->the    0.787182
 19791    an->a    0.787065
 19792   the->a    0.786923
 19793   the->a    0.786894
 19794   a->the    0.786890
 19795   the->a    0.786806
 19796  an->the    0.786760
 19797  an->the    0.786707
 19798   a->the    0.786681
 19799  the->an    0.786671
 19800   the->a    0.786592
 19801   a->the    0.786450
 19802    an->a    0.786448
 19803   a->the    0.786443
 19804   a->the    0.786439
 19805   a->the    0.786436
 19806   a->the    0.786430
 19807    an->a    0.786406
 19808   the->a    0.786395
 19809  an->the    0.786390
 19810    an->a    0.786368
 19811    an->a    0.786357
 19812  an->the    0.786325
 19813    an->a    0.786307
 19814   the->a    0.786265
 19815    an->a    0.786255
 19816    an->a    0.786143
 19817   a->the    0.786048
 19818   the->a    0.786001
 19819  an->the    0.785751
 19820    a->an    0.785743
 19821    an->a    0.785738
 19822  an->the    0.785695
 19823  an->the    0.785565
 19824  an->the    0.785471
 19825   the->a    0.785259
 19826   the->a    0.785160
 19827  an->the    0.785112
 19828   a->the    0.785037
 19829    a->an    0.784988
 19830   the->a    0.784823
 19831   the->a    0.784821
 19832  an->the    0.784735
 19833    an->a    0.784617
 19834    an->a    0.784519
 19835    an->a    0.784431
 19836   the->a    0.784371
 19837   a->the    0.784354
 19838    a->an    0.784315
 19839   a->the    0.784301
 19840   a->the    0.784210
 19841   the->a    0.784051
 19842    a->an    0.784020
 19843   a->the    0.783900
 19844   a->the    0.783761
 19845    an->a    0.783665
 19846  an->the    0.783625
 19847   the->a    0.783596
 19848  the->an    0.783580
 19849  an->the    0.783473
 19850   the->a    0.783434
 19851   the->a    0.783371
 19852    an->a    0.783345
 19853    an->a    0.783230
 19854   the->a    0.783229
 19855   the->a    0.783201
 19856  an->the    0.783130
 19857   a->the    0.783127
 19858  an->the    0.783105
 19859   a->the    0.783078
 19860    an->a    0.782996
 19861    an->a    0.782942
 19862   a->the    0.782880
 19863   the->a    0.782864
 19864  an->the    0.782686
 19865    an->a    0.782678
 19866   the->a    0.782525
 19867   a->the    0.782508
 19868  the->an    0.782463
 19869   a->the    0.782345
 19870    an->a    0.782297
 19871  an->the    0.782233
 19872   the->a    0.782146
 19873    a->an    0.782146
 19874  an->the    0.782045
 19875   the->a    0.782023
 19876   the->a    0.781980
 19877  an->the    0.781900
 19878   a->the    0.781883
 19879   a->the    0.781828
 19880    a->an    0.781668
 19881    an->a    0.781661
 19882   the->a    0.781631
 19883    an->a    0.781622
 19884  an->the    0.781598
 19885    an->a    0.781540

 [19886 rows x 2 columns])
from itertools import izip_longest
from sklearn.metrics import log_loss
from preprocessing import *
import pandas as pd
import xgboost as xgb
import matplotlib.pyplot as plt
art_map = {'a':0, 'an':1, 'the':2}
inverse_art_map={0:'a', 1:'an', 2:'the'}
def is_indefinite_article(a):
    return a in {'a', 'an'}
def is_definite_article(a):
    return a == 'the'
def submit_train(df):
    train_arr = load_train_arr()
    corrections = load_resource(fp_corrections_train)
    submit_arr = create_submission_arr(df, train_arr)
    return evaluate(train_arr, corrections, submit_arr)
def evaluate(text, correct, submission):
    print len(text), len(correct), len(submission)
    # with open(text_file) as f:
    #     text = json.load(f)
    # with open(correct_file) as f:
    #     correct = json.load(f)
    # with open(submission_file) as f:
    #     submission = json.load(f)
    data = []
    for sent, cor, sub in izip_longest(text, correct, submission):
        for w, c, s in izip_longest(sent, cor, sub):
            if w in ['a', 'an', 'the']:
                tel = None if s is None else s[2]
                if s is None or s[0] == w:
                    s = ['', float('-inf')]
                # -score, ok-prediction, is_realy_error
                data.append((-s[1], s[0] == c, c is not None, tel))
                # -1, -0.8, -0.2, ... inf, inf
    print 0.02 * len(data)
    data.sort(key=lambda x: x[0])
    fp2 = 0
    fp = 0
    tp = 0
    all_mistakes = sum(x[2] for x in data)  # num of ALL incorrect
    print 'all_mistakes {}'.format(all_mistakes)
    score = 0
    acc = 0
    counter = 0
    stats = {'w': [], 'r': []}
    stop = False
    for _, c, r, tel in data:
        fp2 += not c  # wrong correction
        fp += not r  # realy errors count
        tp += c  # right correction
        if not stop:
            if not c:
                stats['w'].append(tel)
            if c:
                stats['r'].append(tel)
        acc = max(acc, 1 - (0. + fp + all_mistakes - tp) / len(data))
        if fp2 * 1. / len(data) <= 0.02:
            score = tp * 1. / all_mistakes
        else:
            stop = True
    print 'target score = %.2f %%' % (score * 100)
    print 'accuracy (just for info) = %.2f %%' % (acc * 100)
    return stats
def create_submission_arr(df, sents):
    arr = [[None] * (len(x)) for x in sents]
    cols = df.columns
    def do_work(row):
        if row[correction] is not None:
            corr = row[correction]
            conf = row[confidence]
            sent_index = row[sentence_index]
            pos = row[position]
            arr[sent_index][pos] = (corr, conf, OrderedDict([(c,row[c]) for c in cols]))
    df.apply(do_work, axis=1)
    return arr
def arr_to_df(arr):
    cols = arr[0].keys()
    m = OrderedDict((c, [None if x is None else x[c] for x in arr]) for c in cols)
    df= pd.DataFrame(m)
    df['trans'] = df[article]+'->'+df[correction].apply(str)#.apply(lambda s: inverse_art_map[s])
    # add_short_freq_cols(df)
    cols = ['trans', confidence]
    return df[cols]
def to_xgb_df(df):
    df[article]=df[article].apply(lambda s: art_map[s])
    df[correct_article]=df[correct_article].apply(lambda s: art_map[s])
    def normalized_ratio(row, col1, col2, prior):
        a = row[col1]
        b = row[col2]
        if a is None:
            return None
        return (float(prior) + a) / (float(prior) + b)
    def normalized_ratio1(row, col1, col2, prior):
        a = row[col1]
        b = row[col2]
        if a is None:
            return None
        return (float(prior) + a) / (2*float(prior) + b+a)
    def create_normalized_ratio_col(d, col1, col2, new_col, prior):
        d[new_col] = d.apply(lambda row: normalized_ratio(row, col1, col2, prior), axis=1)
    def create_normalized_ratio_col1(d, col1, col2, new_col, prior):
        d[new_col] = d.apply(lambda row: normalized_ratio1(row, col1, col2, prior), axis=1)
    col_pairs=[
        (a_bi_freq_suff, the_bi_freq_suff, 'bi_freq_suff'),
        (a_three_freq_suff, the_three_freq_suff, 'three_freq_suff'),
        (a_four_freq_suff, the_four_freq_suff, 'four_freq_suff'),
        (a_five_freq_suff, the_five_freq_suff, 'five_freq_suff'),
        (a_bi_freq_pref, the_bi_freq_pref, 'bi_freq_pref'),
        (a_three_freq_pref, the_three_freq_pref, 'three_freq_pref'),
        (a_four_freq_pref, the_four_freq_pref, 'four_freq_pref'),
        (a_five_freq_pref, the_five_freq_pref, 'five_freq_pref'),
        (a_sn_gram_freq, the_sn_gram_freq, 'sn_ngram_freq')
    ]
    frequencies_cols = [x[0] for x in col_pairs]+[x[1] for x in col_pairs]
    # frequencies_cols=[]
    for prior in [1, 10, 100]:#
        for a_col, the_col, name in col_pairs:
            new_col = 'a_the_{}_p{}'.format(name, prior)
            create_normalized_ratio_col(df, a_col, the_col, new_col, prior)
            frequencies_cols.append(new_col)
            new_col = 'the_a_{}_p{}'.format(name, prior)
            create_normalized_ratio_col(df, the_col, a_col, new_col, prior)
            frequencies_cols.append(new_col)
    # cols_to_exclude = [
    #     'the_bi_freq_pref', 'the_three_freq_pref',
    #     'the_four_freq_pref' ,'the_five_freq_pref',
    #     'a_bi_freq_pref', 'a_three_freq_pref',
    #     'a_four_freq_pref', 'a_five_freq_pref'
    # ]
    #
    # frequencies_cols = list(set(frequencies_cols).difference(set(cols_to_exclude)))
    cols = frequencies_cols + [st_with_v,article, correct_article]
    # df, new_cols = add_dummy_cols_df(df, raw_prev_token_POS, prev_token_tags)
    # cols+=new_cols
    #
    # df, new_cols = add_dummy_cols_df(df, raw_next_token_POS, next_token_tags)
    # cols+=new_cols
    return df, cols
def create_splits(df, cv, seed=42):
    s_indexes = set(df[sentence_index])
    np.random.seed(seed)
    m = {j: np.random.randint(0, cv) for j in s_indexes}
    df['fold'] = df[sentence_index].apply(lambda s: m[s])
    res = []
    for f in range(cv):
        train = df[df['fold']!=f]
        test = df[df['fold']==f]
        res.append((train, test))
    return res
def add_corrections_cols(df):
    def get_corrections(row):
        a = row['a']
        an = row['an']
        the = row['the']
        s = [(a, 'a'), (an, 'an'), (the, 'the')]
        s.sort(key=lambda s: s[0], reverse=True)
        proposed_correction = s[0][1]
        art_val = row[article]
        if proposed_correction == art_val:
            return None, None
        else:
            return proposed_correction, s[0][0]
    df[tmp] = df.apply(get_corrections, axis=1)
    df[correction] = df[tmp].apply(lambda s: s[0])
    df[confidence] = df[tmp].apply(lambda s: s[1])
def df_to_submit_array(df, sentences):
    res = [[None]*len(x) for x in sentences]
    def collect_corrections_info(row):
        correction_val = row[correction]
        confidence_val = row[confidence]
        sentence_index_val = row[sentence_index]
        position_val  = row[position]
        if correction_val is None:
            return
        res[sentence_index_val][position_val] = (correction_val, confidence_val)
    df.apply(collect_corrections_info, axis=1)
    # res = [(k,v) for k,v in res.iteritems()]
    # res.sort(key=lambda s: s[0])
    # res = [x[1] for x in res]
    return res
def submit_xgb_test():
    train_arr = load_train()
    test_arr = load_test()
    df = test_arr.copy()
    TARGET = correct_article
    df, cols = to_xgb_df(train_arr)
    df, cols = to_xgb_df(test_arr)
    train_arr = train_arr[cols]
    test_arr=test_arr[cols]
    print len(train_arr), len(test_arr)
    train_target = train_arr[TARGET]
    del train_arr[TARGET]
    test_target = test_arr[TARGET]
    del test_arr[TARGET]
    print test_target.head()
    estimator = xgb.XGBClassifier(n_estimators=180,
                                  subsample=0.8,
                                  colsample_bytree=0.8,
                                  max_depth=5,
                                  # learning_rate=learning_rate,
                                  objective='mlogloss',
                                  nthread=-1
                                  )
    print test_arr.columns.values
    estimator.fit(
        train_arr, train_target,
        verbose=True
    )
    proba = estimator.predict_proba(test_arr)
    classes = list(estimator.classes_)
    print classes
    for c in  classes:
        col = inverse_art_map[c]
        test_arr[col] =proba[:,classes.index(c)]
        df.loc[test_arr.index, col] = test_arr.loc[test_arr.index, col]
    add_corrections_cols(df)
    sentences = load_test_arr()
    res = df_to_submit_array(df, sentences)
    json.dump(res, open('test_submition.json', 'w+'))
    return res
def submit_xgb_out_of_fold_pred(df):
    # df = load_train()
    df = create_out_of_fold_xgb_predictions(df)
    add_corrections_cols(df)
    stats = submit_train(df)
    return arr_to_df(stats['w']), arr_to_df(stats['r'])
def create_out_of_fold_xgb_predictions(df):
    # df = load_train()
    TARGET = correct_article
    df_cp = df.copy()
    df, cols = to_xgb_df(df)
    losses = []
    for train_arr, test_arr in create_splits(df, 3):
        train_arr = train_arr[cols]
        test_arr=test_arr[cols]
        print len(train_arr), len(test_arr)
        train_target = train_arr[TARGET]
        del train_arr[TARGET]
        test_target = test_arr[TARGET]
        del test_arr[TARGET]
        # train_arr, test_arr = train_arr[cols], test_arr[cols]
        estimator = xgb.XGBClassifier(n_estimators=180,
                                      subsample=0.8,
                                      colsample_bytree=0.8,
                                      max_depth=5,
                                      # learning_rate=learning_rate,
                                      objective='mlogloss',
                                      nthread=-1
                                      )
        print test_arr.columns.values
        estimator.fit(
            train_arr, train_target,
            verbose=True
        )
        proba = estimator.predict_proba(test_arr)
        classes = list(estimator.classes_)
        print classes
        for c in  classes:
            col = inverse_art_map[c]
            test_arr[col] =proba[:,classes.index(c)]
            df_cp.loc[test_arr.index, col] = test_arr.loc[test_arr.index, col]
        loss = log_loss(test_target, proba)
        losses.append(loss)
        print loss
    return df_cp
def eval_target_score(arts, probs, labels):
    sz=len(arts)
    all_mistakes = sum(arts[j]!=labels[j] for j in range(sz))
    data=[]
    for j in range(sz):
        p = probs[j]#probs
        a=arts[j]#current article
        correct_a = labels[j]#correct article
        p = [(i, p[i]) for i in range(len(p))]
        p.sort(key=lambda s: s[1], reverse=True)
        s = p[0][0]
        conf = p[0][1]
        conf = float('-inf') if a==s else conf
        c = s ==correct_a
        data.append((-conf, c, correct_a!=a))
    data.sort()
    fp2 = 0
    fp = 0
    tp = 0
    score = 0
    acc = 0
    for _, c, r in data:
        fp2 += not c # wrong correction
        fp += not r#realy errors count
        tp += c#right correction
        acc = max(acc, 1 - (0. + fp + all_mistakes - tp) / len(data))
        if fp2 * 1. / len(data) <= 0.02:
            score = tp * 1. / all_mistakes
    print 'target score = %.2f %%' % (score * 100)
    print 'accuracy (just for info) = %.2f %%' % (acc * 100)
    return 'target_score'  , -score
def perform_xgboost_cv(df):
    # df = load_train()
    TARGET = correct_article
    df, cols = to_xgb_df(df)
    losses = []
    for train_arr, test_arr in create_splits(df, 3):
        train_arr = train_arr[cols]
        test_arr=test_arr[cols]
        def get_articles(labels):
            if len(labels)==len(train_arr):
                return list(train_arr[article])
            elif len(labels) == len(test_arr):
                return list(test_arr[article])
            raise
        def my_obj(preds, dtrain):
            labels = dtrain.get_label()
            arts = get_articles(labels)
            return eval_target_score(arts, preds, labels)
        print len(train_arr), len(test_arr)
        train_target = train_arr[TARGET]
        del train_arr[TARGET]
        test_target = test_arr[TARGET]
        del test_arr[TARGET]
        # train_arr, test_arr = train_arr[cols], test_arr[cols]
        estimator = xgb.XGBClassifier(n_estimators=10000,
                                      subsample=0.8,
                                      colsample_bytree=0.8,
                                      max_depth=5,
                                      # learning_rate=learning_rate,
                                      objective='mlogloss',
                                      nthread=-1
                                      )
        print test_arr.columns.values
        eval_set = [(train_arr, train_target), (test_arr, test_target)]
        estimator.fit(
            train_arr, train_target,
            eval_set=eval_set,
            eval_metric=my_obj,#'mlogloss'
            verbose=True,
            early_stopping_rounds=50
        )
        classes = list(estimator.classes_)
        print classes
        xgb.plot_importance(estimator)
        plt.show()
        proba = estimator.predict_proba(test_arr)
        loss = log_loss(test_target, proba)
        losses.append(loss)
        print loss
cols=['a_bi_freq_suff' ,'a_three_freq_suff' ,'a_four_freq_suff', 'a_five_freq_suff',
      'a_bi_freq_pref' ,'a_three_freq_pref' ,'a_four_freq_pref' ,'a_five_freq_pref',
      'the_bi_freq_suff', 'the_three_freq_suff', 'the_four_freq_suff',
      'the_five_freq_suff', 'the_bi_freq_pref' ,'the_three_freq_pref',
      'the_four_freq_pref' ,'the_five_freq_pref' ,'a_the_bi_freq_suff_p1',
      'the_a_bi_freq_suff_p1' ,'a_the_three_freq_suff_p1',
      'the_a_three_freq_suff_p1' ,'a_the_four_freq_suff_p1',
      'the_a_four_freq_suff_p1' ,'a_the_five_freq_suff_p1',
      'the_a_five_freq_suff_p1' ,'a_the_bi_freq_pref_p1' 'the_a_bi_freq_pref_p1',
      'a_the_three_freq_pref_p1', 'the_a_three_freq_pref_p1',
      'a_the_four_freq_pref_p1', 'the_a_four_freq_pref_p1',
      'a_the_five_freq_pref_p1', 'the_a_five_freq_pref_p1',
      'a_the_bi_freq_suff_p10', 'the_a_bi_freq_suff_p10',
      'a_the_three_freq_suff_p10' ,'the_a_three_freq_suff_p10',
      'a_the_four_freq_suff_p10' ,'the_a_four_freq_suff_p10',
      'a_the_five_freq_suff_p10', 'the_a_five_freq_suff_p10',
      'a_the_bi_freq_pref_p10', 'the_a_bi_freq_pref_p10',
      'a_the_three_freq_pref_p10', 'the_a_three_freq_pref_p10',
      'a_the_four_freq_pref_p10' ,'the_a_four_freq_pref_p10',
      'a_the_five_freq_pref_p10' ,'the_a_five_freq_pref_p10',
      'a_the_bi_freq_suff_p100', 'the_a_bi_freq_suff_p100',
      'a_the_three_freq_suff_p100', 'the_a_three_freq_suff_p100',
      'a_the_four_freq_suff_p100' ,'the_a_four_freq_suff_p100',
      'a_the_five_freq_suff_p100' ,'the_a_five_freq_suff_p100',
      'a_the_bi_freq_pref_p100', 'the_a_bi_freq_pref_p100',
      'a_the_three_freq_pref_p100', 'the_a_three_freq_pref_p100',
      'a_the_four_freq_pref_p100' ,'the_a_four_freq_pref_p100',
      'a_the_five_freq_pref_p100' ,'the_a_five_freq_pref_p100' ,'st_with_v',
      'article']
# train_df = exploring_df(load_train())
# train_df = load_train()
#bad, good = process_max_ngram_freq_naive(train_df, 8, 5, 10)   63%
# bad, good = process_max_ngram_freq_naive(train_df, 8, 5, 10)
perform_xgboost_cv(df.copy())
35673 17890
['a_bi_freq_suff' 'a_three_freq_suff' 'a_four_freq_suff' 'a_five_freq_suff'
 'a_bi_freq_pref' 'a_three_freq_pref' 'a_four_freq_pref' 'a_five_freq_pref'
 'a_sn_gram_freq' 'the_bi_freq_suff' 'the_three_freq_suff'
 'the_four_freq_suff' 'the_five_freq_suff' 'the_bi_freq_pref'
 'the_three_freq_pref' 'the_four_freq_pref' 'the_five_freq_pref'
 'the_sn_gram_freq' 'a_the_bi_freq_suff_p1' 'the_a_bi_freq_suff_p1'
 'a_the_three_freq_suff_p1' 'the_a_three_freq_suff_p1'
 'a_the_four_freq_suff_p1' 'the_a_four_freq_suff_p1'
 'a_the_five_freq_suff_p1' 'the_a_five_freq_suff_p1'
 'a_the_bi_freq_pref_p1' 'the_a_bi_freq_pref_p1' 'a_the_three_freq_pref_p1'
 'the_a_three_freq_pref_p1' 'a_the_four_freq_pref_p1'
 'the_a_four_freq_pref_p1' 'a_the_five_freq_pref_p1'
 'the_a_five_freq_pref_p1' 'a_the_sn_ngram_freq_p1'
 'the_a_sn_ngram_freq_p1' 'a_the_bi_freq_suff_p10' 'the_a_bi_freq_suff_p10'
 'a_the_three_freq_suff_p10' 'the_a_three_freq_suff_p10'
 'a_the_four_freq_suff_p10' 'the_a_four_freq_suff_p10'
 'a_the_five_freq_suff_p10' 'the_a_five_freq_suff_p10'
 'a_the_bi_freq_pref_p10' 'the_a_bi_freq_pref_p10'
 'a_the_three_freq_pref_p10' 'the_a_three_freq_pref_p10'
 'a_the_four_freq_pref_p10' 'the_a_four_freq_pref_p10'
 'a_the_five_freq_pref_p10' 'the_a_five_freq_pref_p10'
 'a_the_sn_ngram_freq_p10' 'the_a_sn_ngram_freq_p10'
 'a_the_bi_freq_suff_p100' 'the_a_bi_freq_suff_p100'
 'a_the_three_freq_suff_p100' 'the_a_three_freq_suff_p100'
 'a_the_four_freq_suff_p100' 'the_a_four_freq_suff_p100'
 'a_the_five_freq_suff_p100' 'the_a_five_freq_suff_p100'
 'a_the_bi_freq_pref_p100' 'the_a_bi_freq_pref_p100'
 'a_the_three_freq_pref_p100' 'the_a_three_freq_pref_p100'
 'a_the_four_freq_pref_p100' 'the_a_four_freq_pref_p100'
 'a_the_five_freq_pref_p100' 'the_a_five_freq_pref_p100'
 'a_the_sn_ngram_freq_p100' 'the_a_sn_ngram_freq_p100' 'st_with_v'
 'article']
target score = 67.28 %
accuracy (just for info) = 88.89 %
target score = 65.13 %
accuracy (just for info) = 88.41 %
[0]	validation_0-merror:0.114456	validation_1-merror:0.11839	validation_0-target_score:-0.672811	validation_1-target_score:-0.651322
Multiple eval metrics have been passed: 'validation_1-target_score' will be used for early stopping.
Will train until validation_1-target_score hasn't improved in 50 rounds.
target score = 69.47 %
accuracy (just for info) = 89.03 %
target score = 67.10 %
accuracy (just for info) = 88.73 %
[1]	validation_0-merror:0.109691	validation_1-merror:0.112745	validation_0-target_score:-0.694717	validation_1-target_score:-0.671019
target score = 69.20 %
accuracy (just for info) = 89.17 %
target score = 67.40 %
accuracy (just for info) = 88.69 %
[2]	validation_0-merror:0.108317	validation_1-merror:0.113192	validation_0-target_score:-0.692028	validation_1-target_score:-0.674007
target score = 69.61 %
accuracy (just for info) = 89.30 %
target score = 67.79 %
accuracy (just for info) = 88.88 %
[3]	validation_0-merror:0.107028	validation_1-merror:0.111291	validation_0-target_score:-0.696061	validation_1-target_score:-0.67788
target score = 69.88 %
accuracy (just for info) = 89.50 %
target score = 68.25 %
accuracy (just for info) = 89.16 %
[4]	validation_0-merror:0.105037	validation_1-merror:0.10844	validation_0-target_score:-0.698751	validation_1-target_score:-0.682527
target score = 69.85 %
accuracy (just for info) = 89.45 %
target score = 68.53 %
accuracy (just for info) = 89.08 %
[5]	validation_0-merror:0.105514	validation_1-merror:0.109167	validation_0-target_score:-0.698527	validation_1-target_score:-0.685294
target score = 69.76 %
accuracy (just for info) = 89.43 %
target score = 68.34 %
accuracy (just for info) = 89.04 %
[6]	validation_0-merror:0.10571	validation_1-merror:0.109614	validation_0-target_score:-0.69763	validation_1-target_score:-0.683413
target score = 70.37 %
accuracy (just for info) = 89.49 %
target score = 68.26 %
accuracy (just for info) = 89.02 %
[7]	validation_0-merror:0.105234	validation_1-merror:0.109838	validation_0-target_score:-0.703681	validation_1-target_score:-0.682638
target score = 70.06 %
accuracy (just for info) = 89.58 %
target score = 68.54 %
accuracy (just for info) = 89.11 %
[8]	validation_0-merror:0.104168	validation_1-merror:0.108888	validation_0-target_score:-0.700599	validation_1-target_score:-0.685404
target score = 70.28 %
accuracy (just for info) = 89.74 %
target score = 69.04 %
accuracy (just for info) = 89.31 %
[9]	validation_0-merror:0.102599	validation_1-merror:0.106875	validation_0-target_score:-0.70284	validation_1-target_score:-0.690384
target score = 70.72 %
accuracy (just for info) = 89.87 %
target score = 68.94 %
accuracy (just for info) = 89.30 %
[10]	validation_0-merror:0.101393	validation_1-merror:0.106987	validation_0-target_score:-0.707154	validation_1-target_score:-0.689388
target score = 71.00 %
accuracy (just for info) = 89.89 %
target score = 69.44 %
accuracy (just for info) = 89.35 %
[11]	validation_0-merror:0.101141	validation_1-merror:0.10654	validation_0-target_score:-0.709956	validation_1-target_score:-0.694368
target score = 71.09 %
accuracy (just for info) = 89.94 %
target score = 69.35 %
accuracy (just for info) = 89.37 %
[12]	validation_0-merror:0.10072	validation_1-merror:0.10626	validation_0-target_score:-0.710908	validation_1-target_score:-0.693482
target score = 71.67 %
accuracy (just for info) = 90.05 %
target score = 69.56 %
accuracy (just for info) = 89.42 %
[13]	validation_0-merror:0.099543	validation_1-merror:0.105869	validation_0-target_score:-0.716735	validation_1-target_score:-0.695585
target score = 72.16 %
accuracy (just for info) = 89.96 %
target score = 69.82 %
accuracy (just for info) = 89.47 %
[14]	validation_0-merror:0.100412	validation_1-merror:0.10531	validation_0-target_score:-0.721553	validation_1-target_score:-0.698241
target score = 72.28 %
accuracy (just for info) = 90.00 %
target score = 69.95 %
accuracy (just for info) = 89.55 %
[15]	validation_0-merror:0.100076	validation_1-merror:0.104472	validation_0-target_score:-0.722786	validation_1-target_score:-0.699458
target score = 72.46 %
accuracy (just for info) = 90.09 %
target score = 70.21 %
accuracy (just for info) = 89.58 %
[16]	validation_0-merror:0.099123	validation_1-merror:0.104192	validation_0-target_score:-0.724634	validation_1-target_score:-0.702114
target score = 72.63 %
accuracy (just for info) = 90.17 %
target score = 70.09 %
accuracy (just for info) = 89.61 %
[17]	validation_0-merror:0.098338	validation_1-merror:0.103857	validation_0-target_score:-0.726259	validation_1-target_score:-0.700896
target score = 72.84 %
accuracy (just for info) = 90.16 %
target score = 70.28 %
accuracy (just for info) = 89.62 %
[18]	validation_0-merror:0.098422	validation_1-merror:0.103801	validation_0-target_score:-0.728444	validation_1-target_score:-0.702777
target score = 72.94 %
accuracy (just for info) = 90.14 %
target score = 70.34 %
accuracy (just for info) = 89.61 %
[19]	validation_0-merror:0.098618	validation_1-merror:0.103913	validation_0-target_score:-0.729397	validation_1-target_score:-0.703441
target score = 73.23 %
accuracy (just for info) = 90.21 %
target score = 70.22 %
accuracy (just for info) = 89.66 %
[20]	validation_0-merror:0.097945	validation_1-merror:0.103354	validation_0-target_score:-0.73231	validation_1-target_score:-0.702224
target score = 73.28 %
accuracy (just for info) = 90.19 %
target score = 70.40 %
accuracy (just for info) = 89.61 %
[21]	validation_0-merror:0.098113	validation_1-merror:0.103913	validation_0-target_score:-0.732814	validation_1-target_score:-0.703995
target score = 73.52 %
accuracy (just for info) = 90.21 %
target score = 70.59 %
accuracy (just for info) = 89.68 %
[22]	validation_0-merror:0.097917	validation_1-merror:0.103242	validation_0-target_score:-0.735167	validation_1-target_score:-0.705876
target score = 73.61 %
accuracy (just for info) = 90.22 %
target score = 70.84 %
accuracy (just for info) = 89.63 %
[23]	validation_0-merror:0.097889	validation_1-merror:0.103689	validation_0-target_score:-0.736064	validation_1-target_score:-0.708421
target score = 73.65 %
accuracy (just for info) = 90.23 %
target score = 70.85 %
accuracy (just for info) = 89.68 %
[24]	validation_0-merror:0.097749	validation_1-merror:0.103186	validation_0-target_score:-0.736456	validation_1-target_score:-0.708532
target score = 73.76 %
accuracy (just for info) = 90.24 %
target score = 70.97 %
accuracy (just for info) = 89.69 %
[25]	validation_0-merror:0.097637	validation_1-merror:0.10313	validation_0-target_score:-0.737576	validation_1-target_score:-0.709749
target score = 73.96 %
accuracy (just for info) = 90.28 %
target score = 71.13 %
accuracy (just for info) = 89.80 %
[26]	validation_0-merror:0.0973	validation_1-merror:0.10218	validation_0-target_score:-0.739593	validation_1-target_score:-0.711298
target score = 74.03 %
accuracy (just for info) = 90.30 %
target score = 71.22 %
accuracy (just for info) = 89.79 %
[27]	validation_0-merror:0.097104	validation_1-merror:0.102571	validation_0-target_score:-0.740322	validation_1-target_score:-0.712183
target score = 74.00 %
accuracy (just for info) = 90.31 %
target score = 71.31 %
accuracy (just for info) = 89.82 %
[28]	validation_0-merror:0.096964	validation_1-merror:0.102124	validation_0-target_score:-0.739985	validation_1-target_score:-0.713068
target score = 74.26 %
accuracy (just for info) = 90.30 %
target score = 71.35 %
accuracy (just for info) = 89.83 %
[29]	validation_0-merror:0.09702	validation_1-merror:0.101677	validation_0-target_score:-0.742619	validation_1-target_score:-0.713511
target score = 74.38 %
accuracy (just for info) = 90.33 %
target score = 71.59 %
accuracy (just for info) = 89.84 %
[30]	validation_0-merror:0.096684	validation_1-merror:0.101621	validation_0-target_score:-0.743795	validation_1-target_score:-0.715946
target score = 74.60 %
accuracy (just for info) = 90.39 %
target score = 71.69 %
accuracy (just for info) = 89.84 %
[31]	validation_0-merror:0.096095	validation_1-merror:0.101621	validation_0-target_score:-0.746036	validation_1-target_score:-0.716941
target score = 74.63 %
accuracy (just for info) = 90.40 %
target score = 71.67 %
accuracy (just for info) = 89.89 %
[32]	validation_0-merror:0.095955	validation_1-merror:0.101118	validation_0-target_score:-0.746316	validation_1-target_score:-0.71672
target score = 74.78 %
accuracy (just for info) = 90.44 %
target score = 71.66 %
accuracy (just for info) = 89.89 %
[33]	validation_0-merror:0.095591	validation_1-merror:0.101062	validation_0-target_score:-0.747773	validation_1-target_score:-0.716609
target score = 74.73 %
accuracy (just for info) = 90.46 %
target score = 71.78 %
accuracy (just for info) = 89.89 %
[34]	validation_0-merror:0.095394	validation_1-merror:0.101174	validation_0-target_score:-0.747269	validation_1-target_score:-0.717827
target score = 74.92 %
accuracy (just for info) = 90.46 %
target score = 71.71 %
accuracy (just for info) = 89.96 %
[35]	validation_0-merror:0.09545	validation_1-merror:0.100391	validation_0-target_score:-0.749174	validation_1-target_score:-0.717052
target score = 75.10 %
accuracy (just for info) = 90.48 %
target score = 71.85 %
accuracy (just for info) = 89.93 %
[36]	validation_0-merror:0.095198	validation_1-merror:0.100783	validation_0-target_score:-0.751022	validation_1-target_score:-0.718491
target score = 75.23 %
accuracy (just for info) = 90.52 %
target score = 71.96 %
accuracy (just for info) = 89.96 %
[37]	validation_0-merror:0.094806	validation_1-merror:0.100447	validation_0-target_score:-0.752311	validation_1-target_score:-0.719597
target score = 75.27 %
accuracy (just for info) = 90.54 %
target score = 72.02 %
accuracy (just for info) = 89.96 %
[38]	validation_0-merror:0.094581	validation_1-merror:0.100447	validation_0-target_score:-0.752703	validation_1-target_score:-0.72015
target score = 75.42 %
accuracy (just for info) = 90.53 %
target score = 72.04 %
accuracy (just for info) = 89.94 %
[39]	validation_0-merror:0.094721	validation_1-merror:0.100615	validation_0-target_score:-0.754216	validation_1-target_score:-0.720372
target score = 75.39 %
accuracy (just for info) = 90.54 %
target score = 72.13 %
accuracy (just for info) = 89.94 %
[40]	validation_0-merror:0.094665	validation_1-merror:0.100559	validation_0-target_score:-0.753936	validation_1-target_score:-0.721257
target score = 75.34 %
accuracy (just for info) = 90.55 %
target score = 72.19 %
accuracy (just for info) = 89.97 %
[41]	validation_0-merror:0.094609	validation_1-merror:0.100335	validation_0-target_score:-0.753432	validation_1-target_score:-0.721921
target score = 75.34 %
accuracy (just for info) = 90.57 %
target score = 72.16 %
accuracy (just for info) = 89.95 %
[42]	validation_0-merror:0.094413	validation_1-merror:0.100559	validation_0-target_score:-0.753376	validation_1-target_score:-0.721589
target score = 75.38 %
accuracy (just for info) = 90.56 %
target score = 72.27 %
accuracy (just for info) = 89.97 %
[43]	validation_0-merror:0.094385	validation_1-merror:0.100335	validation_0-target_score:-0.753824	validation_1-target_score:-0.722696
target score = 75.40 %
accuracy (just for info) = 90.60 %
target score = 72.26 %
accuracy (just for info) = 89.96 %
[44]	validation_0-merror:0.094105	validation_1-merror:0.100391	validation_0-target_score:-0.754048	validation_1-target_score:-0.722585
target score = 75.45 %
accuracy (just for info) = 90.61 %
target score = 72.35 %
accuracy (just for info) = 89.99 %
[45]	validation_0-merror:0.093993	validation_1-merror:0.100056	validation_0-target_score:-0.754496	validation_1-target_score:-0.72347
target score = 75.46 %
accuracy (just for info) = 90.61 %
target score = 72.44 %
accuracy (just for info) = 89.99 %
[46]	validation_0-merror:0.093965	validation_1-merror:0.100056	validation_0-target_score:-0.754608	validation_1-target_score:-0.724355
target score = 75.50 %
accuracy (just for info) = 90.66 %
target score = 72.25 %
accuracy (just for info) = 90.02 %
[47]	validation_0-merror:0.09346	validation_1-merror:0.099832	validation_0-target_score:-0.755	validation_1-target_score:-0.722474
target score = 75.62 %
accuracy (just for info) = 90.67 %
target score = 72.46 %
accuracy (just for info) = 90.00 %
[48]	validation_0-merror:0.093376	validation_1-merror:0.1	validation_0-target_score:-0.756177	validation_1-target_score:-0.724577
target score = 75.71 %
accuracy (just for info) = 90.68 %
target score = 72.52 %
accuracy (just for info) = 90.01 %
[49]	validation_0-merror:0.093292	validation_1-merror:0.099888	validation_0-target_score:-0.757073	validation_1-target_score:-0.725241
target score = 75.79 %
accuracy (just for info) = 90.69 %
target score = 72.58 %
accuracy (just for info) = 90.02 %
[50]	validation_0-merror:0.093208	validation_1-merror:0.099832	validation_0-target_score:-0.757914	validation_1-target_score:-0.725794
target score = 75.78 %
accuracy (just for info) = 90.68 %
target score = 72.58 %
accuracy (just for info) = 90.06 %
[51]	validation_0-merror:0.093264	validation_1-merror:0.099497	validation_0-target_score:-0.757802	validation_1-target_score:-0.725794
target score = 75.82 %
accuracy (just for info) = 90.70 %
target score = 72.57 %
accuracy (just for info) = 90.04 %
[52]	validation_0-merror:0.093068	validation_1-merror:0.099609	validation_0-target_score:-0.75825	validation_1-target_score:-0.725683
target score = 75.92 %
accuracy (just for info) = 90.71 %
target score = 72.60 %
accuracy (just for info) = 90.04 %
[53]	validation_0-merror:0.093012	validation_1-merror:0.099553	validation_0-target_score:-0.759202	validation_1-target_score:-0.726015
target score = 75.98 %
accuracy (just for info) = 90.72 %
target score = 72.62 %
accuracy (just for info) = 90.04 %
[54]	validation_0-merror:0.092871	validation_1-merror:0.099553	validation_0-target_score:-0.759818	validation_1-target_score:-0.726237
target score = 75.99 %
accuracy (just for info) = 90.71 %
target score = 72.72 %
accuracy (just for info) = 90.05 %
[55]	validation_0-merror:0.092983	validation_1-merror:0.099497	validation_0-target_score:-0.759931	validation_1-target_score:-0.727232
target score = 76.09 %
accuracy (just for info) = 90.74 %
target score = 72.89 %
accuracy (just for info) = 90.07 %
[56]	validation_0-merror:0.092759	validation_1-merror:0.099273	validation_0-target_score:-0.760939	validation_1-target_score:-0.728892
target score = 76.13 %
accuracy (just for info) = 90.73 %
target score = 72.89 %
accuracy (just for info) = 90.09 %
[57]	validation_0-merror:0.092815	validation_1-merror:0.099106	validation_0-target_score:-0.761275	validation_1-target_score:-0.728892
target score = 76.21 %
accuracy (just for info) = 90.75 %
target score = 72.94 %
accuracy (just for info) = 90.07 %
[58]	validation_0-merror:0.092591	validation_1-merror:0.099329	validation_0-target_score:-0.762059	validation_1-target_score:-0.729446
target score = 76.28 %
accuracy (just for info) = 90.76 %
target score = 72.82 %
accuracy (just for info) = 90.09 %
[59]	validation_0-merror:0.092479	validation_1-merror:0.099106	validation_0-target_score:-0.762788	validation_1-target_score:-0.728228
target score = 76.37 %
accuracy (just for info) = 90.77 %
target score = 72.78 %
accuracy (just for info) = 90.08 %
[60]	validation_0-merror:0.092451	validation_1-merror:0.099162	validation_0-target_score:-0.76374	validation_1-target_score:-0.727786
target score = 76.34 %
accuracy (just for info) = 90.79 %
target score = 72.82 %
accuracy (just for info) = 90.07 %
[61]	validation_0-merror:0.092255	validation_1-merror:0.099329	validation_0-target_score:-0.763404	validation_1-target_score:-0.728228
target score = 76.45 %
accuracy (just for info) = 90.80 %
target score = 72.89 %
accuracy (just for info) = 90.10 %
[62]	validation_0-merror:0.092114	validation_1-merror:0.09905	validation_0-target_score:-0.764525	validation_1-target_score:-0.728892
target score = 76.49 %
accuracy (just for info) = 90.85 %
target score = 73.04 %
accuracy (just for info) = 90.12 %
[63]	validation_0-merror:0.091638	validation_1-merror:0.09877	validation_0-target_score:-0.764917	validation_1-target_score:-0.730442
target score = 76.55 %
accuracy (just for info) = 90.87 %
target score = 72.93 %
accuracy (just for info) = 90.15 %
[64]	validation_0-merror:0.091442	validation_1-merror:0.098547	validation_0-target_score:-0.765533	validation_1-target_score:-0.729335
target score = 76.64 %
accuracy (just for info) = 90.88 %
target score = 72.96 %
accuracy (just for info) = 90.11 %
[65]	validation_0-merror:0.09133	validation_1-merror:0.098882	validation_0-target_score:-0.766373	validation_1-target_score:-0.729556
target score = 76.74 %
accuracy (just for info) = 90.88 %
target score = 72.89 %
accuracy (just for info) = 90.13 %
[66]	validation_0-merror:0.091386	validation_1-merror:0.098658	validation_0-target_score:-0.767438	validation_1-target_score:-0.728892
target score = 76.81 %
accuracy (just for info) = 90.90 %
target score = 73.06 %
accuracy (just for info) = 90.16 %
[67]	validation_0-merror:0.091217	validation_1-merror:0.098379	validation_0-target_score:-0.76811	validation_1-target_score:-0.730552
target score = 76.90 %
accuracy (just for info) = 90.90 %
target score = 73.07 %
accuracy (just for info) = 90.15 %
[68]	validation_0-merror:0.091161	validation_1-merror:0.098491	validation_0-target_score:-0.768951	validation_1-target_score:-0.730663
target score = 76.95 %
accuracy (just for info) = 90.90 %
target score = 73.06 %
accuracy (just for info) = 90.17 %
[69]	validation_0-merror:0.091161	validation_1-merror:0.098323	validation_0-target_score:-0.769511	validation_1-target_score:-0.730552
target score = 76.94 %
accuracy (just for info) = 90.92 %
target score = 73.01 %
accuracy (just for info) = 90.17 %
[70]	validation_0-merror:0.090909	validation_1-merror:0.098267	validation_0-target_score:-0.769399	validation_1-target_score:-0.73011
target score = 77.00 %
accuracy (just for info) = 90.92 %
target score = 73.09 %
accuracy (just for info) = 90.15 %
[71]	validation_0-merror:0.090881	validation_1-merror:0.098547	validation_0-target_score:-0.770015	validation_1-target_score:-0.730884
target score = 77.10 %
accuracy (just for info) = 90.97 %
target score = 73.20 %
accuracy (just for info) = 90.16 %
[72]	validation_0-merror:0.090376	validation_1-merror:0.098435	validation_0-target_score:-0.771024	validation_1-target_score:-0.731991
target score = 77.11 %
accuracy (just for info) = 91.01 %
target score = 73.28 %
accuracy (just for info) = 90.15 %
[73]	validation_0-merror:0.090012	validation_1-merror:0.098491	validation_0-target_score:-0.771136	validation_1-target_score:-0.732765
target score = 77.14 %
accuracy (just for info) = 91.04 %
target score = 73.32 %
accuracy (just for info) = 90.16 %
[74]	validation_0-merror:0.089704	validation_1-merror:0.098435	validation_0-target_score:-0.771416	validation_1-target_score:-0.733208
target score = 77.16 %
accuracy (just for info) = 91.05 %
target score = 73.30 %
accuracy (just for info) = 90.18 %
[75]	validation_0-merror:0.08962	validation_1-merror:0.098211	validation_0-target_score:-0.77164	validation_1-target_score:-0.732987
target score = 77.31 %
accuracy (just for info) = 91.08 %
target score = 73.31 %
accuracy (just for info) = 90.19 %
[76]	validation_0-merror:0.089367	validation_1-merror:0.098099	validation_0-target_score:-0.773097	validation_1-target_score:-0.733097
target score = 77.46 %
accuracy (just for info) = 91.09 %
target score = 73.22 %
accuracy (just for info) = 90.17 %
[77]	validation_0-merror:0.089339	validation_1-merror:0.098267	validation_0-target_score:-0.774609	validation_1-target_score:-0.732212
target score = 77.46 %
accuracy (just for info) = 91.11 %
target score = 73.27 %
accuracy (just for info) = 90.15 %
[78]	validation_0-merror:0.089115	validation_1-merror:0.098491	validation_0-target_score:-0.774609	validation_1-target_score:-0.732655
target score = 77.59 %
accuracy (just for info) = 91.14 %
target score = 73.34 %
accuracy (just for info) = 90.17 %
[79]	validation_0-merror:0.088751	validation_1-merror:0.098267	validation_0-target_score:-0.775898	validation_1-target_score:-0.733429
target score = 77.61 %
accuracy (just for info) = 91.19 %
target score = 73.31 %
accuracy (just for info) = 90.18 %
[80]	validation_0-merror:0.088386	validation_1-merror:0.098211	validation_0-target_score:-0.776122	validation_1-target_score:-0.733097
target score = 77.67 %
accuracy (just for info) = 91.18 %
target score = 73.24 %
accuracy (just for info) = 90.17 %
[81]	validation_0-merror:0.088386	validation_1-merror:0.098267	validation_0-target_score:-0.776738	validation_1-target_score:-0.732433
target score = 77.76 %
accuracy (just for info) = 91.20 %
target score = 73.30 %
accuracy (just for info) = 90.20 %
[82]	validation_0-merror:0.088134	validation_1-merror:0.098044	validation_0-target_score:-0.777579	validation_1-target_score:-0.732987
target score = 77.81 %
accuracy (just for info) = 91.23 %
target score = 73.33 %
accuracy (just for info) = 90.19 %
[83]	validation_0-merror:0.087854	validation_1-merror:0.098099	validation_0-target_score:-0.778083	validation_1-target_score:-0.733319
target score = 77.89 %
accuracy (just for info) = 91.24 %
target score = 73.30 %
accuracy (just for info) = 90.22 %
[84]	validation_0-merror:0.087741	validation_1-merror:0.097764	validation_0-target_score:-0.778923	validation_1-target_score:-0.732987
target score = 77.88 %
accuracy (just for info) = 91.25 %
target score = 73.44 %
accuracy (just for info) = 90.23 %
[85]	validation_0-merror:0.087685	validation_1-merror:0.097652	validation_0-target_score:-0.778755	validation_1-target_score:-0.734425
target score = 77.93 %
accuracy (just for info) = 91.26 %
target score = 73.54 %
accuracy (just for info) = 90.20 %
[86]	validation_0-merror:0.087573	validation_1-merror:0.097988	validation_0-target_score:-0.779315	validation_1-target_score:-0.735421
target score = 78.03 %
accuracy (just for info) = 91.25 %
target score = 73.53 %
accuracy (just for info) = 90.21 %
[87]	validation_0-merror:0.087657	validation_1-merror:0.097876	validation_0-target_score:-0.780268	validation_1-target_score:-0.73531
target score = 78.16 %
accuracy (just for info) = 91.31 %
target score = 73.67 %
accuracy (just for info) = 90.23 %
[88]	validation_0-merror:0.087041	validation_1-merror:0.097708	validation_0-target_score:-0.781612	validation_1-target_score:-0.736749
target score = 78.20 %
accuracy (just for info) = 91.33 %
target score = 73.64 %
accuracy (just for info) = 90.23 %
[89]	validation_0-merror:0.086844	validation_1-merror:0.097708	validation_0-target_score:-0.782005	validation_1-target_score:-0.736417
target score = 78.26 %
accuracy (just for info) = 91.34 %
target score = 73.49 %
accuracy (just for info) = 90.22 %
[90]	validation_0-merror:0.086816	validation_1-merror:0.09782	validation_0-target_score:-0.782621	validation_1-target_score:-0.734868
target score = 78.41 %
accuracy (just for info) = 91.35 %
target score = 73.51 %
accuracy (just for info) = 90.22 %
[91]	validation_0-merror:0.086648	validation_1-merror:0.097764	validation_0-target_score:-0.784078	validation_1-target_score:-0.735089
target score = 78.43 %
accuracy (just for info) = 91.35 %
target score = 73.52 %
accuracy (just for info) = 90.23 %
[92]	validation_0-merror:0.086592	validation_1-merror:0.097708	validation_0-target_score:-0.784302	validation_1-target_score:-0.7352
target score = 78.41 %
accuracy (just for info) = 91.37 %
target score = 73.67 %
accuracy (just for info) = 90.24 %
[93]	validation_0-merror:0.08648	validation_1-merror:0.097652	validation_0-target_score:-0.784078	validation_1-target_score:-0.736749
target score = 78.53 %
accuracy (just for info) = 91.36 %
target score = 73.74 %
accuracy (just for info) = 90.22 %
[94]	validation_0-merror:0.086536	validation_1-merror:0.097764	validation_0-target_score:-0.785254	validation_1-target_score:-0.737413
target score = 78.61 %
accuracy (just for info) = 91.37 %
target score = 73.79 %
accuracy (just for info) = 90.22 %
[95]	validation_0-merror:0.086368	validation_1-merror:0.097764	validation_0-target_score:-0.786094	validation_1-target_score:-0.737855
target score = 78.65 %
accuracy (just for info) = 91.37 %
target score = 73.85 %
accuracy (just for info) = 90.26 %
[96]	validation_0-merror:0.086508	validation_1-merror:0.097429	validation_0-target_score:-0.786487	validation_1-target_score:-0.738519
target score = 78.77 %
accuracy (just for info) = 91.39 %
target score = 73.71 %
accuracy (just for info) = 90.25 %
[97]	validation_0-merror:0.086256	validation_1-merror:0.097541	validation_0-target_score:-0.787719	validation_1-target_score:-0.737081
target score = 78.81 %
accuracy (just for info) = 91.42 %
target score = 73.61 %
accuracy (just for info) = 90.28 %
[98]	validation_0-merror:0.086003	validation_1-merror:0.097205	validation_0-target_score:-0.788111	validation_1-target_score:-0.736085
target score = 78.92 %
accuracy (just for info) = 91.42 %
target score = 73.73 %
accuracy (just for info) = 90.27 %
[99]	validation_0-merror:0.085975	validation_1-merror:0.097317	validation_0-target_score:-0.789176	validation_1-target_score:-0.737302
target score = 79.14 %
accuracy (just for info) = 91.43 %
target score = 73.67 %
accuracy (just for info) = 90.28 %
[100]	validation_0-merror:0.085835	validation_1-merror:0.097373	validation_0-target_score:-0.791417	validation_1-target_score:-0.736749
target score = 79.28 %
accuracy (just for info) = 91.45 %
target score = 73.70 %
accuracy (just for info) = 90.26 %
[101]	validation_0-merror:0.085639	validation_1-merror:0.097596	validation_0-target_score:-0.792761	validation_1-target_score:-0.73697
target score = 79.38 %
accuracy (just for info) = 91.46 %
target score = 73.72 %
accuracy (just for info) = 90.25 %
[102]	validation_0-merror:0.085555	validation_1-merror:0.097652	validation_0-target_score:-0.793826	validation_1-target_score:-0.737192
target score = 79.41 %
accuracy (just for info) = 91.48 %
target score = 73.67 %
accuracy (just for info) = 90.27 %
[103]	validation_0-merror:0.085331	validation_1-merror:0.097373	validation_0-target_score:-0.79405	validation_1-target_score:-0.736749
target score = 79.38 %
accuracy (just for info) = 91.49 %
target score = 73.70 %
accuracy (just for info) = 90.23 %
[104]	validation_0-merror:0.085219	validation_1-merror:0.097708	validation_0-target_score:-0.793826	validation_1-target_score:-0.73697
target score = 79.44 %
accuracy (just for info) = 91.51 %
target score = 73.75 %
accuracy (just for info) = 90.25 %
[105]	validation_0-merror:0.085134	validation_1-merror:0.097596	validation_0-target_score:-0.794386	validation_1-target_score:-0.737524
target score = 79.42 %
accuracy (just for info) = 91.51 %
target score = 73.80 %
accuracy (just for info) = 90.23 %
[106]	validation_0-merror:0.08505	validation_1-merror:0.097876	validation_0-target_score:-0.794162	validation_1-target_score:-0.737966
target score = 79.57 %
accuracy (just for info) = 91.54 %
target score = 73.84 %
accuracy (just for info) = 90.25 %
[107]	validation_0-merror:0.08477	validation_1-merror:0.097652	validation_0-target_score:-0.795731	validation_1-target_score:-0.738409
target score = 79.71 %
accuracy (just for info) = 91.55 %
target score = 73.92 %
accuracy (just for info) = 90.25 %
[108]	validation_0-merror:0.084714	validation_1-merror:0.097485	validation_0-target_score:-0.797131	validation_1-target_score:-0.739183
target score = 79.83 %
accuracy (just for info) = 91.57 %
target score = 73.77 %
accuracy (just for info) = 90.27 %
[109]	validation_0-merror:0.08449	validation_1-merror:0.097541	validation_0-target_score:-0.798252	validation_1-target_score:-0.737745
target score = 79.93 %
accuracy (just for info) = 91.61 %
target score = 73.86 %
accuracy (just for info) = 90.27 %
[110]	validation_0-merror:0.084069	validation_1-merror:0.097541	validation_0-target_score:-0.799316	validation_1-target_score:-0.73863
target score = 80.02 %
accuracy (just for info) = 91.60 %
target score = 73.91 %
accuracy (just for info) = 90.29 %
[111]	validation_0-merror:0.084125	validation_1-merror:0.097261	validation_0-target_score:-0.800213	validation_1-target_score:-0.739073
target score = 80.06 %
accuracy (just for info) = 91.61 %
target score = 73.80 %
accuracy (just for info) = 90.29 %
[112]	validation_0-merror:0.084069	validation_1-merror:0.097149	validation_0-target_score:-0.800605	validation_1-target_score:-0.737966
target score = 80.13 %
accuracy (just for info) = 91.63 %
target score = 73.76 %
accuracy (just for info) = 90.30 %
[113]	validation_0-merror:0.083845	validation_1-merror:0.097037	validation_0-target_score:-0.801333	validation_1-target_score:-0.737634
target score = 80.14 %
accuracy (just for info) = 91.64 %
target score = 73.75 %
accuracy (just for info) = 90.31 %
[114]	validation_0-merror:0.083761	validation_1-merror:0.096982	validation_0-target_score:-0.801389	validation_1-target_score:-0.737524
target score = 80.14 %
accuracy (just for info) = 91.63 %
target score = 73.75 %
accuracy (just for info) = 90.31 %
[115]	validation_0-merror:0.083845	validation_1-merror:0.096926	validation_0-target_score:-0.801445	validation_1-target_score:-0.737524
target score = 80.29 %
accuracy (just for info) = 91.67 %
target score = 73.81 %
accuracy (just for info) = 90.32 %
[116]	validation_0-merror:0.083424	validation_1-merror:0.096814	validation_0-target_score:-0.802902	validation_1-target_score:-0.738077
target score = 80.27 %
accuracy (just for info) = 91.67 %
target score = 73.77 %
accuracy (just for info) = 90.32 %
[117]	validation_0-merror:0.083424	validation_1-merror:0.096758	validation_0-target_score:-0.802678	validation_1-target_score:-0.737745
target score = 80.32 %
accuracy (just for info) = 91.71 %
target score = 73.71 %
accuracy (just for info) = 90.30 %
[118]	validation_0-merror:0.083032	validation_1-merror:0.096982	validation_0-target_score:-0.803238	validation_1-target_score:-0.737081
target score = 80.36 %
accuracy (just for info) = 91.74 %
target score = 73.76 %
accuracy (just for info) = 90.31 %
[119]	validation_0-merror:0.082696	validation_1-merror:0.09687	validation_0-target_score:-0.803574	validation_1-target_score:-0.737634
target score = 80.38 %
accuracy (just for info) = 91.77 %
target score = 73.92 %
accuracy (just for info) = 90.31 %
[120]	validation_0-merror:0.082415	validation_1-merror:0.096982	validation_0-target_score:-0.803799	validation_1-target_score:-0.739183
target score = 80.45 %
accuracy (just for info) = 91.78 %
target score = 73.89 %
accuracy (just for info) = 90.32 %
[121]	validation_0-merror:0.082331	validation_1-merror:0.09687	validation_0-target_score:-0.804527	validation_1-target_score:-0.738851
target score = 80.45 %
accuracy (just for info) = 91.79 %
target score = 73.92 %
accuracy (just for info) = 90.31 %
[122]	validation_0-merror:0.082219	validation_1-merror:0.096926	validation_0-target_score:-0.804471	validation_1-target_score:-0.739183
target score = 80.50 %
accuracy (just for info) = 91.79 %
target score = 73.89 %
accuracy (just for info) = 90.32 %
[123]	validation_0-merror:0.082219	validation_1-merror:0.096982	validation_0-target_score:-0.804975	validation_1-target_score:-0.738851
target score = 80.70 %
accuracy (just for info) = 91.80 %
target score = 73.85 %
accuracy (just for info) = 90.35 %
[124]	validation_0-merror:0.082163	validation_1-merror:0.09659	validation_0-target_score:-0.806992	validation_1-target_score:-0.738519
target score = 80.79 %
accuracy (just for info) = 91.82 %
target score = 73.74 %
accuracy (just for info) = 90.34 %
[125]	validation_0-merror:0.081967	validation_1-merror:0.096814	validation_0-target_score:-0.807888	validation_1-target_score:-0.737413
target score = 80.82 %
accuracy (just for info) = 91.81 %
target score = 73.80 %
accuracy (just for info) = 90.34 %
[126]	validation_0-merror:0.082023	validation_1-merror:0.096702	validation_0-target_score:-0.808225	validation_1-target_score:-0.737966
target score = 80.91 %
accuracy (just for info) = 91.83 %
target score = 73.82 %
accuracy (just for info) = 90.35 %
[127]	validation_0-merror:0.081855	validation_1-merror:0.09659	validation_0-target_score:-0.809065	validation_1-target_score:-0.738187
target score = 80.95 %
accuracy (just for info) = 91.85 %
target score = 73.85 %
accuracy (just for info) = 90.35 %
[128]	validation_0-merror:0.081602	validation_1-merror:0.09659	validation_0-target_score:-0.809457	validation_1-target_score:-0.738519
target score = 81.02 %
accuracy (just for info) = 91.87 %
target score = 73.80 %
accuracy (just for info) = 90.35 %
[129]	validation_0-merror:0.081406	validation_1-merror:0.096646	validation_0-target_score:-0.810185	validation_1-target_score:-0.737966
target score = 81.04 %
accuracy (just for info) = 91.87 %
target score = 73.80 %
accuracy (just for info) = 90.35 %
[130]	validation_0-merror:0.081406	validation_1-merror:0.09659	validation_0-target_score:-0.81041	validation_1-target_score:-0.737966
target score = 81.10 %
accuracy (just for info) = 91.86 %
target score = 73.81 %
accuracy (just for info) = 90.38 %
[131]	validation_0-merror:0.08149	validation_1-merror:0.096311	validation_0-target_score:-0.811026	validation_1-target_score:-0.738077
target score = 81.14 %
accuracy (just for info) = 91.88 %
target score = 73.85 %
accuracy (just for info) = 90.38 %
[132]	validation_0-merror:0.08135	validation_1-merror:0.096255	validation_0-target_score:-0.811362	validation_1-target_score:-0.738519
target score = 81.20 %
accuracy (just for info) = 91.89 %
target score = 73.85 %
accuracy (just for info) = 90.37 %
[133]	validation_0-merror:0.081182	validation_1-merror:0.096478	validation_0-target_score:-0.812034	validation_1-target_score:-0.738519
target score = 81.20 %
accuracy (just for info) = 91.90 %
target score = 73.90 %
accuracy (just for info) = 90.37 %
[134]	validation_0-merror:0.081154	validation_1-merror:0.096367	validation_0-target_score:-0.811978	validation_1-target_score:-0.738962
target score = 81.23 %
accuracy (just for info) = 91.90 %
target score = 73.86 %
accuracy (just for info) = 90.36 %
[135]	validation_0-merror:0.081042	validation_1-merror:0.096534	validation_0-target_score:-0.812314	validation_1-target_score:-0.73863
target score = 81.35 %
accuracy (just for info) = 91.94 %
target score = 73.75 %
accuracy (just for info) = 90.37 %
[136]	validation_0-merror:0.080958	validation_1-merror:0.096367	validation_0-target_score:-0.813491	validation_1-target_score:-0.737524
target score = 81.50 %
accuracy (just for info) = 91.95 %
target score = 73.63 %
accuracy (just for info) = 90.38 %
[137]	validation_0-merror:0.080761	validation_1-merror:0.096199	validation_0-target_score:-0.815004	validation_1-target_score:-0.736306
target score = 81.71 %
accuracy (just for info) = 91.98 %
target score = 73.56 %
accuracy (just for info) = 90.37 %
[138]	validation_0-merror:0.080341	validation_1-merror:0.096311	validation_0-target_score:-0.817077	validation_1-target_score:-0.735642
target score = 81.86 %
accuracy (just for info) = 92.02 %
target score = 73.64 %
accuracy (just for info) = 90.35 %
[139]	validation_0-merror:0.080004	validation_1-merror:0.096478	validation_0-target_score:-0.818645	validation_1-target_score:-0.736417
target score = 81.96 %
accuracy (just for info) = 92.02 %
target score = 73.72 %
accuracy (just for info) = 90.36 %
[140]	validation_0-merror:0.079892	validation_1-merror:0.096478	validation_0-target_score:-0.819598	validation_1-target_score:-0.737192
target score = 82.00 %
accuracy (just for info) = 92.03 %
target score = 73.65 %
accuracy (just for info) = 90.37 %
[141]	validation_0-merror:0.079836	validation_1-merror:0.096478	validation_0-target_score:-0.820046	validation_1-target_score:-0.736528
target score = 82.07 %
accuracy (just for info) = 92.05 %
target score = 73.61 %
accuracy (just for info) = 90.39 %
[142]	validation_0-merror:0.079556	validation_1-merror:0.096143	validation_0-target_score:-0.820662	validation_1-target_score:-0.736085
target score = 82.14 %
accuracy (just for info) = 92.06 %
target score = 73.55 %
accuracy (just for info) = 90.39 %
[143]	validation_0-merror:0.079472	validation_1-merror:0.096143	validation_0-target_score:-0.821391	validation_1-target_score:-0.735532
target score = 82.20 %
accuracy (just for info) = 92.08 %
target score = 73.59 %
accuracy (just for info) = 90.39 %
[144]	validation_0-merror:0.079388	validation_1-merror:0.096143	validation_0-target_score:-0.822007	validation_1-target_score:-0.735864
target score = 82.25 %
accuracy (just for info) = 92.08 %
target score = 73.74 %
accuracy (just for info) = 90.39 %
[145]	validation_0-merror:0.079248	validation_1-merror:0.096255	validation_0-target_score:-0.822511	validation_1-target_score:-0.737413
target score = 82.28 %
accuracy (just for info) = 92.12 %
target score = 73.82 %
accuracy (just for info) = 90.38 %
[146]	validation_0-merror:0.078939	validation_1-merror:0.096367	validation_0-target_score:-0.822791	validation_1-target_score:-0.738187
target score = 82.30 %
accuracy (just for info) = 92.13 %
target score = 73.84 %
accuracy (just for info) = 90.38 %
[147]	validation_0-merror:0.078743	validation_1-merror:0.096199	validation_0-target_score:-0.823015	validation_1-target_score:-0.738409
target score = 82.37 %
accuracy (just for info) = 92.14 %
target score = 73.83 %
accuracy (just for info) = 90.38 %
[148]	validation_0-merror:0.078659	validation_1-merror:0.096255	validation_0-target_score:-0.823744	validation_1-target_score:-0.738298
target score = 82.36 %
accuracy (just for info) = 92.16 %
target score = 73.76 %
accuracy (just for info) = 90.37 %
[149]	validation_0-merror:0.078435	validation_1-merror:0.096367	validation_0-target_score:-0.823632	validation_1-target_score:-0.737634
target score = 82.42 %
accuracy (just for info) = 92.16 %
target score = 73.81 %
accuracy (just for info) = 90.39 %
[150]	validation_0-merror:0.078407	validation_1-merror:0.096199	validation_0-target_score:-0.824192	validation_1-target_score:-0.738077
target score = 82.50 %
accuracy (just for info) = 92.16 %
target score = 73.76 %
accuracy (just for info) = 90.35 %
[151]	validation_0-merror:0.078407	validation_1-merror:0.09659	validation_0-target_score:-0.824976	validation_1-target_score:-0.737634
target score = 82.57 %
accuracy (just for info) = 92.18 %
target score = 73.74 %
accuracy (just for info) = 90.35 %
[152]	validation_0-merror:0.078295	validation_1-merror:0.096702	validation_0-target_score:-0.825705	validation_1-target_score:-0.737413
target score = 82.65 %
accuracy (just for info) = 92.19 %
target score = 73.84 %
accuracy (just for info) = 90.36 %
[153]	validation_0-merror:0.078154	validation_1-merror:0.096702	validation_0-target_score:-0.826545	validation_1-target_score:-0.738409
target score = 82.68 %
accuracy (just for info) = 92.19 %
target score = 73.84 %
accuracy (just for info) = 90.36 %
[154]	validation_0-merror:0.078154	validation_1-merror:0.096423	validation_0-target_score:-0.826825	validation_1-target_score:-0.738409
target score = 82.74 %
accuracy (just for info) = 92.20 %
target score = 73.73 %
accuracy (just for info) = 90.38 %
[155]	validation_0-merror:0.07807	validation_1-merror:0.096255	validation_0-target_score:-0.827385	validation_1-target_score:-0.737302
target score = 82.74 %
accuracy (just for info) = 92.22 %
target score = 73.80 %
accuracy (just for info) = 90.37 %
[156]	validation_0-merror:0.077818	validation_1-merror:0.096367	validation_0-target_score:-0.827441	validation_1-target_score:-0.737966
target score = 82.79 %
accuracy (just for info) = 92.24 %
target score = 73.79 %
accuracy (just for info) = 90.38 %
[157]	validation_0-merror:0.077678	validation_1-merror:0.096199	validation_0-target_score:-0.827946	validation_1-target_score:-0.737855
target score = 82.79 %
accuracy (just for info) = 92.27 %
target score = 73.80 %
accuracy (just for info) = 90.38 %
[158]	validation_0-merror:0.077369	validation_1-merror:0.096255	validation_0-target_score:-0.827946	validation_1-target_score:-0.737966
Stopping. Best iteration:
[108]	validation_0-merror:0.084714	validation_1-merror:0.097485	validation_0-target_score:-0.797131	validation_1-target_score:-0.739183
[0, 1, 2]
0.23898305688
35672 17891
['a_bi_freq_suff' 'a_three_freq_suff' 'a_four_freq_suff' 'a_five_freq_suff'
 'a_bi_freq_pref' 'a_three_freq_pref' 'a_four_freq_pref' 'a_five_freq_pref'
 'a_sn_gram_freq' 'the_bi_freq_suff' 'the_three_freq_suff'
 'the_four_freq_suff' 'the_five_freq_suff' 'the_bi_freq_pref'
 'the_three_freq_pref' 'the_four_freq_pref' 'the_five_freq_pref'
 'the_sn_gram_freq' 'a_the_bi_freq_suff_p1' 'the_a_bi_freq_suff_p1'
 'a_the_three_freq_suff_p1' 'the_a_three_freq_suff_p1'
 'a_the_four_freq_suff_p1' 'the_a_four_freq_suff_p1'
 'a_the_five_freq_suff_p1' 'the_a_five_freq_suff_p1'
 'a_the_bi_freq_pref_p1' 'the_a_bi_freq_pref_p1' 'a_the_three_freq_pref_p1'
 'the_a_three_freq_pref_p1' 'a_the_four_freq_pref_p1'
 'the_a_four_freq_pref_p1' 'a_the_five_freq_pref_p1'
 'the_a_five_freq_pref_p1' 'a_the_sn_ngram_freq_p1'
 'the_a_sn_ngram_freq_p1' 'a_the_bi_freq_suff_p10' 'the_a_bi_freq_suff_p10'
 'a_the_three_freq_suff_p10' 'the_a_three_freq_suff_p10'
 'a_the_four_freq_suff_p10' 'the_a_four_freq_suff_p10'
 'a_the_five_freq_suff_p10' 'the_a_five_freq_suff_p10'
 'a_the_bi_freq_pref_p10' 'the_a_bi_freq_pref_p10'
 'a_the_three_freq_pref_p10' 'the_a_three_freq_pref_p10'
 'a_the_four_freq_pref_p10' 'the_a_four_freq_pref_p10'
 'a_the_five_freq_pref_p10' 'the_a_five_freq_pref_p10'
 'a_the_sn_ngram_freq_p10' 'the_a_sn_ngram_freq_p10'
 'a_the_bi_freq_suff_p100' 'the_a_bi_freq_suff_p100'
 'a_the_three_freq_suff_p100' 'the_a_three_freq_suff_p100'
 'a_the_four_freq_suff_p100' 'the_a_four_freq_suff_p100'
 'a_the_five_freq_suff_p100' 'the_a_five_freq_suff_p100'
 'a_the_bi_freq_pref_p100' 'the_a_bi_freq_pref_p100'
 'a_the_three_freq_pref_p100' 'the_a_three_freq_pref_p100'
 'a_the_four_freq_pref_p100' 'the_a_four_freq_pref_p100'
 'a_the_five_freq_pref_p100' 'the_a_five_freq_pref_p100'
 'a_the_sn_ngram_freq_p100' 'the_a_sn_ngram_freq_p100' 'st_with_v'
 'article']
target score = 68.72 %
accuracy (just for info) = 87.87 %
target score = 66.62 %
accuracy (just for info) = 87.73 %
[0]	validation_0-merror:0.12158	validation_1-merror:0.122967	validation_0-target_score:-0.687242	validation_1-target_score:-0.666182
Multiple eval metrics have been passed: 'validation_1-target_score' will be used for early stopping.
Will train until validation_1-target_score hasn't improved in 50 rounds.
target score = 69.00 %
accuracy (just for info) = 88.78 %
target score = 67.44 %
accuracy (just for info) = 88.47 %
[1]	validation_0-merror:0.112161	validation_1-merror:0.115365	validation_0-target_score:-0.690028	validation_1-target_score:-0.674351
target score = 69.11 %
accuracy (just for info) = 89.08 %
target score = 67.11 %
accuracy (just for info) = 88.59 %
[2]	validation_0-merror:0.112609	validation_1-merror:0.115253	validation_0-target_score:-0.691142	validation_1-target_score:-0.671106
target score = 69.28 %
accuracy (just for info) = 89.10 %
target score = 67.04 %
accuracy (just for info) = 88.58 %
[3]	validation_0-merror:0.109301	validation_1-merror:0.114247	validation_0-target_score:-0.692758	validation_1-target_score:-0.670434
target score = 69.00 %
accuracy (just for info) = 89.30 %
target score = 66.94 %
accuracy (just for info) = 88.71 %
[4]	validation_0-merror:0.107059	validation_1-merror:0.113074	validation_0-target_score:-0.690028	validation_1-target_score:-0.669427
target score = 68.97 %
accuracy (just for info) = 89.30 %
target score = 66.99 %
accuracy (just for info) = 88.75 %
[5]	validation_0-merror:0.107059	validation_1-merror:0.112571	validation_0-target_score:-0.689749	validation_1-target_score:-0.669875
target score = 69.38 %
accuracy (just for info) = 89.48 %
target score = 67.22 %
accuracy (just for info) = 88.91 %
[6]	validation_0-merror:0.105237	validation_1-merror:0.11095	validation_0-target_score:-0.69376	validation_1-target_score:-0.672225
target score = 69.84 %
accuracy (just for info) = 89.54 %
target score = 67.45 %
accuracy (just for info) = 88.93 %
[7]	validation_0-merror:0.104676	validation_1-merror:0.110726	validation_0-target_score:-0.698384	validation_1-target_score:-0.674463
target score = 70.16 %
accuracy (just for info) = 89.56 %
target score = 67.75 %
accuracy (just for info) = 88.89 %
[8]	validation_0-merror:0.10448	validation_1-merror:0.111061	validation_0-target_score:-0.70156	validation_1-target_score:-0.677484
target score = 70.23 %
accuracy (just for info) = 89.61 %
target score = 67.64 %
accuracy (just for info) = 88.99 %
[9]	validation_0-merror:0.103891	validation_1-merror:0.110111	validation_0-target_score:-0.70234	validation_1-target_score:-0.676365
target score = 70.94 %
accuracy (just for info) = 89.70 %
target score = 68.24 %
accuracy (just for info) = 89.12 %
[10]	validation_0-merror:0.102994	validation_1-merror:0.108882	validation_0-target_score:-0.709415	validation_1-target_score:-0.682408
target score = 70.89 %
accuracy (just for info) = 89.88 %
target score = 68.34 %
accuracy (just for info) = 89.25 %
[11]	validation_0-merror:0.101256	validation_1-merror:0.107596	validation_0-target_score:-0.708914	validation_1-target_score:-0.683415
target score = 71.30 %
accuracy (just for info) = 90.01 %
target score = 68.72 %
accuracy (just for info) = 89.26 %
[12]	validation_0-merror:0.099882	validation_1-merror:0.107484	validation_0-target_score:-0.713036	validation_1-target_score:-0.68722
target score = 71.77 %
accuracy (just for info) = 90.04 %
target score = 69.26 %
accuracy (just for info) = 89.35 %
[13]	validation_0-merror:0.09963	validation_1-merror:0.106646	validation_0-target_score:-0.71766	validation_1-target_score:-0.692592
target score = 71.70 %
accuracy (just for info) = 90.04 %
target score = 68.97 %
accuracy (just for info) = 89.38 %
[14]	validation_0-merror:0.099574	validation_1-merror:0.10631	validation_0-target_score:-0.717047	validation_1-target_score:-0.689682
target score = 71.84 %
accuracy (just for info) = 90.06 %
target score = 69.20 %
accuracy (just for info) = 89.37 %
[15]	validation_0-merror:0.099462	validation_1-merror:0.106422	validation_0-target_score:-0.718384	validation_1-target_score:-0.692032
target score = 72.07 %
accuracy (just for info) = 90.13 %
target score = 69.09 %
accuracy (just for info) = 89.41 %
[16]	validation_0-merror:0.098733	validation_1-merror:0.106031	validation_0-target_score:-0.720724	validation_1-target_score:-0.690913
target score = 72.43 %
accuracy (just for info) = 90.16 %
target score = 69.59 %
accuracy (just for info) = 89.47 %
[17]	validation_0-merror:0.098368	validation_1-merror:0.105416	validation_0-target_score:-0.724345	validation_1-target_score:-0.695949
target score = 72.43 %
accuracy (just for info) = 90.18 %
target score = 69.81 %
accuracy (just for info) = 89.53 %
[18]	validation_0-merror:0.0982	validation_1-merror:0.104857	validation_0-target_score:-0.72429	validation_1-target_score:-0.698075
target score = 72.72 %
accuracy (just for info) = 90.20 %
target score = 70.10 %
accuracy (just for info) = 89.55 %
[19]	validation_0-merror:0.097976	validation_1-merror:0.104578	validation_0-target_score:-0.727187	validation_1-target_score:-0.700985
target score = 72.94 %
accuracy (just for info) = 90.26 %
target score = 70.42 %
accuracy (just for info) = 89.56 %
[20]	validation_0-merror:0.097471	validation_1-merror:0.104578	validation_0-target_score:-0.729415	validation_1-target_score:-0.70423
target score = 72.97 %
accuracy (just for info) = 90.26 %
target score = 70.61 %
accuracy (just for info) = 89.60 %
[21]	validation_0-merror:0.097359	validation_1-merror:0.104131	validation_0-target_score:-0.729694	validation_1-target_score:-0.706132
target score = 73.22 %
accuracy (just for info) = 90.26 %
target score = 70.75 %
accuracy (just for info) = 89.61 %
[22]	validation_0-merror:0.097443	validation_1-merror:0.103907	validation_0-target_score:-0.732201	validation_1-target_score:-0.707475
target score = 73.41 %
accuracy (just for info) = 90.27 %
target score = 70.95 %
accuracy (just for info) = 89.65 %
[23]	validation_0-merror:0.097331	validation_1-merror:0.103516	validation_0-target_score:-0.734095	validation_1-target_score:-0.70949
target score = 73.69 %
accuracy (just for info) = 90.31 %
target score = 71.07 %
accuracy (just for info) = 89.69 %
[24]	validation_0-merror:0.096939	validation_1-merror:0.10318	validation_0-target_score:-0.73688	validation_1-target_score:-0.710721
target score = 73.62 %
accuracy (just for info) = 90.37 %
target score = 71.12 %
accuracy (just for info) = 89.70 %
[25]	validation_0-merror:0.09635	validation_1-merror:0.103069	validation_0-target_score:-0.736156	validation_1-target_score:-0.711168
target score = 73.78 %
accuracy (just for info) = 90.38 %
target score = 71.32 %
accuracy (just for info) = 89.70 %
[26]	validation_0-merror:0.096266	validation_1-merror:0.103069	validation_0-target_score:-0.737772	validation_1-target_score:-0.713183
target score = 73.97 %
accuracy (just for info) = 90.38 %
target score = 71.59 %
accuracy (just for info) = 89.73 %
[27]	validation_0-merror:0.09621	validation_1-merror:0.102789	validation_0-target_score:-0.739666	validation_1-target_score:-0.715868
target score = 74.17 %
accuracy (just for info) = 90.41 %
target score = 71.60 %
accuracy (just for info) = 89.77 %
[28]	validation_0-merror:0.095958	validation_1-merror:0.102454	validation_0-target_score:-0.741727	validation_1-target_score:-0.71598
target score = 74.31 %
accuracy (just for info) = 90.42 %
target score = 71.65 %
accuracy (just for info) = 89.80 %
[29]	validation_0-merror:0.095845	validation_1-merror:0.102062	validation_0-target_score:-0.743064	validation_1-target_score:-0.71654
target score = 74.42 %
accuracy (just for info) = 90.44 %
target score = 71.50 %
accuracy (just for info) = 89.85 %
[30]	validation_0-merror:0.095677	validation_1-merror:0.101615	validation_0-target_score:-0.744178	validation_1-target_score:-0.714973
target score = 74.41 %
accuracy (just for info) = 90.47 %
target score = 71.50 %
accuracy (just for info) = 89.82 %
[31]	validation_0-merror:0.095369	validation_1-merror:0.101895	validation_0-target_score:-0.744067	validation_1-target_score:-0.714973
target score = 74.43 %
accuracy (just for info) = 90.52 %
target score = 71.65 %
accuracy (just for info) = 89.84 %
[32]	validation_0-merror:0.09492	validation_1-merror:0.101671	validation_0-target_score:-0.744345	validation_1-target_score:-0.71654
target score = 74.57 %
accuracy (just for info) = 90.52 %
target score = 71.60 %
accuracy (just for info) = 89.86 %
[33]	validation_0-merror:0.094892	validation_1-merror:0.101504	validation_0-target_score:-0.745682	validation_1-target_score:-0.71598
target score = 74.56 %
accuracy (just for info) = 90.51 %
target score = 71.64 %
accuracy (just for info) = 89.89 %
[34]	validation_0-merror:0.094892	validation_1-merror:0.101224	validation_0-target_score:-0.745627	validation_1-target_score:-0.716428
target score = 74.61 %
accuracy (just for info) = 90.51 %
target score = 71.63 %
accuracy (just for info) = 89.89 %
[35]	validation_0-merror:0.09492	validation_1-merror:0.101224	validation_0-target_score:-0.746128	validation_1-target_score:-0.716316
target score = 74.77 %
accuracy (just for info) = 90.51 %
^Ctarget score = 71.74 %Traceback (most recent call last):
  File "/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py", line 2881, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "<ipython-input-9-9c7caea67209>", line 1, in <module>
    perform_xgboost_cv(df.copy())
  File "<ipython-input-8-67d97faa6684>", line 443, in perform_xgboost_cv
    early_stopping_rounds=50
  File "/usr/local/lib/python2.7/dist-packages/xgboost-0.6-py2.7.egg/xgboost/sklearn.py", line 464, in fit
    verbose_eval=verbose)
  File "/usr/local/lib/python2.7/dist-packages/xgboost-0.6-py2.7.egg/xgboost/training.py", line 204, in train
    xgb_model=xgb_model, callbacks=callbacks)
  File "/usr/local/lib/python2.7/dist-packages/xgboost-0.6-py2.7.egg/xgboost/training.py", line 84, in _train_internal
    bst_eval_set = bst.eval_set(evals, i, feval)
  File "/usr/local/lib/python2.7/dist-packages/xgboost-0.6-py2.7.egg/xgboost/core.py", line 883, in eval_set
    feval_ret = feval(self.predict(dmat), dmat)
  File "<ipython-input-8-67d97faa6684>", line 416, in my_obj
    return eval_target_score(arts, preds, labels)
  File "<ipython-input-8-67d97faa6684>", line 389, in eval_target_score
    print 'target score = %.2f %%' % (score * 100)
KeyboardInterrupt
perform_xgboost_cv(df.copy())
 35673 17890
['a_bi_freq_suff' 'a_three_freq_suff' 'a_four_freq_suff' 'a_five_freq_suff'
 'a_bi_freq_pref' 'a_three_freq_pref' 'a_four_freq_pref' 'a_five_freq_pref'
 'a_sn_gram_freq' 'the_bi_freq_suff' 'the_three_freq_suff'
 'the_four_freq_suff' 'the_five_freq_suff' 'the_bi_freq_pref'
 'the_three_freq_pref' 'the_four_freq_pref' 'the_five_freq_pref'
 'the_sn_gram_freq' 'a_the_bi_freq_suff_p1' 'the_a_bi_freq_suff_p1'
 'a_the_three_freq_suff_p1' 'the_a_three_freq_suff_p1'
 'a_the_four_freq_suff_p1' 'the_a_four_freq_suff_p1'
 'a_the_five_freq_suff_p1' 'the_a_five_freq_suff_p1'
 'a_the_bi_freq_pref_p1' 'the_a_bi_freq_pref_p1' 'a_the_three_freq_pref_p1'
 'the_a_three_freq_pref_p1' 'a_the_four_freq_pref_p1'
 'the_a_four_freq_pref_p1' 'a_the_five_freq_pref_p1'
 'the_a_five_freq_pref_p1' 'a_the_sn_ngram_freq_p1'
 'the_a_sn_ngram_freq_p1' 'a_the_bi_freq_suff_p10' 'the_a_bi_freq_suff_p10'
 'a_the_three_freq_suff_p10' 'the_a_three_freq_suff_p10'
 'a_the_four_freq_suff_p10' 'the_a_four_freq_suff_p10'
 'a_the_five_freq_suff_p10' 'the_a_five_freq_suff_p10'
 'a_the_bi_freq_pref_p10' 'the_a_bi_freq_pref_p10'
 'a_the_three_freq_pref_p10' 'the_a_three_freq_pref_p10'
 'a_the_four_freq_pref_p10' 'the_a_four_freq_pref_p10'
 'a_the_five_freq_pref_p10' 'the_a_five_freq_pref_p10'
 'a_the_sn_ngram_freq_p10' 'the_a_sn_ngram_freq_p10'
 'a_the_bi_freq_suff_p100' 'the_a_bi_freq_suff_p100'
 'a_the_three_freq_suff_p100' 'the_a_three_freq_suff_p100'
 'a_the_four_freq_suff_p100' 'the_a_four_freq_suff_p100'
 'a_the_five_freq_suff_p100' 'the_a_five_freq_suff_p100'
 'a_the_bi_freq_pref_p100' 'the_a_bi_freq_pref_p100'
 'a_the_three_freq_pref_p100' 'the_a_three_freq_pref_p100'
 'a_the_four_freq_pref_p100' 'the_a_four_freq_pref_p100'
 'a_the_five_freq_pref_p100' 'the_a_five_freq_pref_p100'
 'a_the_sn_ngram_freq_p100' 'the_a_sn_ngram_freq_p100' 'st_with_v'
 'article']
target score = 67.28 %
accuracy (just for info) = 88.89 %
target score = 65.13 %
accuracy (just for info) = 88.41 %
[0]	validation_0-merror:0.114456	validation_1-merror:0.11839	validation_0-target_score:-0.672811	validation_1-target_score:-0.651322
Multiple eval metrics have been passed: 'validation_1-target_score' will be used for early stopping.
Will train until validation_1-target_score hasn't improved in 50 rounds.
target score = 69.47 %
accuracy (just for info) = 89.03 %
target score = 67.10 %
accuracy (just for info) = 88.73 %
[1]	validation_0-merror:0.109691	validation_1-merror:0.112745	validation_0-target_score:-0.694717	validation_1-target_score:-0.671019
target score = 69.20 %
accuracy (just for info) = 89.17 %
target score = 67.40 %
accuracy (just for info) = 88.69 %
[2]	validation_0-merror:0.108317	validation_1-merror:0.113192	validation_0-target_score:-0.692028	validation_1-target_score:-0.674007
target score = 69.61 %
accuracy (just for info) = 89.30 %
target score = 67.79 %
accuracy (just for info) = 88.88 %
[3]	validation_0-merror:0.107028	validation_1-merror:0.111291	validation_0-target_score:-0.696061	validation_1-target_score:-0.67788
target score = 69.88 %
accuracy (just for info) = 89.50 %
target score = 68.25 %
accuracy (just for info) = 89.16 %
[4]	validation_0-merror:0.105037	validation_1-merror:0.10844	validation_0-target_score:-0.698751	validation_1-target_score:-0.682527
target score = 69.85 %
accuracy (just for info) = 89.45 %
target score = 68.53 %
accuracy (just for info) = 89.08 %
[5]	validation_0-merror:0.105514	validation_1-merror:0.109167	validation_0-target_score:-0.698527	validation_1-target_score:-0.685294
target score = 69.76 %
accuracy (just for info) = 89.43 %
target score = 68.34 %
accuracy (just for info) = 89.04 %
[6]	validation_0-merror:0.10571	validation_1-merror:0.109614	validation_0-target_score:-0.69763	validation_1-target_score:-0.683413
target score = 70.37 %
accuracy (just for info) = 89.49 %
target score = 68.26 %
accuracy (just for info) = 89.02 %
[7]	validation_0-merror:0.105234	validation_1-merror:0.109838	validation_0-target_score:-0.703681	validation_1-target_score:-0.682638
target score = 70.06 %
accuracy (just for info) = 89.58 %
target score = 68.54 %
accuracy (just for info) = 89.11 %
[8]	validation_0-merror:0.104168	validation_1-merror:0.108888	validation_0-target_score:-0.700599	validation_1-target_score:-0.685404
target score = 70.28 %
accuracy (just for info) = 89.74 %
target score = 69.04 %
accuracy (just for info) = 89.31 %
[9]	validation_0-merror:0.102599	validation_1-merror:0.106875	validation_0-target_score:-0.70284	validation_1-target_score:-0.690384
target score = 70.72 %
accuracy (just for info) = 89.87 %
target score = 68.94 %
accuracy (just for info) = 89.30 %
[10]	validation_0-merror:0.101393	validation_1-merror:0.106987	validation_0-target_score:-0.707154	validation_1-target_score:-0.689388
target score = 71.00 %
accuracy (just for info) = 89.89 %
target score = 69.44 %
accuracy (just for info) = 89.35 %
[11]	validation_0-merror:0.101141	validation_1-merror:0.10654	validation_0-target_score:-0.709956	validation_1-target_score:-0.694368
target score = 71.09 %
accuracy (just for info) = 89.94 %
target score = 69.35 %
accuracy (just for info) = 89.37 %
[12]	validation_0-merror:0.10072	validation_1-merror:0.10626	validation_0-target_score:-0.710908	validation_1-target_score:-0.693482
target score = 71.67 %
accuracy (just for info) = 90.05 %
target score = 69.56 %
accuracy (just for info) = 89.42 %
[13]	validation_0-merror:0.099543	validation_1-merror:0.105869	validation_0-target_score:-0.716735	validation_1-target_score:-0.695585
target score = 72.16 %
accuracy (just for info) = 89.96 %
target score = 69.82 %
accuracy (just for info) = 89.47 %
[14]	validation_0-merror:0.100412	validation_1-merror:0.10531	validation_0-target_score:-0.721553	validation_1-target_score:-0.698241
target score = 72.28 %
accuracy (just for info) = 90.00 %
target score = 69.95 %
accuracy (just for info) = 89.55 %
[15]	validation_0-merror:0.100076	validation_1-merror:0.104472	validation_0-target_score:-0.722786	validation_1-target_score:-0.699458
target score = 72.46 %
accuracy (just for info) = 90.09 %
target score = 70.21 %
accuracy (just for info) = 89.58 %
[16]	validation_0-merror:0.099123	validation_1-merror:0.104192	validation_0-target_score:-0.724634	validation_1-target_score:-0.702114
target score = 72.63 %
accuracy (just for info) = 90.17 %
target score = 70.09 %
accuracy (just for info) = 89.61 %
[17]	validation_0-merror:0.098338	validation_1-merror:0.103857	validation_0-target_score:-0.726259	validation_1-target_score:-0.700896
target score = 72.84 %
accuracy (just for info) = 90.16 %
target score = 70.28 %
accuracy (just for info) = 89.62 %
[18]	validation_0-merror:0.098422	validation_1-merror:0.103801	validation_0-target_score:-0.728444	validation_1-target_score:-0.702777
target score = 72.94 %
accuracy (just for info) = 90.14 %
target score = 70.34 %
accuracy (just for info) = 89.61 %
[19]	validation_0-merror:0.098618	validation_1-merror:0.103913	validation_0-target_score:-0.729397	validation_1-target_score:-0.703441
target score = 73.23 %
accuracy (just for info) = 90.21 %
target score = 70.22 %
accuracy (just for info) = 89.66 %
[20]	validation_0-merror:0.097945	validation_1-merror:0.103354	validation_0-target_score:-0.73231	validation_1-target_score:-0.702224
target score = 73.28 %
accuracy (just for info) = 90.19 %
target score = 70.40 %
accuracy (just for info) = 89.61 %
[21]	validation_0-merror:0.098113	validation_1-merror:0.103913	validation_0-target_score:-0.732814	validation_1-target_score:-0.703995
target score = 73.52 %
accuracy (just for info) = 90.21 %
target score = 70.59 %
accuracy (just for info) = 89.68 %
[22]	validation_0-merror:0.097917	validation_1-merror:0.103242	validation_0-target_score:-0.735167	validation_1-target_score:-0.705876
target score = 73.61 %
accuracy (just for info) = 90.22 %
target score = 70.84 %
accuracy (just for info) = 89.63 %
[23]	validation_0-merror:0.097889	validation_1-merror:0.103689	validation_0-target_score:-0.736064	validation_1-target_score:-0.708421
target score = 73.65 %
accuracy (just for info) = 90.23 %
target score = 70.85 %
accuracy (just for info) = 89.68 %
[24]	validation_0-merror:0.097749	validation_1-merror:0.103186	validation_0-target_score:-0.736456	validation_1-target_score:-0.708532
target score = 73.76 %
accuracy (just for info) = 90.24 %
target score = 70.97 %
accuracy (just for info) = 89.69 %
[25]	validation_0-merror:0.097637	validation_1-merror:0.10313	validation_0-target_score:-0.737576	validation_1-target_score:-0.709749
target score = 73.96 %
accuracy (just for info) = 90.28 %
target score = 71.13 %
accuracy (just for info) = 89.80 %
[26]	validation_0-merror:0.0973	validation_1-merror:0.10218	validation_0-target_score:-0.739593	validation_1-target_score:-0.711298
target score = 74.03 %
accuracy (just for info) = 90.30 %
target score = 71.22 %
accuracy (just for info) = 89.79 %
[27]	validation_0-merror:0.097104	validation_1-merror:0.102571	validation_0-target_score:-0.740322	validation_1-target_score:-0.712183
target score = 74.00 %
accuracy (just for info) = 90.31 %
target score = 71.31 %
accuracy (just for info) = 89.82 %
[28]	validation_0-merror:0.096964	validation_1-merror:0.102124	validation_0-target_score:-0.739985	validation_1-target_score:-0.713068
target score = 74.26 %
accuracy (just for info) = 90.30 %
target score = 71.35 %
accuracy (just for info) = 89.83 %
[29]	validation_0-merror:0.09702	validation_1-merror:0.101677	validation_0-target_score:-0.742619	validation_1-target_score:-0.713511
target score = 74.38 %
accuracy (just for info) = 90.33 %
target score = 71.59 %
accuracy (just for info) = 89.84 %
[30]	validation_0-merror:0.096684	validation_1-merror:0.101621	validation_0-target_score:-0.743795	validation_1-target_score:-0.715946
target score = 74.60 %
accuracy (just for info) = 90.39 %
target score = 71.69 %
accuracy (just for info) = 89.84 %
[31]	validation_0-merror:0.096095	validation_1-merror:0.101621	validation_0-target_score:-0.746036	validation_1-target_score:-0.716941
target score = 74.63 %
accuracy (just for info) = 90.40 %
target score = 71.67 %
accuracy (just for info) = 89.89 %
[32]	validation_0-merror:0.095955	validation_1-merror:0.101118	validation_0-target_score:-0.746316	validation_1-target_score:-0.71672
target score = 74.78 %
accuracy (just for info) = 90.44 %
target score = 71.66 %
accuracy (just for info) = 89.89 %
[33]	validation_0-merror:0.095591	validation_1-merror:0.101062	validation_0-target_score:-0.747773	validation_1-target_score:-0.716609
target score = 74.73 %
accuracy (just for info) = 90.46 %
target score = 71.78 %
accuracy (just for info) = 89.89 %
[34]	validation_0-merror:0.095394	validation_1-merror:0.101174	validation_0-target_score:-0.747269	validation_1-target_score:-0.717827
target score = 74.92 %
accuracy (just for info) = 90.46 %
target score = 71.71 %
accuracy (just for info) = 89.96 %
[35]	validation_0-merror:0.09545	validation_1-merror:0.100391	validation_0-target_score:-0.749174	validation_1-target_score:-0.717052
target score = 75.10 %
accuracy (just for info) = 90.48 %
target score = 71.85 %
accuracy (just for info) = 89.93 %
[36]	validation_0-merror:0.095198	validation_1-merror:0.100783	validation_0-target_score:-0.751022	validation_1-target_score:-0.718491
target score = 75.23 %
accuracy (just for info) = 90.52 %
target score = 71.96 %
accuracy (just for info) = 89.96 %
[37]	validation_0-merror:0.094806	validation_1-merror:0.100447	validation_0-target_score:-0.752311	validation_1-target_score:-0.719597
target score = 75.27 %
accuracy (just for info) = 90.54 %
target score = 72.02 %
accuracy (just for info) = 89.96 %
[38]	validation_0-merror:0.094581	validation_1-merror:0.100447	validation_0-target_score:-0.752703	validation_1-target_score:-0.72015
target score = 75.42 %
accuracy (just for info) = 90.53 %
target score = 72.04 %
accuracy (just for info) = 89.94 %
[39]	validation_0-merror:0.094721	validation_1-merror:0.100615	validation_0-target_score:-0.754216	validation_1-target_score:-0.720372
target score = 75.39 %
accuracy (just for info) = 90.54 %
target score = 72.13 %
accuracy (just for info) = 89.94 %
[40]	validation_0-merror:0.094665	validation_1-merror:0.100559	validation_0-target_score:-0.753936	validation_1-target_score:-0.721257
target score = 75.34 %
accuracy (just for info) = 90.55 %
target score = 72.19 %
accuracy (just for info) = 89.97 %
[41]	validation_0-merror:0.094609	validation_1-merror:0.100335	validation_0-target_score:-0.753432	validation_1-target_score:-0.721921
target score = 75.34 %
accuracy (just for info) = 90.57 %
target score = 72.16 %
accuracy (just for info) = 89.95 %
[42]	validation_0-merror:0.094413	validation_1-merror:0.100559	validation_0-target_score:-0.753376	validation_1-target_score:-0.721589
target score = 75.38 %
accuracy (just for info) = 90.56 %
target score = 72.27 %
accuracy (just for info) = 89.97 %
[43]	validation_0-merror:0.094385	validation_1-merror:0.100335	validation_0-target_score:-0.753824	validation_1-target_score:-0.722696
target score = 75.40 %
accuracy (just for info) = 90.60 %
target score = 72.26 %
accuracy (just for info) = 89.96 %
[44]	validation_0-merror:0.094105	validation_1-merror:0.100391	validation_0-target_score:-0.754048	validation_1-target_score:-0.722585
target score = 75.45 %
accuracy (just for info) = 90.61 %
target score = 72.35 %
accuracy (just for info) = 89.99 %
[45]	validation_0-merror:0.093993	validation_1-merror:0.100056	validation_0-target_score:-0.754496	validation_1-target_score:-0.72347
target score = 75.46 %
accuracy (just for info) = 90.61 %
target score = 72.44 %
accuracy (just for info) = 89.99 %
[46]	validation_0-merror:0.093965	validation_1-merror:0.100056	validation_0-target_score:-0.754608	validation_1-target_score:-0.724355
target score = 75.50 %
accuracy (just for info) = 90.66 %
target score = 72.25 %
accuracy (just for info) = 90.02 %
[47]	validation_0-merror:0.09346	validation_1-merror:0.099832	validation_0-target_score:-0.755	validation_1-target_score:-0.722474
target score = 75.62 %
accuracy (just for info) = 90.67 %
target score = 72.46 %
accuracy (just for info) = 90.00 %
[48]	validation_0-merror:0.093376	validation_1-merror:0.1	validation_0-target_score:-0.756177	validation_1-target_score:-0.724577
target score = 75.71 %
accuracy (just for info) = 90.68 %
target score = 72.52 %
accuracy (just for info) = 90.01 %
[49]	validation_0-merror:0.093292	validation_1-merror:0.099888	validation_0-target_score:-0.757073	validation_1-target_score:-0.725241
target score = 75.79 %
accuracy (just for info) = 90.69 %
target score = 72.58 %
accuracy (just for info) = 90.02 %
[50]	validation_0-merror:0.093208	validation_1-merror:0.099832	validation_0-target_score:-0.757914	validation_1-target_score:-0.725794
target score = 75.78 %
accuracy (just for info) = 90.68 %
target score = 72.58 %
accuracy (just for info) = 90.06 %
[51]	validation_0-merror:0.093264	validation_1-merror:0.099497	validation_0-target_score:-0.757802	validation_1-target_score:-0.725794
target score = 75.82 %
accuracy (just for info) = 90.70 %
target score = 72.57 %
accuracy (just for info) = 90.04 %
[52]	validation_0-merror:0.093068	validation_1-merror:0.099609	validation_0-target_score:-0.75825	validation_1-target_score:-0.725683
target score = 75.92 %
accuracy (just for info) = 90.71 %
target score = 72.60 %
accuracy (just for info) = 90.04 %
[53]	validation_0-merror:0.093012	validation_1-merror:0.099553	validation_0-target_score:-0.759202	validation_1-target_score:-0.726015
target score = 75.98 %
accuracy (just for info) = 90.72 %
target score = 72.62 %
accuracy (just for info) = 90.04 %
[54]	validation_0-merror:0.092871	validation_1-merror:0.099553	validation_0-target_score:-0.759818	validation_1-target_score:-0.726237
target score = 75.99 %
accuracy (just for info) = 90.71 %
target score = 72.72 %
accuracy (just for info) = 90.05 %
[55]	validation_0-merror:0.092983	validation_1-merror:0.099497	validation_0-target_score:-0.759931	validation_1-target_score:-0.727232
target score = 76.09 %
accuracy (just for info) = 90.74 %
target score = 72.89 %
accuracy (just for info) = 90.07 %
[56]	validation_0-merror:0.092759	validation_1-merror:0.099273	validation_0-target_score:-0.760939	validation_1-target_score:-0.728892
target score = 76.13 %
accuracy (just for info) = 90.73 %
target score = 72.89 %
accuracy (just for info) = 90.09 %
[57]	validation_0-merror:0.092815	validation_1-merror:0.099106	validation_0-target_score:-0.761275	validation_1-target_score:-0.728892
target score = 76.21 %
accuracy (just for info) = 90.75 %
target score = 72.94 %
accuracy (just for info) = 90.07 %
[58]	validation_0-merror:0.092591	validation_1-merror:0.099329	validation_0-target_score:-0.762059	validation_1-target_score:-0.729446
target score = 76.28 %
accuracy (just for info) = 90.76 %
target score = 72.82 %
accuracy (just for info) = 90.09 %
[59]	validation_0-merror:0.092479	validation_1-merror:0.099106	validation_0-target_score:-0.762788	validation_1-target_score:-0.728228
target score = 76.37 %
accuracy (just for info) = 90.77 %
target score = 72.78 %
accuracy (just for info) = 90.08 %
[60]	validation_0-merror:0.092451	validation_1-merror:0.099162	validation_0-target_score:-0.76374	validation_1-target_score:-0.727786
target score = 76.34 %
accuracy (just for info) = 90.79 %
target score = 72.82 %
accuracy (just for info) = 90.07 %
[61]	validation_0-merror:0.092255	validation_1-merror:0.099329	validation_0-target_score:-0.763404	validation_1-target_score:-0.728228
target score = 76.45 %
accuracy (just for info) = 90.80 %
target score = 72.89 %
accuracy (just for info) = 90.10 %
[62]	validation_0-merror:0.092114	validation_1-merror:0.09905	validation_0-target_score:-0.764525	validation_1-target_score:-0.728892
target score = 76.49 %
accuracy (just for info) = 90.85 %
target score = 73.04 %
accuracy (just for info) = 90.12 %
[63]	validation_0-merror:0.091638	validation_1-merror:0.09877	validation_0-target_score:-0.764917	validation_1-target_score:-0.730442
target score = 76.55 %
accuracy (just for info) = 90.87 %
target score = 72.93 %
accuracy (just for info) = 90.15 %
[64]	validation_0-merror:0.091442	validation_1-merror:0.098547	validation_0-target_score:-0.765533	validation_1-target_score:-0.729335
target score = 76.64 %
accuracy (just for info) = 90.88 %
target score = 72.96 %
accuracy (just for info) = 90.11 %
[65]	validation_0-merror:0.09133	validation_1-merror:0.098882	validation_0-target_score:-0.766373	validation_1-target_score:-0.729556
target score = 76.74 %
accuracy (just for info) = 90.88 %
target score = 72.89 %
accuracy (just for info) = 90.13 %
[66]	validation_0-merror:0.091386	validation_1-merror:0.098658	validation_0-target_score:-0.767438	validation_1-target_score:-0.728892
target score = 76.81 %
accuracy (just for info) = 90.90 %
target score = 73.06 %
accuracy (just for info) = 90.16 %
[67]	validation_0-merror:0.091217	validation_1-merror:0.098379	validation_0-target_score:-0.76811	validation_1-target_score:-0.730552
target score = 76.90 %
accuracy (just for info) = 90.90 %
target score = 73.07 %
accuracy (just for info) = 90.15 %
[68]	validation_0-merror:0.091161	validation_1-merror:0.098491	validation_0-target_score:-0.768951	validation_1-target_score:-0.730663
target score = 76.95 %
accuracy (just for info) = 90.90 %
target score = 73.06 %
accuracy (just for info) = 90.17 %
[69]	validation_0-merror:0.091161	validation_1-merror:0.098323	validation_0-target_score:-0.769511	validation_1-target_score:-0.730552
target score = 76.94 %
accuracy (just for info) = 90.92 %
target score = 73.01 %
accuracy (just for info) = 90.17 %
[70]	validation_0-merror:0.090909	validation_1-merror:0.098267	validation_0-target_score:-0.769399	validation_1-target_score:-0.73011
target score = 77.00 %
accuracy (just for info) = 90.92 %
target score = 73.09 %
accuracy (just for info) = 90.15 %
[71]	validation_0-merror:0.090881	validation_1-merror:0.098547	validation_0-target_score:-0.770015	validation_1-target_score:-0.730884
target score = 77.10 %
accuracy (just for info) = 90.97 %
target score = 73.20 %
accuracy (just for info) = 90.16 %
[72]	validation_0-merror:0.090376	validation_1-merror:0.098435	validation_0-target_score:-0.771024	validation_1-target_score:-0.731991
target score = 77.11 %
accuracy (just for info) = 91.01 %
target score = 73.28 %
accuracy (just for info) = 90.15 %
[73]	validation_0-merror:0.090012	validation_1-merror:0.098491	validation_0-target_score:-0.771136	validation_1-target_score:-0.732765
target score = 77.14 %
accuracy (just for info) = 91.04 %
target score = 73.32 %
accuracy (just for info) = 90.16 %
[74]	validation_0-merror:0.089704	validation_1-merror:0.098435	validation_0-target_score:-0.771416	validation_1-target_score:-0.733208
target score = 77.16 %
accuracy (just for info) = 91.05 %
target score = 73.30 %
accuracy (just for info) = 90.18 %
[75]	validation_0-merror:0.08962	validation_1-merror:0.098211	validation_0-target_score:-0.77164	validation_1-target_score:-0.732987
target score = 77.31 %
accuracy (just for info) = 91.08 %
target score = 73.31 %
accuracy (just for info) = 90.19 %
[76]	validation_0-merror:0.089367	validation_1-merror:0.098099	validation_0-target_score:-0.773097	validation_1-target_score:-0.733097
target score = 77.46 %
accuracy (just for info) = 91.09 %
target score = 73.22 %
accuracy (just for info) = 90.17 %
[77]	validation_0-merror:0.089339	validation_1-merror:0.098267	validation_0-target_score:-0.774609	validation_1-target_score:-0.732212
target score = 77.46 %
accuracy (just for info) = 91.11 %
target score = 73.27 %
accuracy (just for info) = 90.15 %
[78]	validation_0-merror:0.089115	validation_1-merror:0.098491	validation_0-target_score:-0.774609	validation_1-target_score:-0.732655
target score = 77.59 %
accuracy (just for info) = 91.14 %
target score = 73.34 %
accuracy (just for info) = 90.17 %
[79]	validation_0-merror:0.088751	validation_1-merror:0.098267	validation_0-target_score:-0.775898	validation_1-target_score:-0.733429
target score = 77.61 %
accuracy (just for info) = 91.19 %
target score = 73.31 %
accuracy (just for info) = 90.18 %
[80]	validation_0-merror:0.088386	validation_1-merror:0.098211	validation_0-target_score:-0.776122	validation_1-target_score:-0.733097
target score = 77.67 %
accuracy (just for info) = 91.18 %
target score = 73.24 %
accuracy (just for info) = 90.17 %
[81]	validation_0-merror:0.088386	validation_1-merror:0.098267	validation_0-target_score:-0.776738	validation_1-target_score:-0.732433
target score = 77.76 %
accuracy (just for info) = 91.20 %
target score = 73.30 %
accuracy (just for info) = 90.20 %
[82]	validation_0-merror:0.088134	validation_1-merror:0.098044	validation_0-target_score:-0.777579	validation_1-target_score:-0.732987
target score = 77.81 %
accuracy (just for info) = 91.23 %
target score = 73.33 %
accuracy (just for info) = 90.19 %
[83]	validation_0-merror:0.087854	validation_1-merror:0.098099	validation_0-target_score:-0.778083	validation_1-target_score:-0.733319
target score = 77.89 %
accuracy (just for info) = 91.24 %
target score = 73.30 %
accuracy (just for info) = 90.22 %
[84]	validation_0-merror:0.087741	validation_1-merror:0.097764	validation_0-target_score:-0.778923	validation_1-target_score:-0.732987
target score = 77.88 %
accuracy (just for info) = 91.25 %
target score = 73.44 %
accuracy (just for info) = 90.23 %
[85]	validation_0-merror:0.087685	validation_1-merror:0.097652	validation_0-target_score:-0.778755	validation_1-target_score:-0.734425
target score = 77.93 %
accuracy (just for info) = 91.26 %
target score = 73.54 %
accuracy (just for info) = 90.20 %
[86]	validation_0-merror:0.087573	validation_1-merror:0.097988	validation_0-target_score:-0.779315	validation_1-target_score:-0.735421
target score = 78.03 %
accuracy (just for info) = 91.25 %
target score = 73.53 %
accuracy (just for info) = 90.21 %
[87]	validation_0-merror:0.087657	validation_1-merror:0.097876	validation_0-target_score:-0.780268	validation_1-target_score:-0.73531
target score = 78.16 %
accuracy (just for info) = 91.31 %
target score = 73.67 %
accuracy (just for info) = 90.23 %
[88]	validation_0-merror:0.087041	validation_1-merror:0.097708	validation_0-target_score:-0.781612	validation_1-target_score:-0.736749
target score = 78.20 %
accuracy (just for info) = 91.33 %
target score = 73.64 %
accuracy (just for info) = 90.23 %
[89]	validation_0-merror:0.086844	validation_1-merror:0.097708	validation_0-target_score:-0.782005	validation_1-target_score:-0.736417
target score = 78.26 %
accuracy (just for info) = 91.34 %
target score = 73.49 %
accuracy (just for info) = 90.22 %
[90]	validation_0-merror:0.086816	validation_1-merror:0.09782	validation_0-target_score:-0.782621	validation_1-target_score:-0.734868
target score = 78.41 %
accuracy (just for info) = 91.35 %
target score = 73.51 %
accuracy (just for info) = 90.22 %
[91]	validation_0-merror:0.086648	validation_1-merror:0.097764	validation_0-target_score:-0.784078	validation_1-target_score:-0.735089
target score = 78.43 %
accuracy (just for info) = 91.35 %
target score = 73.52 %
accuracy (just for info) = 90.23 %
[92]	validation_0-merror:0.086592	validation_1-merror:0.097708	validation_0-target_score:-0.784302	validation_1-target_score:-0.7352
target score = 78.41 %
accuracy (just for info) = 91.37 %
target score = 73.67 %
accuracy (just for info) = 90.24 %
[93]	validation_0-merror:0.08648	validation_1-merror:0.097652	validation_0-target_score:-0.784078	validation_1-target_score:-0.736749
target score = 78.53 %
accuracy (just for info) = 91.36 %
target score = 73.74 %
accuracy (just for info) = 90.22 %
[94]	validation_0-merror:0.086536	validation_1-merror:0.097764	validation_0-target_score:-0.785254	validation_1-target_score:-0.737413
target score = 78.61 %
accuracy (just for info) = 91.37 %
target score = 73.79 %
accuracy (just for info) = 90.22 %
[95]	validation_0-merror:0.086368	validation_1-merror:0.097764	validation_0-target_score:-0.786094	validation_1-target_score:-0.737855
target score = 78.65 %
accuracy (just for info) = 91.37 %
target score = 73.85 %
accuracy (just for info) = 90.26 %
[96]	validation_0-merror:0.086508	validation_1-merror:0.097429	validation_0-target_score:-0.786487	validation_1-target_score:-0.738519
target score = 78.77 %
accuracy (just for info) = 91.39 %
target score = 73.71 %
accuracy (just for info) = 90.25 %
[97]	validation_0-merror:0.086256	validation_1-merror:0.097541	validation_0-target_score:-0.787719	validation_1-target_score:-0.737081
target score = 78.81 %
accuracy (just for info) = 91.42 %
target score = 73.61 %
accuracy (just for info) = 90.28 %
[98]	validation_0-merror:0.086003	validation_1-merror:0.097205	validation_0-target_score:-0.788111	validation_1-target_score:-0.736085
target score = 78.92 %
accuracy (just for info) = 91.42 %
target score = 73.73 %
accuracy (just for info) = 90.27 %
[99]	validation_0-merror:0.085975	validation_1-merror:0.097317	validation_0-target_score:-0.789176	validation_1-target_score:-0.737302
target score = 79.14 %
accuracy (just for info) = 91.43 %
target score = 73.67 %
accuracy (just for info) = 90.28 %
[100]	validation_0-merror:0.085835	validation_1-merror:0.097373	validation_0-target_score:-0.791417	validation_1-target_score:-0.736749
target score = 79.28 %
accuracy (just for info) = 91.45 %
target score = 73.70 %
accuracy (just for info) = 90.26 %
[101]	validation_0-merror:0.085639	validation_1-merror:0.097596	validation_0-target_score:-0.792761	validation_1-target_score:-0.73697
target score = 79.38 %
accuracy (just for info) = 91.46 %
target score = 73.72 %
accuracy (just for info) = 90.25 %
[102]	validation_0-merror:0.085555	validation_1-merror:0.097652	validation_0-target_score:-0.793826	validation_1-target_score:-0.737192
target score = 79.41 %
accuracy (just for info) = 91.48 %
target score = 73.67 %
accuracy (just for info) = 90.27 %
[103]	validation_0-merror:0.085331	validation_1-merror:0.097373	validation_0-target_score:-0.79405	validation_1-target_score:-0.736749
target score = 79.38 %
accuracy (just for info) = 91.49 %
target score = 73.70 %
accuracy (just for info) = 90.23 %
[104]	validation_0-merror:0.085219	validation_1-merror:0.097708	validation_0-target_score:-0.793826	validation_1-target_score:-0.73697
target score = 79.44 %
accuracy (just for info) = 91.51 %
target score = 73.75 %
accuracy (just for info) = 90.25 %
[105]	validation_0-merror:0.085134	validation_1-merror:0.097596	validation_0-target_score:-0.794386	validation_1-target_score:-0.737524
target score = 79.42 %
accuracy (just for info) = 91.51 %
target score = 73.80 %
accuracy (just for info) = 90.23 %
[106]	validation_0-merror:0.08505	validation_1-merror:0.097876	validation_0-target_score:-0.794162	validation_1-target_score:-0.737966
target score = 79.57 %
accuracy (just for info) = 91.54 %
target score = 73.84 %
accuracy (just for info) = 90.25 %
[107]	validation_0-merror:0.08477	validation_1-merror:0.097652	validation_0-target_score:-0.795731	validation_1-target_score:-0.738409
target score = 79.71 %
accuracy (just for info) = 91.55 %
target score = 73.92 %
accuracy (just for info) = 90.25 %
[108]	validation_0-merror:0.084714	validation_1-merror:0.097485	validation_0-target_score:-0.797131	validation_1-target_score:-0.739183
target score = 79.83 %
accuracy (just for info) = 91.57 %
target score = 73.77 %
accuracy (just for info) = 90.27 %
[109]	validation_0-merror:0.08449	validation_1-merror:0.097541	validation_0-target_score:-0.798252	validation_1-target_score:-0.737745
target score = 79.93 %
accuracy (just for info) = 91.61 %
target score = 73.86 %
accuracy (just for info) = 90.27 %
[110]	validation_0-merror:0.084069	validation_1-merror:0.097541	validation_0-target_score:-0.799316	validation_1-target_score:-0.73863
target score = 80.02 %
accuracy (just for info) = 91.60 %
target score = 73.91 %
accuracy (just for info) = 90.29 %
[111]	validation_0-merror:0.084125	validation_1-merror:0.097261	validation_0-target_score:-0.800213	validation_1-target_score:-0.739073
target score = 80.06 %
accuracy (just for info) = 91.61 %
target score = 73.80 %
accuracy (just for info) = 90.29 %
[112]	validation_0-merror:0.084069	validation_1-merror:0.097149	validation_0-target_score:-0.800605	validation_1-target_score:-0.737966
target score = 80.13 %
accuracy (just for info) = 91.63 %
target score = 73.76 %
accuracy (just for info) = 90.30 %
[113]	validation_0-merror:0.083845	validation_1-merror:0.097037	validation_0-target_score:-0.801333	validation_1-target_score:-0.737634
target score = 80.14 %
accuracy (just for info) = 91.64 %
target score = 73.75 %
accuracy (just for info) = 90.31 %
[114]	validation_0-merror:0.083761	validation_1-merror:0.096982	validation_0-target_score:-0.801389	validation_1-target_score:-0.737524
target score = 80.14 %
accuracy (just for info) = 91.63 %
target score = 73.75 %
accuracy (just for info) = 90.31 %
[115]	validation_0-merror:0.083845	validation_1-merror:0.096926	validation_0-target_score:-0.801445	validation_1-target_score:-0.737524
target score = 80.29 %
accuracy (just for info) = 91.67 %
target score = 73.81 %
accuracy (just for info) = 90.32 %
[116]	validation_0-merror:0.083424	validation_1-merror:0.096814	validation_0-target_score:-0.802902	validation_1-target_score:-0.738077
target score = 80.27 %
accuracy (just for info) = 91.67 %
target score = 73.77 %
accuracy (just for info) = 90.32 %
[117]	validation_0-merror:0.083424	validation_1-merror:0.096758	validation_0-target_score:-0.802678	validation_1-target_score:-0.737745
target score = 80.32 %
accuracy (just for info) = 91.71 %
target score = 73.71 %
accuracy (just for info) = 90.30 %
[118]	validation_0-merror:0.083032	validation_1-merror:0.096982	validation_0-target_score:-0.803238	validation_1-target_score:-0.737081
target score = 80.36 %
accuracy (just for info) = 91.74 %
target score = 73.76 %
accuracy (just for info) = 90.31 %
[119]	validation_0-merror:0.082696	validation_1-merror:0.09687	validation_0-target_score:-0.803574	validation_1-target_score:-0.737634
target score = 80.38 %
accuracy (just for info) = 91.77 %
target score = 73.92 %
accuracy (just for info) = 90.31 %
[120]	validation_0-merror:0.082415	validation_1-merror:0.096982	validation_0-target_score:-0.803799	validation_1-target_score:-0.739183
target score = 80.45 %
accuracy (just for info) = 91.78 %
target score = 73.89 %
accuracy (just for info) = 90.32 %
[121]	validation_0-merror:0.082331	validation_1-merror:0.09687	validation_0-target_score:-0.804527	validation_1-target_score:-0.738851
target score = 80.45 %
accuracy (just for info) = 91.79 %
target score = 73.92 %
accuracy (just for info) = 90.31 %
[122]	validation_0-merror:0.082219	validation_1-merror:0.096926	validation_0-target_score:-0.804471	validation_1-target_score:-0.739183
target score = 80.50 %
accuracy (just for info) = 91.79 %
target score = 73.89 %
accuracy (just for info) = 90.32 %
[123]	validation_0-merror:0.082219	validation_1-merror:0.096982	validation_0-target_score:-0.804975	validation_1-target_score:-0.738851
target score = 80.70 %
accuracy (just for info) = 91.80 %
target score = 73.85 %
accuracy (just for info) = 90.35 %
[124]	validation_0-merror:0.082163	validation_1-merror:0.09659	validation_0-target_score:-0.806992	validation_1-target_score:-0.738519
target score = 80.79 %
accuracy (just for info) = 91.82 %
target score = 73.74 %
accuracy (just for info) = 90.34 %
[125]	validation_0-merror:0.081967	validation_1-merror:0.096814	validation_0-target_score:-0.807888	validation_1-target_score:-0.737413
target score = 80.82 %
accuracy (just for info) = 91.81 %
target score = 73.80 %
accuracy (just for info) = 90.34 %
[126]	validation_0-merror:0.082023	validation_1-merror:0.096702	validation_0-target_score:-0.808225	validation_1-target_score:-0.737966
target score = 80.91 %
accuracy (just for info) = 91.83 %
target score = 73.82 %
accuracy (just for info) = 90.35 %
[127]	validation_0-merror:0.081855	validation_1-merror:0.09659	validation_0-target_score:-0.809065	validation_1-target_score:-0.738187
target score = 80.95 %
accuracy (just for info) = 91.85 %
target score = 73.85 %
accuracy (just for info) = 90.35 %
[128]	validation_0-merror:0.081602	validation_1-merror:0.09659	validation_0-target_score:-0.809457	validation_1-target_score:-0.738519
target score = 81.02 %
accuracy (just for info) = 91.87 %
target score = 73.80 %
accuracy (just for info) = 90.35 %
[129]	validation_0-merror:0.081406	validation_1-merror:0.096646	validation_0-target_score:-0.810185	validation_1-target_score:-0.737966
target score = 81.04 %
accuracy (just for info) = 91.87 %
target score = 73.80 %
accuracy (just for info) = 90.35 %
[130]	validation_0-merror:0.081406	validation_1-merror:0.09659	validation_0-target_score:-0.81041	validation_1-target_score:-0.737966
target score = 81.10 %
accuracy (just for info) = 91.86 %
target score = 73.81 %
accuracy (just for info) = 90.38 %
[131]	validation_0-merror:0.08149	validation_1-merror:0.096311	validation_0-target_score:-0.811026	validation_1-target_score:-0.738077
target score = 81.14 %
accuracy (just for info) = 91.88 %
target score = 73.85 %
accuracy (just for info) = 90.38 %
[132]	validation_0-merror:0.08135	validation_1-merror:0.096255	validation_0-target_score:-0.811362	validation_1-target_score:-0.738519
target score = 81.20 %
accuracy (just for info) = 91.89 %
target score = 73.85 %
accuracy (just for info) = 90.37 %
[133]	validation_0-merror:0.081182	validation_1-merror:0.096478	validation_0-target_score:-0.812034	validation_1-target_score:-0.738519
target score = 81.20 %
accuracy (just for info) = 91.90 %
target score = 73.90 %
accuracy (just for info) = 90.37 %
[134]	validation_0-merror:0.081154	validation_1-merror:0.096367	validation_0-target_score:-0.811978	validation_1-target_score:-0.738962
target score = 81.23 %
accuracy (just for info) = 91.90 %
target score = 73.86 %
accuracy (just for info) = 90.36 %
[135]	validation_0-merror:0.081042	validation_1-merror:0.096534	validation_0-target_score:-0.812314	validation_1-target_score:-0.73863
target score = 81.35 %
accuracy (just for info) = 91.94 %
target score = 73.75 %
accuracy (just for info) = 90.37 %
[136]	validation_0-merror:0.080958	validation_1-merror:0.096367	validation_0-target_score:-0.813491	validation_1-target_score:-0.737524
target score = 81.50 %
accuracy (just for info) = 91.95 %
target score = 73.63 %
accuracy (just for info) = 90.38 %
[137]	validation_0-merror:0.080761	validation_1-merror:0.096199	validation_0-target_score:-0.815004	validation_1-target_score:-0.736306
target score = 81.71 %
accuracy (just for info) = 91.98 %
target score = 73.56 %
accuracy (just for info) = 90.37 %
[138]	validation_0-merror:0.080341	validation_1-merror:0.096311	validation_0-target_score:-0.817077	validation_1-target_score:-0.735642
target score = 81.86 %
accuracy (just for info) = 92.02 %
target score = 73.64 %
accuracy (just for info) = 90.35 %
[139]	validation_0-merror:0.080004	validation_1-merror:0.096478	validation_0-target_score:-0.818645	validation_1-target_score:-0.736417
target score = 81.96 %
accuracy (just for info) = 92.02 %
target score = 73.72 %
accuracy (just for info) = 90.36 %
[140]	validation_0-merror:0.079892	validation_1-merror:0.096478	validation_0-target_score:-0.819598	validation_1-target_score:-0.737192
target score = 82.00 %
accuracy (just for info) = 92.03 %
target score = 73.65 %
accuracy (just for info) = 90.37 %
[141]	validation_0-merror:0.079836	validation_1-merror:0.096478	validation_0-target_score:-0.820046	validation_1-target_score:-0.736528
target score = 82.07 %
accuracy (just for info) = 92.05 %
target score = 73.61 %
accuracy (just for info) = 90.39 %
[142]	validation_0-merror:0.079556	validation_1-merror:0.096143	validation_0-target_score:-0.820662	validation_1-target_score:-0.736085
target score = 82.14 %
accuracy (just for info) = 92.06 %
target score = 73.55 %
accuracy (just for info) = 90.39 %
[143]	validation_0-merror:0.079472	validation_1-merror:0.096143	validation_0-target_score:-0.821391	validation_1-target_score:-0.735532
target score = 82.20 %
accuracy (just for info) = 92.08 %
target score = 73.59 %
accuracy (just for info) = 90.39 %
[144]	validation_0-merror:0.079388	validation_1-merror:0.096143	validation_0-target_score:-0.822007	validation_1-target_score:-0.735864
target score = 82.25 %
accuracy (just for info) = 92.08 %
target score = 73.74 %
accuracy (just for info) = 90.39 %
[145]	validation_0-merror:0.079248	validation_1-merror:0.096255	validation_0-target_score:-0.822511	validation_1-target_score:-0.737413
target score = 82.28 %
accuracy (just for info) = 92.12 %
target score = 73.82 %
accuracy (just for info) = 90.38 %
[146]	validation_0-merror:0.078939	validation_1-merror:0.096367	validation_0-target_score:-0.822791	validation_1-target_score:-0.738187
target score = 82.30 %
accuracy (just for info) = 92.13 %
target score = 73.84 %
accuracy (just for info) = 90.38 %
[147]	validation_0-merror:0.078743	validation_1-merror:0.096199	validation_0-target_score:-0.823015	validation_1-target_score:-0.738409
target score = 82.37 %
accuracy (just for info) = 92.14 %
target score = 73.83 %
accuracy (just for info) = 90.38 %
[148]	validation_0-merror:0.078659	validation_1-merror:0.096255	validation_0-target_score:-0.823744	validation_1-target_score:-0.738298
target score = 82.36 %
accuracy (just for info) = 92.16 %
target score = 73.76 %
accuracy (just for info) = 90.37 %
[149]	validation_0-merror:0.078435	validation_1-merror:0.096367	validation_0-target_score:-0.823632	validation_1-target_score:-0.737634
target score = 82.42 %
accuracy (just for info) = 92.16 %
target score = 73.81 %
accuracy (just for info) = 90.39 %
[150]	validation_0-merror:0.078407	validation_1-merror:0.096199	validation_0-target_score:-0.824192	validation_1-target_score:-0.738077
target score = 82.50 %
accuracy (just for info) = 92.16 %
target score = 73.76 %
accuracy (just for info) = 90.35 %
[151]	validation_0-merror:0.078407	validation_1-merror:0.09659	validation_0-target_score:-0.824976	validation_1-target_score:-0.737634
target score = 82.57 %
accuracy (just for info) = 92.18 %
target score = 73.74 %
accuracy (just for info) = 90.35 %
[152]	validation_0-merror:0.078295	validation_1-merror:0.096702	validation_0-target_score:-0.825705	validation_1-target_score:-0.737413
target score = 82.65 %
accuracy (just for info) = 92.19 %
target score = 73.84 %
accuracy (just for info) = 90.36 %
[153]	validation_0-merror:0.078154	validation_1-merror:0.096702	validation_0-target_score:-0.826545	validation_1-target_score:-0.738409
target score = 82.68 %
accuracy (just for info) = 92.19 %
target score = 73.84 %
accuracy (just for info) = 90.36 %
[154]	validation_0-merror:0.078154	validation_1-merror:0.096423	validation_0-target_score:-0.826825	validation_1-target_score:-0.738409
target score = 82.74 %
accuracy (just for info) = 92.20 %
target score = 73.73 %
accuracy (just for info) = 90.38 %
[155]	validation_0-merror:0.07807	validation_1-merror:0.096255	validation_0-target_score:-0.827385	validation_1-target_score:-0.737302
target score = 82.74 %
accuracy (just for info) = 92.22 %
target score = 73.80 %
accuracy (just for info) = 90.37 %
[156]	validation_0-merror:0.077818	validation_1-merror:0.096367	validation_0-target_score:-0.827441	validation_1-target_score:-0.737966
target score = 82.79 %
accuracy (just for info) = 92.24 %
target score = 73.79 %
accuracy (just for info) = 90.38 %
[157]	validation_0-merror:0.077678	validation_1-merror:0.096199	validation_0-target_score:-0.827946	validation_1-target_score:-0.737855
target score = 82.79 %
accuracy (just for info) = 92.27 %
target score = 73.80 %
accuracy (just for info) = 90.38 %
[158]	validation_0-merror:0.077369	validation_1-merror:0.096255	validation_0-target_score:-0.827946	validation_1-target_score:-0.737966
Stopping. Best iteration:
[108]	validation_0-merror:0.084714	validation_1-merror:0.097485	validation_0-target_score:-0.797131	validation_1-target_score:-0.739183
[0, 1, 2]
0.23898305688
35672 17891
['a_bi_freq_suff' 'a_three_freq_suff' 'a_four_freq_suff' 'a_five_freq_suff'
 'a_bi_freq_pref' 'a_three_freq_pref' 'a_four_freq_pref' 'a_five_freq_pref'
 'a_sn_gram_freq' 'the_bi_freq_suff' 'the_three_freq_suff'
 'the_four_freq_suff' 'the_five_freq_suff' 'the_bi_freq_pref'
 'the_three_freq_pref' 'the_four_freq_pref' 'the_five_freq_pref'
 'the_sn_gram_freq' 'a_the_bi_freq_suff_p1' 'the_a_bi_freq_suff_p1'
 'a_the_three_freq_suff_p1' 'the_a_three_freq_suff_p1'
 'a_the_four_freq_suff_p1' 'the_a_four_freq_suff_p1'
 'a_the_five_freq_suff_p1' 'the_a_five_freq_suff_p1'
 'a_the_bi_freq_pref_p1' 'the_a_bi_freq_pref_p1' 'a_the_three_freq_pref_p1'
 'the_a_three_freq_pref_p1' 'a_the_four_freq_pref_p1'
 'the_a_four_freq_pref_p1' 'a_the_five_freq_pref_p1'
 'the_a_five_freq_pref_p1' 'a_the_sn_ngram_freq_p1'
 'the_a_sn_ngram_freq_p1' 'a_the_bi_freq_suff_p10' 'the_a_bi_freq_suff_p10'
 'a_the_three_freq_suff_p10' 'the_a_three_freq_suff_p10'
 'a_the_four_freq_suff_p10' 'the_a_four_freq_suff_p10'
 'a_the_five_freq_suff_p10' 'the_a_five_freq_suff_p10'
 'a_the_bi_freq_pref_p10' 'the_a_bi_freq_pref_p10'
 'a_the_three_freq_pref_p10' 'the_a_three_freq_pref_p10'
 'a_the_four_freq_pref_p10' 'the_a_four_freq_pref_p10'
 'a_the_five_freq_pref_p10' 'the_a_five_freq_pref_p10'
 'a_the_sn_ngram_freq_p10' 'the_a_sn_ngram_freq_p10'
 'a_the_bi_freq_suff_p100' 'the_a_bi_freq_suff_p100'
 'a_the_three_freq_suff_p100' 'the_a_three_freq_suff_p100'
 'a_the_four_freq_suff_p100' 'the_a_four_freq_suff_p100'
 'a_the_five_freq_suff_p100' 'the_a_five_freq_suff_p100'
 'a_the_bi_freq_pref_p100' 'the_a_bi_freq_pref_p100'
 'a_the_three_freq_pref_p100' 'the_a_three_freq_pref_p100'
 'a_the_four_freq_pref_p100' 'the_a_four_freq_pref_p100'
 'a_the_five_freq_pref_p100' 'the_a_five_freq_pref_p100'
 'a_the_sn_ngram_freq_p100' 'the_a_sn_ngram_freq_p100' 'st_with_v'
 'article']
target score = 68.72 %
accuracy (just for info) = 87.87 %
target score = 66.62 %
accuracy (just for info) = 87.73 %
[0]	validation_0-merror:0.12158	validation_1-merror:0.122967	validation_0-target_score:-0.687242	validation_1-target_score:-0.666182
Multiple eval metrics have been passed: 'validation_1-target_score' will be used for early stopping.
Will train until validation_1-target_score hasn't improved in 50 rounds.
target score = 69.00 %
accuracy (just for info) = 88.78 %
target score = 67.44 %
accuracy (just for info) = 88.47 %
[1]	validation_0-merror:0.112161	validation_1-merror:0.115365	validation_0-target_score:-0.690028	validation_1-target_score:-0.674351
target score = 69.11 %
accuracy (just for info) = 89.08 %
target score = 67.11 %
accuracy (just for info) = 88.59 %
[2]	validation_0-merror:0.112609	validation_1-merror:0.115253	validation_0-target_score:-0.691142	validation_1-target_score:-0.671106
target score = 69.28 %
accuracy (just for info) = 89.10 %
target score = 67.04 %
accuracy (just for info) = 88.58 %
[3]	validation_0-merror:0.109301	validation_1-merror:0.114247	validation_0-target_score:-0.692758	validation_1-target_score:-0.670434
target score = 69.00 %
accuracy (just for info) = 89.30 %
target score = 66.94 %
accuracy (just for info) = 88.71 %
[4]	validation_0-merror:0.107059	validation_1-merror:0.113074	validation_0-target_score:-0.690028	validation_1-target_score:-0.669427
target score = 68.97 %
accuracy (just for info) = 89.30 %
target score = 66.99 %
accuracy (just for info) = 88.75 %
[5]	validation_0-merror:0.107059	validation_1-merror:0.112571	validation_0-target_score:-0.689749	validation_1-target_score:-0.669875
target score = 69.38 %
accuracy (just for info) = 89.48 %
target score = 67.22 %
accuracy (just for info) = 88.91 %
[6]	validation_0-merror:0.105237	validation_1-merror:0.11095	validation_0-target_score:-0.69376	validation_1-target_score:-0.672225
target score = 69.84 %
accuracy (just for info) = 89.54 %
target score = 67.45 %
accuracy (just for info) = 88.93 %
[7]	validation_0-merror:0.104676	validation_1-merror:0.110726	validation_0-target_score:-0.698384	validation_1-target_score:-0.674463
target score = 70.16 %
accuracy (just for info) = 89.56 %
target score = 67.75 %
accuracy (just for info) = 88.89 %
[8]	validation_0-merror:0.10448	validation_1-merror:0.111061	validation_0-target_score:-0.70156	validation_1-target_score:-0.677484
target score = 70.23 %
accuracy (just for info) = 89.61 %
target score = 67.64 %
accuracy (just for info) = 88.99 %
[9]	validation_0-merror:0.103891	validation_1-merror:0.110111	validation_0-target_score:-0.70234	validation_1-target_score:-0.676365
target score = 70.94 %
accuracy (just for info) = 89.70 %
target score = 68.24 %
accuracy (just for info) = 89.12 %
[10]	validation_0-merror:0.102994	validation_1-merror:0.108882	validation_0-target_score:-0.709415	validation_1-target_score:-0.682408
target score = 70.89 %
accuracy (just for info) = 89.88 %
target score = 68.34 %
accuracy (just for info) = 89.25 %
[11]	validation_0-merror:0.101256	validation_1-merror:0.107596	validation_0-target_score:-0.708914	validation_1-target_score:-0.683415
target score = 71.30 %
accuracy (just for info) = 90.01 %
target score = 68.72 %
accuracy (just for info) = 89.26 %
[12]	validation_0-merror:0.099882	validation_1-merror:0.107484	validation_0-target_score:-0.713036	validation_1-target_score:-0.68722
target score = 71.77 %
accuracy (just for info) = 90.04 %
target score = 69.26 %
accuracy (just for info) = 89.35 %
[13]	validation_0-merror:0.09963	validation_1-merror:0.106646	validation_0-target_score:-0.71766	validation_1-target_score:-0.692592
target score = 71.70 %
accuracy (just for info) = 90.04 %
target score = 68.97 %
accuracy (just for info) = 89.38 %
[14]	validation_0-merror:0.099574	validation_1-merror:0.10631	validation_0-target_score:-0.717047	validation_1-target_score:-0.689682
target score = 71.84 %
accuracy (just for info) = 90.06 %
target score = 69.20 %
accuracy (just for info) = 89.37 %
[15]	validation_0-merror:0.099462	validation_1-merror:0.106422	validation_0-target_score:-0.718384	validation_1-target_score:-0.692032
target score = 72.07 %
accuracy (just for info) = 90.13 %
target score = 69.09 %
accuracy (just for info) = 89.41 %
[16]	validation_0-merror:0.098733	validation_1-merror:0.106031	validation_0-target_score:-0.720724	validation_1-target_score:-0.690913
target score = 72.43 %
accuracy (just for info) = 90.16 %
target score = 69.59 %
accuracy (just for info) = 89.47 %
[17]	validation_0-merror:0.098368	validation_1-merror:0.105416	validation_0-target_score:-0.724345	validation_1-target_score:-0.695949
target score = 72.43 %
accuracy (just for info) = 90.18 %
target score = 69.81 %
accuracy (just for info) = 89.53 %
[18]	validation_0-merror:0.0982	validation_1-merror:0.104857	validation_0-target_score:-0.72429	validation_1-target_score:-0.698075
target score = 72.72 %
accuracy (just for info) = 90.20 %
target score = 70.10 %
accuracy (just for info) = 89.55 %
[19]	validation_0-merror:0.097976	validation_1-merror:0.104578	validation_0-target_score:-0.727187	validation_1-target_score:-0.700985
target score = 72.94 %
accuracy (just for info) = 90.26 %
target score = 70.42 %
accuracy (just for info) = 89.56 %
[20]	validation_0-merror:0.097471	validation_1-merror:0.104578	validation_0-target_score:-0.729415	validation_1-target_score:-0.70423
target score = 72.97 %
accuracy (just for info) = 90.26 %
target score = 70.61 %
accuracy (just for info) = 89.60 %
[21]	validation_0-merror:0.097359	validation_1-merror:0.104131	validation_0-target_score:-0.729694	validation_1-target_score:-0.706132
target score = 73.22 %
accuracy (just for info) = 90.26 %
target score = 70.75 %
accuracy (just for info) = 89.61 %
[22]	validation_0-merror:0.097443	validation_1-merror:0.103907	validation_0-target_score:-0.732201	validation_1-target_score:-0.707475
target score = 73.41 %
accuracy (just for info) = 90.27 %
target score = 70.95 %
accuracy (just for info) = 89.65 %
[23]	validation_0-merror:0.097331	validation_1-merror:0.103516	validation_0-target_score:-0.734095	validation_1-target_score:-0.70949
target score = 73.69 %
accuracy (just for info) = 90.31 %
target score = 71.07 %
accuracy (just for info) = 89.69 %
[24]	validation_0-merror:0.096939	validation_1-merror:0.10318	validation_0-target_score:-0.73688	validation_1-target_score:-0.710721
target score = 73.62 %
accuracy (just for info) = 90.37 %
target score = 71.12 %
accuracy (just for info) = 89.70 %
[25]	validation_0-merror:0.09635	validation_1-merror:0.103069	validation_0-target_score:-0.736156	validation_1-target_score:-0.711168
target score = 73.78 %
accuracy (just for info) = 90.38 %
target score = 71.32 %
accuracy (just for info) = 89.70 %
[26]	validation_0-merror:0.096266	validation_1-merror:0.103069	validation_0-target_score:-0.737772	validation_1-target_score:-0.713183
target score = 73.97 %
accuracy (just for info) = 90.38 %
target score = 71.59 %
accuracy (just for info) = 89.73 %
[27]	validation_0-merror:0.09621	validation_1-merror:0.102789	validation_0-target_score:-0.739666	validation_1-target_score:-0.715868
target score = 74.17 %
accuracy (just for info) = 90.41 %
target score = 71.60 %
accuracy (just for info) = 89.77 %
[28]	validation_0-merror:0.095958	validation_1-merror:0.102454	validation_0-target_score:-0.741727	validation_1-target_score:-0.71598
target score = 74.31 %
accuracy (just for info) = 90.42 %
target score = 71.65 %
accuracy (just for info) = 89.80 %
[29]	validation_0-merror:0.095845	validation_1-merror:0.102062	validation_0-target_score:-0.743064	validation_1-target_score:-0.71654
target score = 74.42 %
accuracy (just for info) = 90.44 %
target score = 71.50 %
accuracy (just for info) = 89.85 %
[30]	validation_0-merror:0.095677	validation_1-merror:0.101615	validation_0-target_score:-0.744178	validation_1-target_score:-0.714973
target score = 74.41 %
accuracy (just for info) = 90.47 %
target score = 71.50 %
accuracy (just for info) = 89.82 %
[31]	validation_0-merror:0.095369	validation_1-merror:0.101895	validation_0-target_score:-0.744067	validation_1-target_score:-0.714973
target score = 74.43 %
accuracy (just for info) = 90.52 %
target score = 71.65 %
accuracy (just for info) = 89.84 %
[32]	validation_0-merror:0.09492	validation_1-merror:0.101671	validation_0-target_score:-0.744345	validation_1-target_score:-0.71654
target score = 74.57 %
accuracy (just for info) = 90.52 %
target score = 71.60 %
accuracy (just for info) = 89.86 %
[33]	validation_0-merror:0.094892	validation_1-merror:0.101504	validation_0-target_score:-0.745682	validation_1-target_score:-0.71598
target score = 74.56 %
accuracy (just for info) = 90.51 %
target score = 71.64 %
accuracy (just for info) = 89.89 %
[34]	validation_0-merror:0.094892	validation_1-merror:0.101224	validation_0-target_score:-0.745627	validation_1-target_score:-0.716428
target score = 74.61 %
accuracy (just for info) = 90.51 %
target score = 71.63 %
accuracy (just for info) = 89.89 %
[35]	validation_0-merror:0.09492	validation_1-merror:0.101224	validation_0-target_score:-0.746128	validation_1-target_score:-0.716316
target score = 74.77 %
accuracy (just for info) = 90.51 %
target score = 71.74 %
accuracy (just for info) = 89.89 %
[36]	validation_0-merror:0.094948	validation_1-merror:0.101168	validation_0-target_score:-0.747744	validation_1-target_score:-0.717435
target score = 74.90 %
accuracy (just for info) = 90.52 %
target score = 71.83 %
accuracy (just for info) = 89.88 %
[37]	validation_0-merror:0.09478	validation_1-merror:0.10128	validation_0-target_score:-0.748969	validation_1-target_score:-0.71833
target score = 75.04 %
accuracy (just for info) = 90.52 %
target score = 71.78 %
accuracy (just for info) = 89.87 %
[38]	validation_0-merror:0.094836	validation_1-merror:0.101392	validation_0-target_score:-0.750362	validation_1-target_score:-0.717771
target score = 75.24 %
accuracy (just for info) = 90.53 %
target score = 71.79 %
accuracy (just for info) = 89.91 %
[39]	validation_0-merror:0.094696	validation_1-merror:0.101001	validation_0-target_score:-0.752423	validation_1-target_score:-0.717883
target score = 75.18 %
accuracy (just for info) = 90.54 %
target score = 71.90 %
accuracy (just for info) = 89.91 %
[40]	validation_0-merror:0.094556	validation_1-merror:0.100945	validation_0-target_score:-0.751755	validation_1-target_score:-0.719002
target score = 75.20 %
accuracy (just for info) = 90.56 %
target score = 71.97 %
accuracy (just for info) = 89.93 %
[41]	validation_0-merror:0.094444	validation_1-merror:0.100777	validation_0-target_score:-0.752033	validation_1-target_score:-0.719673
target score = 75.39 %
accuracy (just for info) = 90.56 %
target score = 71.92 %
accuracy (just for info) = 89.90 %
[42]	validation_0-merror:0.09436	validation_1-merror:0.101056	validation_0-target_score:-0.753928	validation_1-target_score:-0.719226
target score = 75.49 %
accuracy (just for info) = 90.59 %
target score = 72.06 %
accuracy (just for info) = 89.91 %
[43]	validation_0-merror:0.094135	validation_1-merror:0.101001	validation_0-target_score:-0.754875	validation_1-target_score:-0.720568
target score = 75.58 %
accuracy (just for info) = 90.59 %
target score = 72.19 %
accuracy (just for info) = 89.89 %
[44]	validation_0-merror:0.094135	validation_1-merror:0.101112	validation_0-target_score:-0.755822	validation_1-target_score:-0.721911
target score = 75.67 %
accuracy (just for info) = 90.62 %
target score = 72.35 %
accuracy (just for info) = 89.88 %
[45]	validation_0-merror:0.093799	validation_1-merror:0.10128	validation_0-target_score:-0.756713	validation_1-target_score:-0.723478
target score = 75.74 %
accuracy (just for info) = 90.62 %
target score = 72.37 %
accuracy (just for info) = 89.91 %
[46]	validation_0-merror:0.093771	validation_1-merror:0.101001	validation_0-target_score:-0.757382	validation_1-target_score:-0.723702
target score = 75.79 %
accuracy (just for info) = 90.62 %
target score = 72.30 %
accuracy (just for info) = 89.89 %
[47]	validation_0-merror:0.093799	validation_1-merror:0.101168	validation_0-target_score:-0.757939	validation_1-target_score:-0.72303
target score = 75.84 %
accuracy (just for info) = 90.64 %
target score = 72.49 %
accuracy (just for info) = 89.88 %
[48]	validation_0-merror:0.093659	validation_1-merror:0.101224	validation_0-target_score:-0.75844	validation_1-target_score:-0.724933
target score = 75.94 %
accuracy (just for info) = 90.64 %
target score = 72.46 %
accuracy (just for info) = 89.91 %
[49]	validation_0-merror:0.093631	validation_1-merror:0.100945	validation_0-target_score:-0.759387	validation_1-target_score:-0.724597
target score = 75.93 %
accuracy (just for info) = 90.65 %
target score = 72.49 %
accuracy (just for info) = 89.89 %
[50]	validation_0-merror:0.093547	validation_1-merror:0.101112	validation_0-target_score:-0.759331	validation_1-target_score:-0.724933
target score = 76.12 %
accuracy (just for info) = 90.67 %
target score = 72.54 %
accuracy (just for info) = 89.91 %
[51]	validation_0-merror:0.093379	validation_1-merror:0.100945	validation_0-target_score:-0.761226	validation_1-target_score:-0.72538
target score = 76.10 %
accuracy (just for info) = 90.68 %
target score = 72.62 %
accuracy (just for info) = 89.92 %
[52]	validation_0-merror:0.093238	validation_1-merror:0.100889	validation_0-target_score:-0.761003	validation_1-target_score:-0.726164
target score = 76.25 %
accuracy (just for info) = 90.69 %
target score = 72.59 %
accuracy (just for info) = 89.90 %
[53]	validation_0-merror:0.093182	validation_1-merror:0.101001	validation_0-target_score:-0.762507	validation_1-target_score:-0.72594
target score = 76.27 %
accuracy (just for info) = 90.69 %
target score = 72.56 %
accuracy (just for info) = 89.91 %
[54]	validation_0-merror:0.093154	validation_1-merror:0.100945	validation_0-target_score:-0.76273	validation_1-target_score:-0.725604
target score = 76.30 %
accuracy (just for info) = 90.72 %
target score = 72.58 %
accuracy (just for info) = 89.92 %
[55]	validation_0-merror:0.092958	validation_1-merror:0.100777	validation_0-target_score:-0.763008	validation_1-target_score:-0.725828
target score = 76.36 %
accuracy (just for info) = 90.73 %
target score = 72.62 %
accuracy (just for info) = 89.94 %
[56]	validation_0-merror:0.092902	validation_1-merror:0.100609	validation_0-target_score:-0.763621	validation_1-target_score:-0.726164
target score = 76.41 %
accuracy (just for info) = 90.72 %
target score = 72.74 %
accuracy (just for info) = 89.96 %
[57]	validation_0-merror:0.092958	validation_1-merror:0.100442	validation_0-target_score:-0.764067	validation_1-target_score:-0.727395
target score = 76.47 %
accuracy (just for info) = 90.73 %
target score = 72.68 %
accuracy (just for info) = 89.94 %
[58]	validation_0-merror:0.092846	validation_1-merror:0.100609	validation_0-target_score:-0.764735	validation_1-target_score:-0.726835
target score = 76.48 %
accuracy (just for info) = 90.74 %
target score = 72.65 %
accuracy (just for info) = 89.97 %
[59]	validation_0-merror:0.09279	validation_1-merror:0.10033	validation_0-target_score:-0.764791	validation_1-target_score:-0.7265
target score = 76.60 %
accuracy (just for info) = 90.76 %
target score = 72.69 %
accuracy (just for info) = 89.97 %
[60]	validation_0-merror:0.09251	validation_1-merror:0.10033	validation_0-target_score:-0.766017	validation_1-target_score:-0.726947
target score = 76.72 %
accuracy (just for info) = 90.78 %
target score = 72.68 %
accuracy (just for info) = 89.99 %
[61]	validation_0-merror:0.092257	validation_1-merror:0.100106	validation_0-target_score:-0.767187	validation_1-target_score:-0.726835
target score = 76.78 %
accuracy (just for info) = 90.79 %
target score = 72.73 %
accuracy (just for info) = 89.95 %
[62]	validation_0-merror:0.092201	validation_1-merror:0.100553	validation_0-target_score:-0.767799	validation_1-target_score:-0.727283
target score = 76.74 %
accuracy (just for info) = 90.79 %
target score = 72.85 %
accuracy (just for info) = 89.97 %
[63]	validation_0-merror:0.092173	validation_1-merror:0.10033	validation_0-target_score:-0.767409	validation_1-target_score:-0.728514
target score = 76.70 %
accuracy (just for info) = 90.81 %
target score = 72.61 %
accuracy (just for info) = 89.98 %
[64]	validation_0-merror:0.091977	validation_1-merror:0.100162	validation_0-target_score:-0.766964	validation_1-target_score:-0.726052
target score = 76.72 %
accuracy (just for info) = 90.82 %
target score = 72.68 %
accuracy (just for info) = 90.02 %
[65]	validation_0-merror:0.091865	validation_1-merror:0.099883	validation_0-target_score:-0.767242	validation_1-target_score:-0.726835
target score = 76.84 %
accuracy (just for info) = 90.85 %
target score = 72.81 %
accuracy (just for info) = 90.02 %
[66]	validation_0-merror:0.091641	validation_1-merror:0.099883	validation_0-target_score:-0.768412	validation_1-target_score:-0.728066
target score = 77.01 %
accuracy (just for info) = 90.86 %
target score = 72.88 %
accuracy (just for info) = 90.04 %
[67]	validation_0-merror:0.091528	validation_1-merror:0.099659	validation_0-target_score:-0.770139	validation_1-target_score:-0.72885
target score = 77.12 %
accuracy (just for info) = 90.91 %
target score = 72.94 %
accuracy (just for info) = 90.06 %
[68]	validation_0-merror:0.091024	validation_1-merror:0.099603	validation_0-target_score:-0.771198	validation_1-target_score:-0.729409
target score = 77.25 %
accuracy (just for info) = 90.93 %
target score = 73.08 %
accuracy (just for info) = 90.05 %
[69]	validation_0-merror:0.090828	validation_1-merror:0.099547	validation_0-target_score:-0.772535	validation_1-target_score:-0.730752
target score = 77.34 %
accuracy (just for info) = 90.93 %
target score = 73.15 %
accuracy (just for info) = 90.07 %
[70]	validation_0-merror:0.090856	validation_1-merror:0.099268	validation_0-target_score:-0.773426	validation_1-target_score:-0.731535
target score = 77.40 %
accuracy (just for info) = 90.95 %
target score = 73.32 %
accuracy (just for info) = 90.08 %
[71]	validation_0-merror:0.090828	validation_1-merror:0.099324	validation_0-target_score:-0.774039	validation_1-target_score:-0.733214
target score = 77.58 %
accuracy (just for info) = 90.96 %
target score = 73.27 %
accuracy (just for info) = 90.08 %
[72]	validation_0-merror:0.090491	validation_1-merror:0.09938	validation_0-target_score:-0.775822	validation_1-target_score:-0.732654
target score = 77.74 %
accuracy (just for info) = 90.98 %
target score = 73.19 %
accuracy (just for info) = 90.08 %
[73]	validation_0-merror:0.090323	validation_1-merror:0.099268	validation_0-target_score:-0.777437	validation_1-target_score:-0.731871
target score = 77.75 %
accuracy (just for info) = 90.98 %
target score = 73.22 %
accuracy (just for info) = 90.08 %
[74]	validation_0-merror:0.090379	validation_1-merror:0.099212	validation_0-target_score:-0.777549	validation_1-target_score:-0.732207
target score = 77.80 %
accuracy (just for info) = 91.00 %
target score = 73.23 %
accuracy (just for info) = 90.11 %
[75]	validation_0-merror:0.090211	validation_1-merror:0.098932	validation_0-target_score:-0.777994	validation_1-target_score:-0.732319
target score = 77.86 %
accuracy (just for info) = 91.01 %
target score = 73.20 %
accuracy (just for info) = 90.10 %
[76]	validation_0-merror:0.090071	validation_1-merror:0.099044	validation_0-target_score:-0.778607	validation_1-target_score:-0.731983
target score = 77.86 %
accuracy (just for info) = 91.03 %
target score = 73.13 %
accuracy (just for info) = 90.10 %
[77]	validation_0-merror:0.089734	validation_1-merror:0.099044	validation_0-target_score:-0.778607	validation_1-target_score:-0.731312
target score = 77.93 %
accuracy (just for info) = 91.07 %
target score = 72.99 %
accuracy (just for info) = 90.07 %
[78]	validation_0-merror:0.089398	validation_1-merror:0.099324	validation_0-target_score:-0.779331	validation_1-target_score:-0.729857
target score = 77.97 %
accuracy (just for info) = 91.09 %
target score = 73.01 %
accuracy (just for info) = 90.06 %
[79]	validation_0-merror:0.089258	validation_1-merror:0.099435	validation_0-target_score:-0.779666	validation_1-target_score:-0.730081
target score = 77.97 %
accuracy (just for info) = 91.11 %
target score = 73.14 %
accuracy (just for info) = 90.06 %
[80]	validation_0-merror:0.089061	validation_1-merror:0.09938	validation_0-target_score:-0.779666	validation_1-target_score:-0.731423
target score = 78.03 %
accuracy (just for info) = 91.11 %
target score = 73.29 %
accuracy (just for info) = 90.10 %
[81]	validation_0-merror:0.089033	validation_1-merror:0.0991	validation_0-target_score:-0.780334	validation_1-target_score:-0.732878
target score = 78.26 %
accuracy (just for info) = 91.15 %
target score = 73.21 %
accuracy (just for info) = 90.08 %
[82]	validation_0-merror:0.088641	validation_1-merror:0.099324	validation_0-target_score:-0.782563	validation_1-target_score:-0.732095
target score = 78.42 %
accuracy (just for info) = 91.15 %
target score = 73.24 %
accuracy (just for info) = 90.03 %
[83]	validation_0-merror:0.088585	validation_1-merror:0.099827	validation_0-target_score:-0.784178	validation_1-target_score:-0.732431
target score = 78.45 %
accuracy (just for info) = 91.16 %
target score = 73.24 %
accuracy (just for info) = 90.03 %
[84]	validation_0-merror:0.088501	validation_1-merror:0.099883	validation_0-target_score:-0.784457	validation_1-target_score:-0.732431
target score = 78.51 %
accuracy (just for info) = 91.18 %
target score = 73.33 %
accuracy (just for info) = 90.06 %
[85]	validation_0-merror:0.088305	validation_1-merror:0.099491	validation_0-target_score:-0.785125	validation_1-target_score:-0.733326
target score = 78.52 %
accuracy (just for info) = 91.19 %
target score = 73.30 %
accuracy (just for info) = 90.06 %
[86]	validation_0-merror:0.088192	validation_1-merror:0.099547	validation_0-target_score:-0.785237	validation_1-target_score:-0.73299
target score = 78.58 %
accuracy (just for info) = 91.19 %
target score = 73.32 %
accuracy (just for info) = 90.06 %
[87]	validation_0-merror:0.088192	validation_1-merror:0.099547	validation_0-target_score:-0.785794	validation_1-target_score:-0.733214
target score = 78.64 %
accuracy (just for info) = 91.21 %
target score = 73.38 %
accuracy (just for info) = 90.08 %
[88]	validation_0-merror:0.088024	validation_1-merror:0.099491	validation_0-target_score:-0.786351	validation_1-target_score:-0.733774
target score = 78.62 %
accuracy (just for info) = 91.22 %
target score = 73.46 %
accuracy (just for info) = 90.06 %
[89]	validation_0-merror:0.087996	validation_1-merror:0.099547	validation_0-target_score:-0.78624	validation_1-target_score:-0.734557
target score = 78.81 %
accuracy (just for info) = 91.25 %
target score = 73.65 %
accuracy (just for info) = 90.05 %
[90]	validation_0-merror:0.08766	validation_1-merror:0.099715	validation_0-target_score:-0.788134	validation_1-target_score:-0.736459
target score = 78.86 %
accuracy (just for info) = 91.25 %
target score = 73.69 %
accuracy (just for info) = 90.05 %
[91]	validation_0-merror:0.087576	validation_1-merror:0.099603	validation_0-target_score:-0.788635	validation_1-target_score:-0.736907
target score = 78.90 %
accuracy (just for info) = 91.26 %
target score = 73.71 %
accuracy (just for info) = 90.07 %
[92]	validation_0-merror:0.087464	validation_1-merror:0.099435	validation_0-target_score:-0.789025	validation_1-target_score:-0.737131
target score = 78.97 %
accuracy (just for info) = 91.28 %
target score = 73.58 %
accuracy (just for info) = 90.08 %
[93]	validation_0-merror:0.087239	validation_1-merror:0.099491	validation_0-target_score:-0.789694	validation_1-target_score:-0.735788
target score = 79.06 %
accuracy (just for info) = 91.28 %
target score = 73.61 %
accuracy (just for info) = 90.02 %
[94]	validation_0-merror:0.087295	validation_1-merror:0.099827	validation_0-target_score:-0.790585	validation_1-target_score:-0.736124
target score = 79.12 %
accuracy (just for info) = 91.31 %
target score = 73.65 %
accuracy (just for info) = 90.05 %
[95]	validation_0-merror:0.086959	validation_1-merror:0.099603	validation_0-target_score:-0.791198	validation_1-target_score:-0.736459
target score = 79.18 %
accuracy (just for info) = 91.32 %
target score = 73.75 %
accuracy (just for info) = 90.04 %
[96]	validation_0-merror:0.086847	validation_1-merror:0.099715	validation_0-target_score:-0.791811	validation_1-target_score:-0.737466
target score = 79.21 %
accuracy (just for info) = 91.32 %
target score = 73.50 %
accuracy (just for info) = 90.02 %
[97]	validation_0-merror:0.086875	validation_1-merror:0.099883	validation_0-target_score:-0.792145	validation_1-target_score:-0.735004
target score = 79.35 %
accuracy (just for info) = 91.35 %
target score = 73.63 %
accuracy (just for info) = 90.01 %
[98]	validation_0-merror:0.086566	validation_1-merror:0.099939	validation_0-target_score:-0.793482	validation_1-target_score:-0.736347
target score = 79.44 %
accuracy (just for info) = 91.38 %
target score = 73.70 %
accuracy (just for info) = 90.00 %
[99]	validation_0-merror:0.086286	validation_1-merror:0.10005	validation_0-target_score:-0.794373	validation_1-target_score:-0.737019
target score = 79.44 %
accuracy (just for info) = 91.38 %
target score = 73.68 %
accuracy (just for info) = 90.02 %
[100]	validation_0-merror:0.086286	validation_1-merror:0.099939	validation_0-target_score:-0.794429	validation_1-target_score:-0.736795
target score = 79.57 %
accuracy (just for info) = 91.39 %
target score = 73.72 %
accuracy (just for info) = 90.04 %
[101]	validation_0-merror:0.086202	validation_1-merror:0.099659	validation_0-target_score:-0.79571	validation_1-target_score:-0.737243
target score = 79.61 %
accuracy (just for info) = 91.39 %
target score = 73.77 %
accuracy (just for info) = 90.03 %
[102]	validation_0-merror:0.086174	validation_1-merror:0.099827	validation_0-target_score:-0.7961	validation_1-target_score:-0.73769
target score = 79.72 %
accuracy (just for info) = 91.43 %
target score = 73.75 %
accuracy (just for info) = 90.04 %
[103]	validation_0-merror:0.085838	validation_1-merror:0.099659	validation_0-target_score:-0.797214	validation_1-target_score:-0.737466
target score = 79.73 %
accuracy (just for info) = 91.44 %
target score = 73.74 %
accuracy (just for info) = 90.05 %
[104]	validation_0-merror:0.085697	validation_1-merror:0.099547	validation_0-target_score:-0.797326	validation_1-target_score:-0.737355
target score = 79.76 %
accuracy (just for info) = 91.44 %
target score = 73.72 %
accuracy (just for info) = 90.02 %
[105]	validation_0-merror:0.085726	validation_1-merror:0.099771	validation_0-target_score:-0.797604	validation_1-target_score:-0.737243
target score = 79.79 %
accuracy (just for info) = 91.45 %
target score = 73.67 %
accuracy (just for info) = 90.06 %
[106]	validation_0-merror:0.085585	validation_1-merror:0.099435	validation_0-target_score:-0.797939	validation_1-target_score:-0.736683
target score = 79.81 %
accuracy (just for info) = 91.47 %
target score = 73.65 %
accuracy (just for info) = 90.08 %
[107]	validation_0-merror:0.085389	validation_1-merror:0.099268	validation_0-target_score:-0.798106	validation_1-target_score:-0.736459
target score = 79.78 %
accuracy (just for info) = 91.49 %
target score = 73.50 %
accuracy (just for info) = 90.07 %
[108]	validation_0-merror:0.085165	validation_1-merror:0.09938	validation_0-target_score:-0.797772	validation_1-target_score:-0.735004
target score = 79.87 %
accuracy (just for info) = 91.53 %
target score = 73.57 %
accuracy (just for info) = 90.05 %
[109]	validation_0-merror:0.084828	validation_1-merror:0.099603	validation_0-target_score:-0.798663	validation_1-target_score:-0.735676
target score = 79.89 %
accuracy (just for info) = 91.55 %
target score = 73.59 %
accuracy (just for info) = 90.05 %
[110]	validation_0-merror:0.084688	validation_1-merror:0.099603	validation_0-target_score:-0.798886	validation_1-target_score:-0.7359
target score = 80.03 %
accuracy (just for info) = 91.56 %
target score = 73.65 %
accuracy (just for info) = 90.05 %
[111]	validation_0-merror:0.084464	validation_1-merror:0.099547	validation_0-target_score:-0.800279	validation_1-target_score:-0.736459
target score = 80.11 %
accuracy (just for info) = 91.58 %
target score = 73.55 %
accuracy (just for info) = 90.06 %
[112]	validation_0-merror:0.084324	validation_1-merror:0.099435	validation_0-target_score:-0.801058	validation_1-target_score:-0.735452
target score = 80.19 %
accuracy (just for info) = 91.61 %
target score = 73.58 %
accuracy (just for info) = 90.06 %
[113]	validation_0-merror:0.084128	validation_1-merror:0.099435	validation_0-target_score:-0.801894	validation_1-target_score:-0.735788
target score = 80.25 %
accuracy (just for info) = 91.60 %
target score = 73.67 %
accuracy (just for info) = 90.08 %
[114]	validation_0-merror:0.084156	validation_1-merror:0.099268	validation_0-target_score:-0.802451	validation_1-target_score:-0.736683
target score = 80.31 %
accuracy (just for info) = 91.60 %
target score = 73.65 %
accuracy (just for info) = 90.06 %
[115]	validation_0-merror:0.0841	validation_1-merror:0.099491	validation_0-target_score:-0.803064	validation_1-target_score:-0.736459
target score = 80.30 %
accuracy (just for info) = 91.61 %
target score = 73.59 %
accuracy (just for info) = 90.04 %
[116]	validation_0-merror:0.084044	validation_1-merror:0.099659	validation_0-target_score:-0.802953	validation_1-target_score:-0.7359
target score = 80.51 %
accuracy (just for info) = 91.64 %
target score = 73.57 %
accuracy (just for info) = 90.04 %
[117]	validation_0-merror:0.083847	validation_1-merror:0.099659	validation_0-target_score:-0.80507	validation_1-target_score:-0.735676
target score = 80.51 %
accuracy (just for info) = 91.66 %
target score = 73.56 %
accuracy (just for info) = 90.04 %
[118]	validation_0-merror:0.083539	validation_1-merror:0.099659	validation_0-target_score:-0.805125	validation_1-target_score:-0.735564
target score = 80.55 %
accuracy (just for info) = 91.67 %
target score = 73.46 %
accuracy (just for info) = 90.05 %
[119]	validation_0-merror:0.083399	validation_1-merror:0.099547	validation_0-target_score:-0.80546	validation_1-target_score:-0.734557
target score = 80.59 %
accuracy (just for info) = 91.67 %
target score = 73.68 %
accuracy (just for info) = 90.02 %
[120]	validation_0-merror:0.083455	validation_1-merror:0.099883	validation_0-target_score:-0.805905	validation_1-target_score:-0.736795
target score = 80.64 %
accuracy (just for info) = 91.68 %
target score = 73.58 %
accuracy (just for info) = 90.01 %
[121]	validation_0-merror:0.083231	validation_1-merror:0.099939	validation_0-target_score:-0.806407	validation_1-target_score:-0.735788
target score = 80.72 %
accuracy (just for info) = 91.69 %
target score = 73.69 %
accuracy (just for info) = 90.02 %
[122]	validation_0-merror:0.083146	validation_1-merror:0.099827	validation_0-target_score:-0.807187	validation_1-target_score:-0.736907
target score = 80.85 %
accuracy (just for info) = 91.71 %
target score = 73.80 %
accuracy (just for info) = 90.03 %
[123]	validation_0-merror:0.083006	validation_1-merror:0.099715	validation_0-target_score:-0.808468	validation_1-target_score:-0.738026
target score = 80.97 %
accuracy (just for info) = 91.76 %
target score = 73.71 %
accuracy (just for info) = 90.05 %
[124]	validation_0-merror:0.082502	validation_1-merror:0.099827	validation_0-target_score:-0.809694	validation_1-target_score:-0.737131
target score = 80.99 %
accuracy (just for info) = 91.78 %
target score = 73.70 %
accuracy (just for info) = 90.06 %
[125]	validation_0-merror:0.082277	validation_1-merror:0.099715	validation_0-target_score:-0.809916	validation_1-target_score:-0.737019
target score = 81.07 %
accuracy (just for info) = 91.81 %
target score = 73.70 %
accuracy (just for info) = 90.06 %
[126]	validation_0-merror:0.082025	validation_1-merror:0.099491	validation_0-target_score:-0.810696	validation_1-target_score:-0.737019
target score = 81.10 %
accuracy (just for info) = 91.81 %
target score = 73.67 %
accuracy (just for info) = 90.05 %
[127]	validation_0-merror:0.081997	validation_1-merror:0.099547	validation_0-target_score:-0.811031	validation_1-target_score:-0.736683
target score = 81.30 %
accuracy (just for info) = 91.86 %
target score = 73.72 %
accuracy (just for info) = 90.06 %
[128]	validation_0-merror:0.081464	validation_1-merror:0.099435	validation_0-target_score:-0.812981	validation_1-target_score:-0.737243
target score = 81.35 %
accuracy (just for info) = 91.86 %
target score = 73.78 %
accuracy (just for info) = 90.06 %
[129]	validation_0-merror:0.081492	validation_1-merror:0.099435	validation_0-target_score:-0.813482	validation_1-target_score:-0.737802
target score = 81.39 %
accuracy (just for info) = 91.85 %
target score = 73.69 %
accuracy (just for info) = 90.07 %
[130]	validation_0-merror:0.081605	validation_1-merror:0.099324	validation_0-target_score:-0.813872	validation_1-target_score:-0.736907
target score = 81.42 %
accuracy (just for info) = 91.87 %
target score = 73.63 %
accuracy (just for info) = 90.03 %
[131]	validation_0-merror:0.081352	validation_1-merror:0.099771	validation_0-target_score:-0.81415	validation_1-target_score:-0.736347
target score = 81.53 %
accuracy (just for info) = 91.91 %
target score = 73.60 %
accuracy (just for info) = 90.08 %
[132]	validation_0-merror:0.080988	validation_1-merror:0.099324	validation_0-target_score:-0.815265	validation_1-target_score:-0.736012
target score = 81.59 %
accuracy (just for info) = 91.93 %
target score = 73.71 %
accuracy (just for info) = 90.04 %
[133]	validation_0-merror:0.080848	validation_1-merror:0.099883	validation_0-target_score:-0.815877	validation_1-target_score:-0.737131
target score = 81.69 %
accuracy (just for info) = 91.93 %
target score = 73.75 %
accuracy (just for info) = 90.05 %
[134]	validation_0-merror:0.080736	validation_1-merror:0.099715	validation_0-target_score:-0.81688	validation_1-target_score:-0.737466
target score = 81.74 %
accuracy (just for info) = 91.96 %
target score = 73.74 %
accuracy (just for info) = 90.03 %
[135]	validation_0-merror:0.080455	validation_1-merror:0.099771	validation_0-target_score:-0.817437	validation_1-target_score:-0.737355
target score = 81.79 %
accuracy (just for info) = 91.97 %
target score = 73.59 %
accuracy (just for info) = 90.05 %
[136]	validation_0-merror:0.080399	validation_1-merror:0.099547	validation_0-target_score:-0.817939	validation_1-target_score:-0.7359
target score = 81.82 %
accuracy (just for info) = 91.97 %
target score = 73.53 %
accuracy (just for info) = 90.06 %
[137]	validation_0-merror:0.080399	validation_1-merror:0.099491	validation_0-target_score:-0.818162	validation_1-target_score:-0.73534
target score = 81.87 %
accuracy (just for info) = 92.00 %
target score = 73.63 %
accuracy (just for info) = 90.06 %
[138]	validation_0-merror:0.080119	validation_1-merror:0.099435	validation_0-target_score:-0.818663	validation_1-target_score:-0.736347
target score = 81.97 %
accuracy (just for info) = 92.01 %
target score = 73.60 %
accuracy (just for info) = 90.02 %
[139]	validation_0-merror:0.080091	validation_1-merror:0.099827	validation_0-target_score:-0.819666	validation_1-target_score:-0.736012
target score = 82.05 %
accuracy (just for info) = 92.02 %
target score = 73.55 %
accuracy (just for info) = 90.03 %
[140]	validation_0-merror:0.079895	validation_1-merror:0.099715	validation_0-target_score:-0.820501	validation_1-target_score:-0.735452
target score = 82.08 %
accuracy (just for info) = 92.05 %
target score = 73.58 %
accuracy (just for info) = 90.00 %
[141]	validation_0-merror:0.079586	validation_1-merror:0.100162	validation_0-target_score:-0.82078	validation_1-target_score:-0.735788
target score = 82.16 %
accuracy (just for info) = 92.06 %
target score = 73.65 %
accuracy (just for info) = 90.01 %
[142]	validation_0-merror:0.079418	validation_1-merror:0.099994	validation_0-target_score:-0.82156	validation_1-target_score:-0.736459
target score = 82.20 %
accuracy (just for info) = 92.11 %
target score = 73.62 %
accuracy (just for info) = 90.02 %
[143]	validation_0-merror:0.079026	validation_1-merror:0.099883	validation_0-target_score:-0.822006	validation_1-target_score:-0.736235
target score = 82.22 %
accuracy (just for info) = 92.12 %
target score = 73.62 %
accuracy (just for info) = 90.03 %
[144]	validation_0-merror:0.078941	validation_1-merror:0.099771	validation_0-target_score:-0.822173	validation_1-target_score:-0.736235
target score = 82.29 %
accuracy (just for info) = 92.15 %
target score = 73.67 %
accuracy (just for info) = 90.03 %
[145]	validation_0-merror:0.078661	validation_1-merror:0.099771	validation_0-target_score:-0.822897	validation_1-target_score:-0.736683
target score = 82.30 %
accuracy (just for info) = 92.13 %
target score = 73.74 %
accuracy (just for info) = 90.02 %
[146]	validation_0-merror:0.078829	validation_1-merror:0.099883	validation_0-target_score:-0.822953	validation_1-target_score:-0.737355
target score = 82.33 %
accuracy (just for info) = 92.14 %
target score = 73.72 %
accuracy (just for info) = 90.04 %
[147]	validation_0-merror:0.078717	validation_1-merror:0.099659	validation_0-target_score:-0.823287	validation_1-target_score:-0.737243
target score = 82.43 %
accuracy (just for info) = 92.15 %
target score = 73.78 %
accuracy (just for info) = 90.03 %
[148]	validation_0-merror:0.078661	validation_1-merror:0.099771	validation_0-target_score:-0.824345	validation_1-target_score:-0.737802
target score = 82.51 %
accuracy (just for info) = 92.18 %
target score = 73.68 %
accuracy (just for info) = 90.03 %
[149]	validation_0-merror:0.078353	validation_1-merror:0.099715	validation_0-target_score:-0.825125	validation_1-target_score:-0.736795
target score = 82.55 %
accuracy (just for info) = 92.18 %
target score = 73.72 %
accuracy (just for info) = 90.05 %
[150]	validation_0-merror:0.078353	validation_1-merror:0.099491	validation_0-target_score:-0.825515	validation_1-target_score:-0.737243
target score = 82.65 %
accuracy (just for info) = 92.20 %
target score = 73.75 %
accuracy (just for info) = 90.05 %
[151]	validation_0-merror:0.078157	validation_1-merror:0.099547	validation_0-target_score:-0.826518	validation_1-target_score:-0.737466
target score = 82.67 %
accuracy (just for info) = 92.21 %
target score = 73.71 %
accuracy (just for info) = 90.02 %
[152]	validation_0-merror:0.078016	validation_1-merror:0.099883	validation_0-target_score:-0.826741	validation_1-target_score:-0.737131
target score = 82.71 %
accuracy (just for info) = 92.22 %
target score = 73.71 %
accuracy (just for info) = 90.05 %
[153]	validation_0-merror:0.077876	validation_1-merror:0.099659	validation_0-target_score:-0.827075	validation_1-target_score:-0.737131
target score = 82.72 %
accuracy (just for info) = 92.23 %
target score = 73.84 %
accuracy (just for info) = 90.07 %
[154]	validation_0-merror:0.077764	validation_1-merror:0.099268	validation_0-target_score:-0.827187	validation_1-target_score:-0.738362
target score = 82.74 %
accuracy (just for info) = 92.27 %
target score = 73.80 %
accuracy (just for info) = 90.08 %
[155]	validation_0-merror:0.077372	validation_1-merror:0.099435	validation_0-target_score:-0.827409	validation_1-target_score:-0.738026
target score = 82.81 %
accuracy (just for info) = 92.27 %
target score = 73.86 %
accuracy (just for info) = 90.08 %
[156]	validation_0-merror:0.077344	validation_1-merror:0.099547	validation_0-target_score:-0.828134	validation_1-target_score:-0.738585
target score = 82.86 %
accuracy (just for info) = 92.28 %
target score = 73.95 %
accuracy (just for info) = 90.08 %
[157]	validation_0-merror:0.077316	validation_1-merror:0.099547	validation_0-target_score:-0.828635	validation_1-target_score:-0.739481
target score = 82.90 %
accuracy (just for info) = 92.30 %
target score = 73.86 %
accuracy (just for info) = 90.05 %
[158]	validation_0-merror:0.077175	validation_1-merror:0.099491	validation_0-target_score:-0.828969	validation_1-target_score:-0.738585
target score = 82.89 %
accuracy (just for info) = 92.31 %
target score = 73.82 %
accuracy (just for info) = 90.07 %
[159]	validation_0-merror:0.077063	validation_1-merror:0.099324	validation_0-target_score:-0.828914	validation_1-target_score:-0.73825
target score = 82.96 %
accuracy (just for info) = 92.33 %
target score = 73.84 %
accuracy (just for info) = 90.07 %
[160]	validation_0-merror:0.076811	validation_1-merror:0.099324	validation_0-target_score:-0.829638	validation_1-target_score:-0.738362
target score = 83.08 %
accuracy (just for info) = 92.34 %
target score = 73.84 %
accuracy (just for info) = 90.10 %
[161]	validation_0-merror:0.076699	validation_1-merror:0.099044	validation_0-target_score:-0.830808	validation_1-target_score:-0.738362
target score = 83.13 %
accuracy (just for info) = 92.37 %
target score = 73.69 %
accuracy (just for info) = 90.07 %
[162]	validation_0-merror:0.076418	validation_1-merror:0.099324	validation_0-target_score:-0.831309	validation_1-target_score:-0.736907
target score = 83.18 %
accuracy (just for info) = 92.38 %
target score = 73.70 %
accuracy (just for info) = 90.06 %
[163]	validation_0-merror:0.076306	validation_1-merror:0.09938	validation_0-target_score:-0.831811	validation_1-target_score:-0.737019
target score = 83.19 %
accuracy (just for info) = 92.39 %
target score = 73.77 %
accuracy (just for info) = 90.07 %
[164]	validation_0-merror:0.076278	validation_1-merror:0.099324	validation_0-target_score:-0.831922	validation_1-target_score:-0.73769
target score = 83.25 %
accuracy (just for info) = 92.41 %
target score = 73.75 %
accuracy (just for info) = 90.07 %
[165]	validation_0-merror:0.076082	validation_1-merror:0.09938	validation_0-target_score:-0.832479	validation_1-target_score:-0.737466
target score = 83.30 %
accuracy (just for info) = 92.40 %
target score = 73.72 %
accuracy (just for info) = 90.06 %
[166]	validation_0-merror:0.076194	validation_1-merror:0.099491	validation_0-target_score:-0.833036	validation_1-target_score:-0.737243
target score = 83.34 %
accuracy (just for info) = 92.42 %
target score = 73.63 %
accuracy (just for info) = 90.06 %
[167]	validation_0-merror:0.076026	validation_1-merror:0.099491	validation_0-target_score:-0.83337	validation_1-target_score:-0.736347
target score = 83.39 %
accuracy (just for info) = 92.44 %
target score = 73.80 %
accuracy (just for info) = 90.10 %
[168]	validation_0-merror:0.075886	validation_1-merror:0.0991	validation_0-target_score:-0.833928	validation_1-target_score:-0.738026
target score = 83.44 %
accuracy (just for info) = 92.45 %
target score = 73.65 %
accuracy (just for info) = 90.09 %
[169]	validation_0-merror:0.075662	validation_1-merror:0.099156	validation_0-target_score:-0.834373	validation_1-target_score:-0.736459
target score = 83.51 %
accuracy (just for info) = 92.49 %
target score = 73.80 %
accuracy (just for info) = 90.05 %
[170]	validation_0-merror:0.075381	validation_1-merror:0.099603	validation_0-target_score:-0.835097	validation_1-target_score:-0.738026
target score = 83.60 %
accuracy (just for info) = 92.50 %
target score = 73.69 %
accuracy (just for info) = 90.07 %
[171]	validation_0-merror:0.075269	validation_1-merror:0.09938	validation_0-target_score:-0.835989	validation_1-target_score:-0.736907
target score = 83.64 %
accuracy (just for info) = 92.52 %
target score = 73.80 %
accuracy (just for info) = 90.07 %
[172]	validation_0-merror:0.075017	validation_1-merror:0.099491	validation_0-target_score:-0.836379	validation_1-target_score:-0.738026
target score = 83.78 %
accuracy (just for info) = 92.53 %
target score = 73.69 %
accuracy (just for info) = 90.07 %
[173]	validation_0-merror:0.075073	validation_1-merror:0.099435	validation_0-target_score:-0.837827	validation_1-target_score:-0.736907
target score = 83.81 %
accuracy (just for info) = 92.56 %
target score = 73.76 %
accuracy (just for info) = 90.10 %
[174]	validation_0-merror:0.074708	validation_1-merror:0.0991	validation_0-target_score:-0.83805	validation_1-target_score:-0.737578
target score = 83.81 %
accuracy (just for info) = 92.57 %
target score = 73.75 %
accuracy (just for info) = 90.07 %
[175]	validation_0-merror:0.07454	validation_1-merror:0.099435	validation_0-target_score:-0.838106	validation_1-target_score:-0.737466
target score = 83.89 %
accuracy (just for info) = 92.57 %
target score = 73.84 %
accuracy (just for info) = 90.09 %
[176]	validation_0-merror:0.074428	validation_1-merror:0.099156	validation_0-target_score:-0.838886	validation_1-target_score:-0.738362
target score = 83.91 %
accuracy (just for info) = 92.61 %
target score = 73.99 %
accuracy (just for info) = 90.09 %
[177]	validation_0-merror:0.074148	validation_1-merror:0.099156	validation_0-target_score:-0.839109	validation_1-target_score:-0.739928
target score = 83.91 %
accuracy (just for info) = 92.64 %
target score = 73.99 %
accuracy (just for info) = 90.08 %
[178]	validation_0-merror:0.073839	validation_1-merror:0.099268	validation_0-target_score:-0.839109	validation_1-target_score:-0.739928
target score = 83.96 %
accuracy (just for info) = 92.66 %
target score = 73.72 %
accuracy (just for info) = 90.10 %
[179]	validation_0-merror:0.073671	validation_1-merror:0.0991	validation_0-target_score:-0.83961	validation_1-target_score:-0.737243
target score = 84.07 %
accuracy (just for info) = 92.68 %
target score = 73.77 %
accuracy (just for info) = 90.11 %
[180]	validation_0-merror:0.073447	validation_1-merror:0.098988	validation_0-target_score:-0.840724	validation_1-target_score:-0.73769
target score = 84.16 %
accuracy (just for info) = 92.69 %
target score = 73.75 %
accuracy (just for info) = 90.12 %
[181]	validation_0-merror:0.073363	validation_1-merror:0.098877	validation_0-target_score:-0.841616	validation_1-target_score:-0.737466
target score = 84.17 %
accuracy (just for info) = 92.71 %
target score = 73.81 %
accuracy (just for info) = 90.12 %
[182]	validation_0-merror:0.073251	validation_1-merror:0.098877	validation_0-target_score:-0.841671	validation_1-target_score:-0.738138
target score = 84.23 %
accuracy (just for info) = 92.72 %
target score = 73.85 %
accuracy (just for info) = 90.16 %
[183]	validation_0-merror:0.073083	validation_1-merror:0.098541	validation_0-target_score:-0.842284	validation_1-target_score:-0.738474
target score = 84.32 %
accuracy (just for info) = 92.75 %
target score = 73.84 %
accuracy (just for info) = 90.14 %
[184]	validation_0-merror:0.072802	validation_1-merror:0.098653	validation_0-target_score:-0.843231	validation_1-target_score:-0.738362
target score = 84.37 %
accuracy (just for info) = 92.76 %
target score = 73.71 %
accuracy (just for info) = 90.15 %
[185]	validation_0-merror:0.072634	validation_1-merror:0.098541	validation_0-target_score:-0.843733	validation_1-target_score:-0.737131
target score = 84.40 %
accuracy (just for info) = 92.80 %
target score = 73.72 %
accuracy (just for info) = 90.12 %
[186]	validation_0-merror:0.072214	validation_1-merror:0.098877	validation_0-target_score:-0.844011	validation_1-target_score:-0.737243
target score = 84.45 %
accuracy (just for info) = 92.82 %
target score = 73.70 %
accuracy (just for info) = 90.12 %
[187]	validation_0-merror:0.072017	validation_1-merror:0.098821	validation_0-target_score:-0.844457	validation_1-target_score:-0.737019
target score = 84.50 %
accuracy (just for info) = 92.83 %
target score = 73.74 %
accuracy (just for info) = 90.13 %
[188]	validation_0-merror:0.071877	validation_1-merror:0.098709	validation_0-target_score:-0.844958	validation_1-target_score:-0.737355
target score = 84.61 %
accuracy (just for info) = 92.84 %
target score = 73.72 %
accuracy (just for info) = 90.13 %
[189]	validation_0-merror:0.071877	validation_1-merror:0.098653	validation_0-target_score:-0.846072	validation_1-target_score:-0.737243
target score = 84.67 %
accuracy (just for info) = 92.85 %
target score = 73.74 %
accuracy (just for info) = 90.15 %
[190]	validation_0-merror:0.071793	validation_1-merror:0.098541	validation_0-target_score:-0.846685	validation_1-target_score:-0.737355
target score = 84.65 %
accuracy (just for info) = 92.85 %
target score = 73.79 %
accuracy (just for info) = 90.16 %
[191]	validation_0-merror:0.071597	validation_1-merror:0.098429	validation_0-target_score:-0.846462	validation_1-target_score:-0.737914
target score = 84.74 %
accuracy (just for info) = 92.87 %
target score = 73.89 %
accuracy (just for info) = 90.18 %
[192]	validation_0-merror:0.071429	validation_1-merror:0.098262	validation_0-target_score:-0.847409	validation_1-target_score:-0.738921
target score = 84.76 %
accuracy (just for info) = 92.89 %
target score = 73.86 %
accuracy (just for info) = 90.18 %
[193]	validation_0-merror:0.071288	validation_1-merror:0.098318	validation_0-target_score:-0.847577	validation_1-target_score:-0.738585
target score = 84.80 %
accuracy (just for info) = 92.90 %
target score = 73.85 %
accuracy (just for info) = 90.17 %
[194]	validation_0-merror:0.071148	validation_1-merror:0.098318	validation_0-target_score:-0.847967	validation_1-target_score:-0.738474
target score = 84.79 %
accuracy (just for info) = 92.90 %
target score = 73.78 %
accuracy (just for info) = 90.16 %
[195]	validation_0-merror:0.071176	validation_1-merror:0.098429	validation_0-target_score:-0.847855	validation_1-target_score:-0.737802
target score = 84.94 %
accuracy (just for info) = 92.95 %
target score = 73.96 %
accuracy (just for info) = 90.17 %
[196]	validation_0-merror:0.070756	validation_1-merror:0.098373	validation_0-target_score:-0.849359	validation_1-target_score:-0.739593
target score = 84.93 %
accuracy (just for info) = 92.98 %
target score = 73.88 %
accuracy (just for info) = 90.17 %
[197]	validation_0-merror:0.070447	validation_1-merror:0.098318	validation_0-target_score:-0.849304	validation_1-target_score:-0.738809
target score = 84.99 %
accuracy (just for info) = 92.99 %
target score = 73.94 %
accuracy (just for info) = 90.17 %
[198]	validation_0-merror:0.070335	validation_1-merror:0.098318	validation_0-target_score:-0.849861	validation_1-target_score:-0.739369
target score = 85.06 %
accuracy (just for info) = 92.99 %
target score = 73.89 %
accuracy (just for info) = 90.17 %
[199]	validation_0-merror:0.070307	validation_1-merror:0.098318	validation_0-target_score:-0.850585	validation_1-target_score:-0.738921
target score = 85.13 %
accuracy (just for info) = 93.04 %
target score = 74.02 %
accuracy (just for info) = 90.14 %
[200]	validation_0-merror:0.069803	validation_1-merror:0.098709	validation_0-target_score:-0.851253	validation_1-target_score:-0.740152
target score = 85.21 %
accuracy (just for info) = 93.04 %
target score = 73.97 %
accuracy (just for info) = 90.20 %
[201]	validation_0-merror:0.069775	validation_1-merror:0.098094	validation_0-target_score:-0.852145	validation_1-target_score:-0.739705
target score = 85.23 %
accuracy (just for info) = 93.06 %
target score = 73.96 %
accuracy (just for info) = 90.22 %
[202]	validation_0-merror:0.06955	validation_1-merror:0.09787	validation_0-target_score:-0.852312	validation_1-target_score:-0.739593
target score = 85.25 %
accuracy (just for info) = 93.07 %
target score = 73.93 %
accuracy (just for info) = 90.22 %
[203]	validation_0-merror:0.06941	validation_1-merror:0.097982	validation_0-target_score:-0.852535	validation_1-target_score:-0.739257
target score = 85.34 %
accuracy (just for info) = 93.10 %
target score = 74.02 %
accuracy (just for info) = 90.20 %
[204]	validation_0-merror:0.069158	validation_1-merror:0.09815	validation_0-target_score:-0.853426	validation_1-target_score:-0.740152
target score = 85.36 %
accuracy (just for info) = 93.10 %
target score = 73.97 %
accuracy (just for info) = 90.19 %
[205]	validation_0-merror:0.06913	validation_1-merror:0.09815	validation_0-target_score:-0.853649	validation_1-target_score:-0.739705
target score = 85.44 %
accuracy (just for info) = 93.13 %
target score = 74.00 %
accuracy (just for info) = 90.20 %
[206]	validation_0-merror:0.06885	validation_1-merror:0.097982	validation_0-target_score:-0.854373	validation_1-target_score:-0.74004
target score = 85.45 %
accuracy (just for info) = 93.14 %
target score = 74.02 %
accuracy (just for info) = 90.21 %
[207]	validation_0-merror:0.068765	validation_1-merror:0.098094	validation_0-target_score:-0.85454	validation_1-target_score:-0.740152
target score = 85.48 %
accuracy (just for info) = 93.15 %
target score = 74.05 %
accuracy (just for info) = 90.22 %
[208]	validation_0-merror:0.068681	validation_1-merror:0.097815	validation_0-target_score:-0.854763	validation_1-target_score:-0.740488
target score = 85.54 %
accuracy (just for info) = 93.17 %
target score = 74.03 %
accuracy (just for info) = 90.21 %
[209]	validation_0-merror:0.068429	validation_1-merror:0.098038	validation_0-target_score:-0.855376	validation_1-target_score:-0.740264
target score = 85.67 %
accuracy (just for info) = 93.19 %
target score = 74.02 %
accuracy (just for info) = 90.19 %
[210]	validation_0-merror:0.068205	validation_1-merror:0.098206	validation_0-target_score:-0.856657	validation_1-target_score:-0.740152
target score = 85.68 %
accuracy (just for info) = 93.20 %
target score = 73.96 %
accuracy (just for info) = 90.19 %
[211]	validation_0-merror:0.068149	validation_1-merror:0.098318	validation_0-target_score:-0.856825	validation_1-target_score:-0.739593
target score = 85.74 %
accuracy (just for info) = 93.22 %
target score = 73.99 %
accuracy (just for info) = 90.18 %
[212]	validation_0-merror:0.068009	validation_1-merror:0.098429	validation_0-target_score:-0.857437	validation_1-target_score:-0.739928
target score = 85.80 %
accuracy (just for info) = 93.24 %
target score = 73.98 %
accuracy (just for info) = 90.19 %
[213]	validation_0-merror:0.067784	validation_1-merror:0.09815	validation_0-target_score:-0.857994	validation_1-target_score:-0.739816
target score = 85.84 %
accuracy (just for info) = 93.25 %
target score = 73.98 %
accuracy (just for info) = 90.20 %
[214]	validation_0-merror:0.067756	validation_1-merror:0.098262	validation_0-target_score:-0.85844	validation_1-target_score:-0.739816
target score = 85.86 %
accuracy (just for info) = 93.27 %
target score = 73.88 %
accuracy (just for info) = 90.20 %
[215]	validation_0-merror:0.06742	validation_1-merror:0.098206	validation_0-target_score:-0.858607	validation_1-target_score:-0.738809
target score = 85.86 %
accuracy (just for info) = 93.30 %
target score = 73.86 %
accuracy (just for info) = 90.19 %
[216]	validation_0-merror:0.067196	validation_1-merror:0.098318	validation_0-target_score:-0.858607	validation_1-target_score:-0.738585
target score = 85.96 %
accuracy (just for info) = 93.31 %
target score = 73.82 %
accuracy (just for info) = 90.19 %
[217]	validation_0-merror:0.067083	validation_1-merror:0.098373	validation_0-target_score:-0.85961	validation_1-target_score:-0.73825
target score = 85.97 %
accuracy (just for info) = 93.32 %
target score = 73.90 %
accuracy (just for info) = 90.21 %
[218]	validation_0-merror:0.066943	validation_1-merror:0.098038	validation_0-target_score:-0.859666	validation_1-target_score:-0.739033
target score = 85.99 %
accuracy (just for info) = 93.36 %
target score = 73.85 %
accuracy (just for info) = 90.18 %
[219]	validation_0-merror:0.066523	validation_1-merror:0.098373	validation_0-target_score:-0.859944	validation_1-target_score:-0.738474
target score = 86.03 %
accuracy (just for info) = 93.38 %
target score = 73.85 %
accuracy (just for info) = 90.18 %
[220]	validation_0-merror:0.066298	validation_1-merror:0.098262	validation_0-target_score:-0.860334	validation_1-target_score:-0.738474
target score = 86.11 %
accuracy (just for info) = 93.41 %
target score = 73.93 %
accuracy (just for info) = 90.17 %
[221]	validation_0-merror:0.066102	validation_1-merror:0.098373	validation_0-target_score:-0.861114	validation_1-target_score:-0.739257
target score = 86.22 %
accuracy (just for info) = 93.40 %
target score = 73.94 %
accuracy (just for info) = 90.17 %
[222]	validation_0-merror:0.066214	validation_1-merror:0.098373	validation_0-target_score:-0.862228	validation_1-target_score:-0.739369
target score = 86.27 %
accuracy (just for info) = 93.42 %
target score = 73.93 %
accuracy (just for info) = 90.17 %
[223]	validation_0-merror:0.06599	validation_1-merror:0.098373	validation_0-target_score:-0.862674	validation_1-target_score:-0.739257
target score = 86.32 %
accuracy (just for info) = 93.43 %
target score = 73.96 %
accuracy (just for info) = 90.17 %
[224]	validation_0-merror:0.065906	validation_1-merror:0.098485	validation_0-target_score:-0.863231	validation_1-target_score:-0.739593
target score = 86.41 %
accuracy (just for info) = 93.44 %
target score = 73.91 %
accuracy (just for info) = 90.15 %
[225]	validation_0-merror:0.065766	validation_1-merror:0.098597	validation_0-target_score:-0.864067	validation_1-target_score:-0.739145
target score = 86.43 %
accuracy (just for info) = 93.44 %
target score = 73.85 %
accuracy (just for info) = 90.18 %
[226]	validation_0-merror:0.065794	validation_1-merror:0.098429	validation_0-target_score:-0.86429	validation_1-target_score:-0.738474
target score = 86.40 %
accuracy (just for info) = 93.45 %
target score = 73.82 %
accuracy (just for info) = 90.17 %
[227]	validation_0-merror:0.065654	validation_1-merror:0.098485	validation_0-target_score:-0.864011	validation_1-target_score:-0.73825
target score = 86.46 %
accuracy (just for info) = 93.46 %
target score = 73.93 %
accuracy (just for info) = 90.17 %
[228]	validation_0-merror:0.065598	validation_1-merror:0.098597	validation_0-target_score:-0.864624	validation_1-target_score:-0.739257
target score = 86.53 %
accuracy (just for info) = 93.47 %
target score = 73.89 %
accuracy (just for info) = 90.17 %
[229]	validation_0-merror:0.065514	validation_1-merror:0.098541	validation_0-target_score:-0.865348	validation_1-target_score:-0.738921
target score = 86.61 %
accuracy (just for info) = 93.49 %
target score = 73.91 %
accuracy (just for info) = 90.16 %
[230]	validation_0-merror:0.065289	validation_1-merror:0.098597	validation_0-target_score:-0.866128	validation_1-target_score:-0.739145
target score = 86.63 %
accuracy (just for info) = 93.49 %
target score = 73.90 %
accuracy (just for info) = 90.15 %
[231]	validation_0-merror:0.065373	validation_1-merror:0.098709	validation_0-target_score:-0.866295	validation_1-target_score:-0.739033
target score = 86.65 %
accuracy (just for info) = 93.48 %
target score = 73.89 %
accuracy (just for info) = 90.16 %
[232]	validation_0-merror:0.065429	validation_1-merror:0.098597	validation_0-target_score:-0.866462	validation_1-target_score:-0.738921
target score = 86.67 %
accuracy (just for info) = 93.50 %
target score = 73.93 %
accuracy (just for info) = 90.19 %
[233]	validation_0-merror:0.065261	validation_1-merror:0.098206	validation_0-target_score:-0.866741	validation_1-target_score:-0.739257
target score = 86.77 %
accuracy (just for info) = 93.53 %
target score = 73.89 %
accuracy (just for info) = 90.22 %
[234]	validation_0-merror:0.065037	validation_1-merror:0.09787	validation_0-target_score:-0.867688	validation_1-target_score:-0.738921
target score = 86.85 %
accuracy (just for info) = 93.55 %
target score = 74.00 %
accuracy (just for info) = 90.18 %
[235]	validation_0-merror:0.064785	validation_1-merror:0.098318	validation_0-target_score:-0.868524	validation_1-target_score:-0.74004
target score = 86.87 %
accuracy (just for info) = 93.59 %
target score = 74.04 %
accuracy (just for info) = 90.17 %
[236]	validation_0-merror:0.064392	validation_1-merror:0.098485	validation_0-target_score:-0.868691	validation_1-target_score:-0.740376
target score = 86.93 %
accuracy (just for info) = 93.61 %
target score = 74.03 %
accuracy (just for info) = 90.19 %
[237]	validation_0-merror:0.064028	validation_1-merror:0.098206	validation_0-target_score:-0.869304	validation_1-target_score:-0.740264
target score = 87.00 %
accuracy (just for info) = 93.63 %
target score = 74.03 %
accuracy (just for info) = 90.16 %
[238]	validation_0-merror:0.06386	validation_1-merror:0.098597	validation_0-target_score:-0.869972	validation_1-target_score:-0.740264
target score = 87.01 %
accuracy (just for info) = 93.64 %
target score = 74.04 %
accuracy (just for info) = 90.16 %
[239]	validation_0-merror:0.063804	validation_1-merror:0.098485	validation_0-target_score:-0.870084	validation_1-target_score:-0.740376
target score = 87.04 %
accuracy (just for info) = 93.68 %
target score = 74.02 %
accuracy (just for info) = 90.18 %
[240]	validation_0-merror:0.063355	validation_1-merror:0.098262	validation_0-target_score:-0.870362	validation_1-target_score:-0.740152
target score = 87.06 %
accuracy (just for info) = 93.70 %
target score = 73.98 %
accuracy (just for info) = 90.18 %
[241]	validation_0-merror:0.063243	validation_1-merror:0.098318	validation_0-target_score:-0.870585	validation_1-target_score:-0.739816
target score = 87.13 %
accuracy (just for info) = 93.71 %
target score = 74.00 %
accuracy (just for info) = 90.18 %
[242]	validation_0-merror:0.063047	validation_1-merror:0.098373	validation_0-target_score:-0.871309	validation_1-target_score:-0.74004
target score = 87.15 %
accuracy (just for info) = 93.71 %
target score = 73.91 %
accuracy (just for info) = 90.18 %
[243]	validation_0-merror:0.063047	validation_1-merror:0.098485	validation_0-target_score:-0.871476	validation_1-target_score:-0.739145
target score = 87.21 %
accuracy (just for info) = 93.73 %
target score = 73.95 %
accuracy (just for info) = 90.16 %
[244]	validation_0-merror:0.06285	validation_1-merror:0.098541	validation_0-target_score:-0.872145	validation_1-target_score:-0.739481
target score = 87.23 %
accuracy (just for info) = 93.76 %
target score = 74.03 %
accuracy (just for info) = 90.19 %
[245]	validation_0-merror:0.06257	validation_1-merror:0.098262	validation_0-target_score:-0.872312	validation_1-target_score:-0.740264
target score = 87.22 %
accuracy (just for info) = 93.77 %
target score = 74.08 %
accuracy (just for info) = 90.18 %
[246]	validation_0-merror:0.06243	validation_1-merror:0.098262	validation_0-target_score:-0.872201	validation_1-target_score:-0.740824
target score = 87.20 %
accuracy (just for info) = 93.80 %
target score = 73.97 %
accuracy (just for info) = 90.21 %
[247]	validation_0-merror:0.062065	validation_1-merror:0.097982	validation_0-target_score:-0.871978	validation_1-target_score:-0.739705
target score = 87.42 %
accuracy (just for info) = 93.82 %
target score = 73.89 %
accuracy (just for info) = 90.20 %
[248]	validation_0-merror:0.061953	validation_1-merror:0.09815	validation_0-target_score:-0.87415	validation_1-target_score:-0.738921
target score = 87.41 %
accuracy (just for info) = 93.83 %
target score = 73.88 %
accuracy (just for info) = 90.20 %
[249]	validation_0-merror:0.061813	validation_1-merror:0.098094	validation_0-target_score:-0.874095	validation_1-target_score:-0.738809
target score = 87.46 %
accuracy (just for info) = 93.84 %
target score = 73.90 %
accuracy (just for info) = 90.18 %
[250]	validation_0-merror:0.061757	validation_1-merror:0.098262	validation_0-target_score:-0.874596	validation_1-target_score:-0.739033
target score = 87.51 %
accuracy (just for info) = 93.85 %
target score = 73.98 %
accuracy (just for info) = 90.19 %
[251]	validation_0-merror:0.061645	validation_1-merror:0.09815	validation_0-target_score:-0.875097	validation_1-target_score:-0.739816
target score = 87.51 %
accuracy (just for info) = 93.86 %
target score = 73.82 %
accuracy (just for info) = 90.17 %
[252]	validation_0-merror:0.061533	validation_1-merror:0.098373	validation_0-target_score:-0.875097	validation_1-target_score:-0.73825
target score = 87.56 %
accuracy (just for info) = 93.87 %
target score = 73.82 %
accuracy (just for info) = 90.17 %
[253]	validation_0-merror:0.061477	validation_1-merror:0.098485	validation_0-target_score:-0.875599	validation_1-target_score:-0.73825
target score = 87.63 %
accuracy (just for info) = 93.89 %
target score = 73.80 %
accuracy (just for info) = 90.17 %
[254]	validation_0-merror:0.061281	validation_1-merror:0.098429	validation_0-target_score:-0.876323	validation_1-target_score:-0.738026
target score = 87.68 %
accuracy (just for info) = 93.90 %
target score = 73.87 %
accuracy (just for info) = 90.17 %
[255]	validation_0-merror:0.06114	validation_1-merror:0.098429	validation_0-target_score:-0.876825	validation_1-target_score:-0.738697
target score = 87.68 %
accuracy (just for info) = 93.91 %
target score = 73.84 %
accuracy (just for info) = 90.17 %
[256]	validation_0-merror:0.061028	validation_1-merror:0.098373	validation_0-target_score:-0.876769	validation_1-target_score:-0.738362
target score = 87.79 %
accuracy (just for info) = 93.94 %
target score = 73.84 %
accuracy (just for info) = 90.20 %
[257]	validation_0-merror:0.060832	validation_1-merror:0.098038	validation_0-target_score:-0.877883	validation_1-target_score:-0.738362
target score = 87.92 %
accuracy (just for info) = 93.95 %
target score = 73.85 %
accuracy (just for info) = 90.18 %
[258]	validation_0-merror:0.060692	validation_1-merror:0.098262	validation_0-target_score:-0.87922	validation_1-target_score:-0.738474
target score = 87.97 %
accuracy (just for info) = 93.98 %
target score = 73.85 %
accuracy (just for info) = 90.18 %
[259]	validation_0-merror:0.060355	validation_1-merror:0.098262	validation_0-target_score:-0.879666	validation_1-target_score:-0.738474
target score = 88.01 %
accuracy (just for info) = 94.00 %
target score = 73.77 %
accuracy (just for info) = 90.20 %
[260]	validation_0-merror:0.060103	validation_1-merror:0.09815	validation_0-target_score:-0.880056	validation_1-target_score:-0.73769
target score = 88.08 %
accuracy (just for info) = 94.03 %
target score = 73.85 %
accuracy (just for info) = 90.20 %
[261]	validation_0-merror:0.059767	validation_1-merror:0.098206	validation_0-target_score:-0.880836	validation_1-target_score:-0.738474
target score = 88.09 %
accuracy (just for info) = 94.01 %
target score = 73.85 %
accuracy (just for info) = 90.19 %
[262]	validation_0-merror:0.059963	validation_1-merror:0.098318	validation_0-target_score:-0.880947	validation_1-target_score:-0.738474
target score = 88.14 %
accuracy (just for info) = 94.03 %
target score = 73.90 %
accuracy (just for info) = 90.18 %
[263]	validation_0-merror:0.059767	validation_1-merror:0.098485	validation_0-target_score:-0.881448	validation_1-target_score:-0.739033
target score = 88.19 %
accuracy (just for info) = 94.07 %
target score = 73.86 %
accuracy (just for info) = 90.16 %
[264]	validation_0-merror:0.059486	validation_1-merror:0.098485	validation_0-target_score:-0.88195	validation_1-target_score:-0.738585
target score = 88.23 %
accuracy (just for info) = 94.07 %
target score = 73.93 %
accuracy (just for info) = 90.17 %
[265]	validation_0-merror:0.059402	validation_1-merror:0.098429	validation_0-target_score:-0.882284	validation_1-target_score:-0.739257
target score = 88.23 %
accuracy (just for info) = 94.08 %
target score = 73.93 %
accuracy (just for info) = 90.17 %
[266]	validation_0-merror:0.059346	validation_1-merror:0.098318	validation_0-target_score:-0.882284	validation_1-target_score:-0.739257
target score = 88.23 %
accuracy (just for info) = 94.08 %
target score = 73.94 %
accuracy (just for info) = 90.19 %
[267]	validation_0-merror:0.059374	validation_1-merror:0.098206	validation_0-target_score:-0.88234	validation_1-target_score:-0.739369
target score = 88.31 %
accuracy (just for info) = 94.10 %
target score = 73.90 %
accuracy (just for info) = 90.21 %
[268]	validation_0-merror:0.05915	validation_1-merror:0.098262	validation_0-target_score:-0.88312	validation_1-target_score:-0.739033
target score = 88.31 %
accuracy (just for info) = 94.11 %
target score = 73.93 %
accuracy (just for info) = 90.19 %
[269]	validation_0-merror:0.059038	validation_1-merror:0.09815	validation_0-target_score:-0.883064	validation_1-target_score:-0.739257
target score = 88.32 %
accuracy (just for info) = 94.11 %
target score = 73.85 %
accuracy (just for info) = 90.20 %
[270]	validation_0-merror:0.059038	validation_1-merror:0.098094	validation_0-target_score:-0.883231	validation_1-target_score:-0.738474
target score = 88.38 %
accuracy (just for info) = 94.16 %
target score = 74.05 %
accuracy (just for info) = 90.20 %
[271]	validation_0-merror:0.058561	validation_1-merror:0.098038	validation_0-target_score:-0.883788	validation_1-target_score:-0.740488
target score = 88.42 %
accuracy (just for info) = 94.16 %
target score = 73.96 %
accuracy (just for info) = 90.21 %
[272]	validation_0-merror:0.058477	validation_1-merror:0.097982	validation_0-target_score:-0.884234	validation_1-target_score:-0.739593
target score = 88.52 %
accuracy (just for info) = 94.18 %
target score = 73.96 %
accuracy (just for info) = 90.21 %
[273]	validation_0-merror:0.058281	validation_1-merror:0.097982	validation_0-target_score:-0.885181	validation_1-target_score:-0.739593
target score = 88.54 %
accuracy (just for info) = 94.23 %
target score = 74.02 %
accuracy (just for info) = 90.20 %
[274]	validation_0-merror:0.057804	validation_1-merror:0.09815	validation_0-target_score:-0.885404	validation_1-target_score:-0.740152
target score = 88.62 %
accuracy (just for info) = 94.24 %
target score = 74.05 %
accuracy (just for info) = 90.20 %
[275]	validation_0-merror:0.057636	validation_1-merror:0.09815	validation_0-target_score:-0.88624	validation_1-target_score:-0.740488
target score = 88.65 %
accuracy (just for info) = 94.25 %
target score = 74.05 %
accuracy (just for info) = 90.20 %
[276]	validation_0-merror:0.057608	validation_1-merror:0.098206	validation_0-target_score:-0.886462	validation_1-target_score:-0.740488
target score = 88.69 %
accuracy (just for info) = 94.29 %
target score = 73.91 %
accuracy (just for info) = 90.19 %
[277]	validation_0-merror:0.057188	validation_1-merror:0.09815	validation_0-target_score:-0.886908	validation_1-target_score:-0.739145
target score = 88.71 %
accuracy (just for info) = 94.30 %
target score = 73.93 %
accuracy (just for info) = 90.18 %
[278]	validation_0-merror:0.057076	validation_1-merror:0.098262	validation_0-target_score:-0.887131	validation_1-target_score:-0.739257
target score = 88.74 %
accuracy (just for info) = 94.31 %
target score = 73.93 %
accuracy (just for info) = 90.16 %
[279]	validation_0-merror:0.056963	validation_1-merror:0.098429	validation_0-target_score:-0.887409	validation_1-target_score:-0.739257
target score = 88.84 %
accuracy (just for info) = 94.32 %
target score = 74.08 %
accuracy (just for info) = 90.17 %
[280]	validation_0-merror:0.056851	validation_1-merror:0.098373	validation_0-target_score:-0.888357	validation_1-target_score:-0.740824
target score = 88.85 %
accuracy (just for info) = 94.33 %
target score = 73.91 %
accuracy (just for info) = 90.17 %
[281]	validation_0-merror:0.056739	validation_1-merror:0.098429	validation_0-target_score:-0.888524	validation_1-target_score:-0.739145
target score = 88.92 %
accuracy (just for info) = 94.33 %
target score = 73.82 %
accuracy (just for info) = 90.17 %
[282]	validation_0-merror:0.056739	validation_1-merror:0.098318	validation_0-target_score:-0.889192	validation_1-target_score:-0.73825
target score = 88.96 %
accuracy (just for info) = 94.34 %
target score = 73.86 %
accuracy (just for info) = 90.20 %
[283]	validation_0-merror:0.056655	validation_1-merror:0.09815	validation_0-target_score:-0.889638	validation_1-target_score:-0.738585
target score = 88.95 %
accuracy (just for info) = 94.35 %
target score = 73.96 %
accuracy (just for info) = 90.20 %
[284]	validation_0-merror:0.056655	validation_1-merror:0.09815	validation_0-target_score:-0.889526	validation_1-target_score:-0.739593
target score = 89.00 %
accuracy (just for info) = 94.34 %
target score = 73.87 %
accuracy (just for info) = 90.20 %
[285]	validation_0-merror:0.056739	validation_1-merror:0.098094	validation_0-target_score:-0.889972	validation_1-target_score:-0.738697
target score = 89.01 %
accuracy (just for info) = 94.35 %
target score = 74.02 %
accuracy (just for info) = 90.19 %
[286]	validation_0-merror:0.056683	validation_1-merror:0.098094	validation_0-target_score:-0.890084	validation_1-target_score:-0.740152
target score = 89.06 %
accuracy (just for info) = 94.37 %
target score = 73.88 %
accuracy (just for info) = 90.19 %
[287]	validation_0-merror:0.056403	validation_1-merror:0.098318	validation_0-target_score:-0.890641	validation_1-target_score:-0.738809
target score = 89.16 %
accuracy (just for info) = 94.39 %
target score = 73.77 %
accuracy (just for info) = 90.18 %
[288]	validation_0-merror:0.056179	validation_1-merror:0.098318	validation_0-target_score:-0.891588	validation_1-target_score:-0.73769
target score = 89.21 %
accuracy (just for info) = 94.41 %
target score = 73.81 %
accuracy (just for info) = 90.19 %
[289]	validation_0-merror:0.056038	validation_1-merror:0.098373	validation_0-target_score:-0.892145	validation_1-target_score:-0.738138
target score = 89.26 %
accuracy (just for info) = 94.43 %
target score = 73.85 %
accuracy (just for info) = 90.19 %
[290]	validation_0-merror:0.05587	validation_1-merror:0.098541	validation_0-target_score:-0.892591	validation_1-target_score:-0.738474
target score = 89.31 %
accuracy (just for info) = 94.43 %
target score = 73.80 %
accuracy (just for info) = 90.18 %
[291]	validation_0-merror:0.055842	validation_1-merror:0.098206	validation_0-target_score:-0.893092	validation_1-target_score:-0.738026
target score = 89.31 %
accuracy (just for info) = 94.44 %
target score = 73.75 %
accuracy (just for info) = 90.19 %
[292]	validation_0-merror:0.055786	validation_1-merror:0.098206	validation_0-target_score:-0.893092	validation_1-target_score:-0.737466
target score = 89.37 %
accuracy (just for info) = 94.43 %
target score = 73.74 %
accuracy (just for info) = 90.18 %
[293]	validation_0-merror:0.055842	validation_1-merror:0.098206	validation_0-target_score:-0.893705	validation_1-target_score:-0.737355
target score = 89.38 %
accuracy (just for info) = 94.43 %
target score = 73.65 %
accuracy (just for info) = 90.19 %
[294]	validation_0-merror:0.055814	validation_1-merror:0.098094	validation_0-target_score:-0.893816	validation_1-target_score:-0.736459
target score = 89.42 %
accuracy (just for info) = 94.44 %
target score = 73.85 %
accuracy (just for info) = 90.17 %
[295]	validation_0-merror:0.05573	validation_1-merror:0.098262	validation_0-target_score:-0.89415	validation_1-target_score:-0.738474
target score = 89.45 %
accuracy (just for info) = 94.46 %
target score = 73.86 %
accuracy (just for info) = 90.17 %
[296]	validation_0-merror:0.055478	validation_1-merror:0.098318	validation_0-target_score:-0.894485	validation_1-target_score:-0.738585
Stopping. Best iteration:
[246]	validation_0-merror:0.06243	validation_1-merror:0.098262	validation_0-target_score:-0.872201	validation_1-target_score:-0.740824
[0, 1, 2]
0.240175689553
35781 17782
['a_bi_freq_suff' 'a_three_freq_suff' 'a_four_freq_suff' 'a_five_freq_suff'
 'a_bi_freq_pref' 'a_three_freq_pref' 'a_four_freq_pref' 'a_five_freq_pref'
 'a_sn_gram_freq' 'the_bi_freq_suff' 'the_three_freq_suff'
 'the_four_freq_suff' 'the_five_freq_suff' 'the_bi_freq_pref'
 'the_three_freq_pref' 'the_four_freq_pref' 'the_five_freq_pref'
 'the_sn_gram_freq' 'a_the_bi_freq_suff_p1' 'the_a_bi_freq_suff_p1'
 'a_the_three_freq_suff_p1' 'the_a_three_freq_suff_p1'
 'a_the_four_freq_suff_p1' 'the_a_four_freq_suff_p1'
 'a_the_five_freq_suff_p1' 'the_a_five_freq_suff_p1'
 'a_the_bi_freq_pref_p1' 'the_a_bi_freq_pref_p1' 'a_the_three_freq_pref_p1'
 'the_a_three_freq_pref_p1' 'a_the_four_freq_pref_p1'
 'the_a_four_freq_pref_p1' 'a_the_five_freq_pref_p1'
 'the_a_five_freq_pref_p1' 'a_the_sn_ngram_freq_p1'
 'the_a_sn_ngram_freq_p1' 'a_the_bi_freq_suff_p10' 'the_a_bi_freq_suff_p10'
 'a_the_three_freq_suff_p10' 'the_a_three_freq_suff_p10'
 'a_the_four_freq_suff_p10' 'the_a_four_freq_suff_p10'
 'a_the_five_freq_suff_p10' 'the_a_five_freq_suff_p10'
 'a_the_bi_freq_pref_p10' 'the_a_bi_freq_pref_p10'
 'a_the_three_freq_pref_p10' 'the_a_three_freq_pref_p10'
 'a_the_four_freq_pref_p10' 'the_a_four_freq_pref_p10'
 'a_the_five_freq_pref_p10' 'the_a_five_freq_pref_p10'
 'a_the_sn_ngram_freq_p10' 'the_a_sn_ngram_freq_p10'
 'a_the_bi_freq_suff_p100' 'the_a_bi_freq_suff_p100'
 'a_the_three_freq_suff_p100' 'the_a_three_freq_suff_p100'
 'a_the_four_freq_suff_p100' 'the_a_four_freq_suff_p100'
 'a_the_five_freq_suff_p100' 'the_a_five_freq_suff_p100'
 'a_the_bi_freq_pref_p100' 'the_a_bi_freq_pref_p100'
 'a_the_three_freq_pref_p100' 'the_a_three_freq_pref_p100'
 'a_the_four_freq_pref_p100' 'the_a_four_freq_pref_p100'
 'a_the_five_freq_pref_p100' 'the_a_five_freq_pref_p100'
 'a_the_sn_ngram_freq_p100' 'the_a_sn_ngram_freq_p100' 'st_with_v'
 'article']
target score = 65.73 %
accuracy (just for info) = 88.00 %
target score = 66.29 %
accuracy (just for info) = 88.08 %
[0]	validation_0-merror:0.120846	validation_1-merror:0.124395	validation_0-target_score:-0.657264	validation_1-target_score:-0.662852
Multiple eval metrics have been passed: 'validation_1-target_score' will be used for early stopping.
Will train until validation_1-target_score hasn't improved in 50 rounds.
target score = 66.01 %
accuracy (just for info) = 89.06 %
target score = 65.84 %
accuracy (just for info) = 88.79 %
[1]	validation_0-merror:0.109472	validation_1-merror:0.112136	validation_0-target_score:-0.660101	validation_1-target_score:-0.658364
target score = 68.09 %
accuracy (just for info) = 89.02 %
target score = 68.11 %
accuracy (just for info) = 88.76 %
[2]	validation_0-merror:0.109835	validation_1-merror:0.112473	validation_0-target_score:-0.68091	validation_1-target_score:-0.68114
target score = 68.46 %
accuracy (just for info) = 89.23 %
target score = 67.44 %
accuracy (just for info) = 88.75 %
[3]	validation_0-merror:0.107683	validation_1-merror:0.11253	validation_0-target_score:-0.684582	validation_1-target_score:-0.674408
target score = 69.15 %
accuracy (just for info) = 89.32 %
target score = 68.93 %
accuracy (just for info) = 88.87 %
[4]	validation_0-merror:0.106789	validation_1-merror:0.111292	validation_0-target_score:-0.691537	validation_1-target_score:-0.68933
target score = 69.67 %
accuracy (just for info) = 89.56 %
target score = 68.87 %
accuracy (just for info) = 89.02 %
[5]	validation_0-merror:0.104441	validation_1-merror:0.10983	validation_0-target_score:-0.696656	validation_1-target_score:-0.688657
target score = 70.05 %
accuracy (just for info) = 89.62 %
target score = 69.35 %
accuracy (just for info) = 89.17 %
[6]	validation_0-merror:0.103798	validation_1-merror:0.108256	validation_0-target_score:-0.700495	validation_1-target_score:-0.693481
target score = 70.32 %
accuracy (just for info) = 89.72 %
target score = 69.39 %
accuracy (just for info) = 89.23 %
[7]	validation_0-merror:0.10282	validation_1-merror:0.107806	validation_0-target_score:-0.703166	validation_1-target_score:-0.69393
target score = 70.31 %
accuracy (just for info) = 89.61 %
target score = 68.97 %
accuracy (just for info) = 89.14 %
[8]	validation_0-merror:0.104022	validation_1-merror:0.108874	validation_0-target_score:-0.703055	validation_1-target_score:-0.689667
target score = 70.53 %
accuracy (just for info) = 89.66 %
target score = 69.51 %
accuracy (just for info) = 89.11 %
[9]	validation_0-merror:0.103463	validation_1-merror:0.109212	validation_0-target_score:-0.70528	validation_1-target_score:-0.695052
target score = 70.57 %
accuracy (just for info) = 89.70 %
target score = 69.90 %
accuracy (just for info) = 89.19 %
[10]	validation_0-merror:0.103071	validation_1-merror:0.108537	validation_0-target_score:-0.70567	validation_1-target_score:-0.698979
target score = 70.87 %
accuracy (just for info) = 89.70 %
target score = 70.16 %
accuracy (just for info) = 89.26 %
[11]	validation_0-merror:0.103099	validation_1-merror:0.107468	validation_0-target_score:-0.708674	validation_1-target_score:-0.70156
target score = 70.96 %
accuracy (just for info) = 89.85 %
target score = 70.30 %
accuracy (just for info) = 89.32 %
[12]	validation_0-merror:0.10159	validation_1-merror:0.106962	validation_0-target_score:-0.70962	validation_1-target_score:-0.703018
target score = 71.30 %
accuracy (just for info) = 89.92 %
target score = 70.29 %
accuracy (just for info) = 89.35 %
[13]	validation_0-merror:0.100808	validation_1-merror:0.106568	validation_0-target_score:-0.712958	validation_1-target_score:-0.702906
target score = 71.87 %
accuracy (just for info) = 90.00 %
target score = 70.62 %
accuracy (just for info) = 89.42 %
[14]	validation_0-merror:0.100053	validation_1-merror:0.105837	validation_0-target_score:-0.718745	validation_1-target_score:-0.70616
target score = 71.73 %
accuracy (just for info) = 90.04 %
target score = 71.05 %
accuracy (just for info) = 89.52 %
[15]	validation_0-merror:0.099662	validation_1-merror:0.104825	validation_0-target_score:-0.717298	validation_1-target_score:-0.710535
target score = 72.05 %
accuracy (just for info) = 90.11 %
target score = 71.06 %
accuracy (just for info) = 89.58 %
[16]	validation_0-merror:0.098935	validation_1-merror:0.104263	validation_0-target_score:-0.72047	validation_1-target_score:-0.710647
target score = 72.26 %
accuracy (just for info) = 90.10 %
target score = 71.35 %
accuracy (just for info) = 89.60 %
[17]	validation_0-merror:0.098991	validation_1-merror:0.104038	validation_0-target_score:-0.72264	validation_1-target_score:-0.713452
target score = 72.39 %
accuracy (just for info) = 90.12 %
target score = 71.49 %
accuracy (just for info) = 89.64 %
[18]	validation_0-merror:0.098823	validation_1-merror:0.103644	validation_0-target_score:-0.723919	validation_1-target_score:-0.714911
target score = 72.44 %
accuracy (just for info) = 90.18 %
target score = 71.61 %
accuracy (just for info) = 89.67 %
[19]	validation_0-merror:0.09832	validation_1-merror:0.10325	validation_0-target_score:-0.724364	validation_1-target_score:-0.716145
target score = 72.42 %
accuracy (just for info) = 90.22 %
target score = 71.67 %
accuracy (just for info) = 89.65 %
[20]	validation_0-merror:0.097789	validation_1-merror:0.103532	validation_0-target_score:-0.724197	validation_1-target_score:-0.716706
target score = 72.63 %
accuracy (just for info) = 90.24 %
target score = 71.67 %
accuracy (just for info) = 89.68 %
[21]	validation_0-merror:0.097678	validation_1-merror:0.103194	validation_0-target_score:-0.726312	validation_1-target_score:-0.716706
target score = 72.85 %
accuracy (just for info) = 90.30 %
target score = 71.79 %
accuracy (just for info) = 89.71 %
[22]	validation_0-merror:0.097091	validation_1-merror:0.102913	validation_0-target_score:-0.728482	validation_1-target_score:-0.71794
target score = 72.90 %
accuracy (just for info) = 90.32 %
target score = 71.84 %
accuracy (just for info) = 89.78 %
[23]	validation_0-merror:0.096783	validation_1-merror:0.102182	validation_0-target_score:-0.729038	validation_1-target_score:-0.718389
target score = 73.20 %
accuracy (just for info) = 90.35 %
target score = 71.92 %
accuracy (just for info) = 89.78 %
[24]	validation_0-merror:0.096504	validation_1-merror:0.102182	validation_0-target_score:-0.732043	validation_1-target_score:-0.719174
target score = 73.40 %
accuracy (just for info) = 90.34 %
target score = 72.04 %
accuracy (just for info) = 89.83 %
[25]	validation_0-merror:0.096671	validation_1-merror:0.101788	validation_0-target_score:-0.73399	validation_1-target_score:-0.720408
target score = 73.60 %
accuracy (just for info) = 90.36 %
target score = 71.87 %
accuracy (just for info) = 89.85 %
[26]	validation_0-merror:0.096392	validation_1-merror:0.101451	validation_0-target_score:-0.735993	validation_1-target_score:-0.718725
target score = 73.76 %
accuracy (just for info) = 90.36 %
target score = 72.19 %
accuracy (just for info) = 89.88 %
[27]	validation_0-merror:0.09642	validation_1-merror:0.101226	validation_0-target_score:-0.737606	validation_1-target_score:-0.721867
target score = 73.77 %
accuracy (just for info) = 90.38 %
target score = 72.31 %
accuracy (just for info) = 89.89 %
[28]	validation_0-merror:0.096308	validation_1-merror:0.101057	validation_0-target_score:-0.737718	validation_1-target_score:-0.723101
target score = 73.94 %
accuracy (just for info) = 90.43 %
target score = 72.30 %
accuracy (just for info) = 89.93 %
[29]	validation_0-merror:0.095721	validation_1-merror:0.10072	validation_0-target_score:-0.739442	validation_1-target_score:-0.722989
target score = 74.17 %
accuracy (just for info) = 90.41 %
target score = 72.44 %
accuracy (just for info) = 89.98 %
[30]	validation_0-merror:0.095917	validation_1-merror:0.10027	validation_0-target_score:-0.741724	validation_1-target_score:-0.724447
target score = 74.18 %
accuracy (just for info) = 90.43 %
target score = 72.44 %
accuracy (just for info) = 89.98 %
[31]	validation_0-merror:0.095693	validation_1-merror:0.10027	validation_0-target_score:-0.741779	validation_1-target_score:-0.724447
target score = 74.28 %
accuracy (just for info) = 90.45 %
target score = 72.56 %
accuracy (just for info) = 89.97 %
[32]	validation_0-merror:0.095554	validation_1-merror:0.100326	validation_0-target_score:-0.742781	validation_1-target_score:-0.725569
target score = 74.43 %
accuracy (just for info) = 90.49 %
target score = 72.57 %
accuracy (just for info) = 89.97 %
[33]	validation_0-merror:0.095106	validation_1-merror:0.100382	validation_0-target_score:-0.744339	validation_1-target_score:-0.725682
target score = 74.45 %
accuracy (just for info) = 90.53 %
target score = 72.44 %
accuracy (just for info) = 89.99 %
[34]	validation_0-merror:0.094911	validation_1-merror:0.100157	validation_0-target_score:-0.74445	validation_1-target_score:-0.724447
target score = 74.52 %
accuracy (just for info) = 90.52 %
target score = 72.52 %
accuracy (just for info) = 90.00 %
[35]	validation_0-merror:0.094855	validation_1-merror:0.100101	validation_0-target_score:-0.745173	validation_1-target_score:-0.725233
target score = 74.58 %
accuracy (just for info) = 90.54 %
target score = 72.61 %
accuracy (just for info) = 90.01 %
[36]	validation_0-merror:0.094659	validation_1-merror:0.099876	validation_0-target_score:-0.745785	validation_1-target_score:-0.72613
target score = 74.63 %
accuracy (just for info) = 90.55 %
target score = 72.74 %
accuracy (just for info) = 90.05 %
[37]	validation_0-merror:0.094547	validation_1-merror:0.099539	validation_0-target_score:-0.746286	validation_1-target_score:-0.727365
target score = 74.72 %
accuracy (just for info) = 90.56 %
target score = 72.79 %
accuracy (just for info) = 90.05 %
[38]	validation_0-merror:0.094464	validation_1-merror:0.099539	validation_0-target_score:-0.747232	validation_1-target_score:-0.727926
target score = 74.76 %
accuracy (just for info) = 90.59 %
target score = 72.87 %
accuracy (just for info) = 90.05 %
[39]	validation_0-merror:0.094156	validation_1-merror:0.099539	validation_0-target_score:-0.747566	validation_1-target_score:-0.728711
target score = 74.76 %
accuracy (just for info) = 90.60 %
target score = 72.88 %
accuracy (just for info) = 90.05 %
[40]	validation_0-merror:0.094044	validation_1-merror:0.099483	validation_0-target_score:-0.747621	validation_1-target_score:-0.728823
target score = 74.84 %
accuracy (just for info) = 90.60 %
target score = 72.93 %
accuracy (just for info) = 90.12 %
[41]	validation_0-merror:0.093988	validation_1-merror:0.098808	validation_0-target_score:-0.7484	validation_1-target_score:-0.729272
target score = 74.98 %
accuracy (just for info) = 90.63 %
target score = 73.04 %
accuracy (just for info) = 90.06 %
[42]	validation_0-merror:0.093709	validation_1-merror:0.099426	validation_0-target_score:-0.749791	validation_1-target_score:-0.730394
target score = 75.01 %
accuracy (just for info) = 90.62 %
target score = 73.03 %
accuracy (just for info) = 90.10 %
[43]	validation_0-merror:0.093877	validation_1-merror:0.098976	validation_0-target_score:-0.75007	validation_1-target_score:-0.730282
target score = 75.13 %
accuracy (just for info) = 90.63 %
target score = 73.12 %
accuracy (just for info) = 90.11 %
[44]	validation_0-merror:0.093709	validation_1-merror:0.09892	validation_0-target_score:-0.751349	validation_1-target_score:-0.731179
target score = 75.14 %
accuracy (just for info) = 90.63 %
target score = 73.04 %
accuracy (just for info) = 90.12 %
[45]	validation_0-merror:0.093737	validation_1-merror:0.098752	validation_0-target_score:-0.751405	validation_1-target_score:-0.730394
target score = 75.27 %
accuracy (just for info) = 90.63 %
target score = 73.14 %
accuracy (just for info) = 90.13 %
[46]	validation_0-merror:0.093709	validation_1-merror:0.098695	validation_0-target_score:-0.752685	validation_1-target_score:-0.731404
target score = 75.36 %
accuracy (just for info) = 90.69 %
target score = 73.07 %
accuracy (just for info) = 90.10 %
[47]	validation_0-merror:0.093094	validation_1-merror:0.098976	validation_0-target_score:-0.753575	validation_1-target_score:-0.73073
target score = 75.40 %
accuracy (just for info) = 90.69 %
target score = 73.11 %
accuracy (just for info) = 90.12 %
[48]	validation_0-merror:0.09315	validation_1-merror:0.098864	validation_0-target_score:-0.75402	validation_1-target_score:-0.731067
target score = 75.50 %
accuracy (just for info) = 90.68 %
target score = 73.21 %
accuracy (just for info) = 90.14 %
[49]	validation_0-merror:0.093178	validation_1-merror:0.098583	validation_0-target_score:-0.754966	validation_1-target_score:-0.732077
target score = 75.58 %
accuracy (just for info) = 90.69 %
target score = 73.21 %
accuracy (just for info) = 90.14 %
[50]	validation_0-merror:0.093066	validation_1-merror:0.098639	validation_0-target_score:-0.7558	validation_1-target_score:-0.732077
target score = 75.60 %
accuracy (just for info) = 90.71 %
target score = 73.20 %
accuracy (just for info) = 90.10 %
[51]	validation_0-merror:0.092898	validation_1-merror:0.098976	validation_0-target_score:-0.756023	validation_1-target_score:-0.731965
target score = 75.69 %
accuracy (just for info) = 90.72 %
target score = 73.15 %
accuracy (just for info) = 90.12 %
[52]	validation_0-merror:0.092843	validation_1-merror:0.098808	validation_0-target_score:-0.756858	validation_1-target_score:-0.731516
target score = 75.72 %
accuracy (just for info) = 90.71 %
target score = 73.11 %
accuracy (just for info) = 90.15 %
[53]	validation_0-merror:0.092871	validation_1-merror:0.098527	validation_0-target_score:-0.757191	validation_1-target_score:-0.731067
target score = 75.77 %
accuracy (just for info) = 90.73 %
target score = 73.16 %
accuracy (just for info) = 90.16 %
[54]	validation_0-merror:0.092731	validation_1-merror:0.098414	validation_0-target_score:-0.757692	validation_1-target_score:-0.731628
target score = 75.89 %
accuracy (just for info) = 90.75 %
target score = 73.44 %
accuracy (just for info) = 90.14 %
[55]	validation_0-merror:0.092535	validation_1-merror:0.098639	validation_0-target_score:-0.758861	validation_1-target_score:-0.734433
target score = 75.98 %
accuracy (just for info) = 90.76 %
target score = 73.57 %
accuracy (just for info) = 90.14 %
[56]	validation_0-merror:0.092395	validation_1-merror:0.098583	validation_0-target_score:-0.759806	validation_1-target_score:-0.735667
target score = 76.04 %
accuracy (just for info) = 90.78 %
target score = 73.68 %
accuracy (just for info) = 90.16 %
[57]	validation_0-merror:0.0922	validation_1-merror:0.098414	validation_0-target_score:-0.760363	validation_1-target_score:-0.736789
target score = 76.10 %
accuracy (just for info) = 90.79 %
target score = 73.51 %
accuracy (just for info) = 90.20 %
[58]	validation_0-merror:0.092172	validation_1-merror:0.097964	validation_0-target_score:-0.76103	validation_1-target_score:-0.735106
target score = 76.11 %
accuracy (just for info) = 90.79 %
target score = 73.31 %
accuracy (just for info) = 90.20 %
[59]	validation_0-merror:0.092116	validation_1-merror:0.09802	validation_0-target_score:-0.761086	validation_1-target_score:-0.733087
target score = 76.19 %
accuracy (just for info) = 90.79 %
target score = 73.30 %
accuracy (just for info) = 90.21 %
[60]	validation_0-merror:0.092228	validation_1-merror:0.097908	validation_0-target_score:-0.761921	validation_1-target_score:-0.732974
target score = 76.24 %
accuracy (just for info) = 90.79 %
target score = 73.41 %
accuracy (just for info) = 90.21 %
[61]	validation_0-merror:0.092144	validation_1-merror:0.097852	validation_0-target_score:-0.762366	validation_1-target_score:-0.734096
target score = 76.34 %
accuracy (just for info) = 90.82 %
target score = 73.41 %
accuracy (just for info) = 90.21 %
[62]	validation_0-merror:0.091892	validation_1-merror:0.097964	validation_0-target_score:-0.763367	validation_1-target_score:-0.734096
target score = 76.43 %
accuracy (just for info) = 90.84 %
target score = 73.40 %
accuracy (just for info) = 90.21 %
[63]	validation_0-merror:0.091753	validation_1-merror:0.097908	validation_0-target_score:-0.764313	validation_1-target_score:-0.733984
target score = 76.42 %
accuracy (just for info) = 90.87 %
target score = 73.36 %
accuracy (just for info) = 90.22 %
[64]	validation_0-merror:0.091473	validation_1-merror:0.097796	validation_0-target_score:-0.764202	validation_1-target_score:-0.733647
target score = 76.70 %
accuracy (just for info) = 90.86 %
target score = 73.27 %
accuracy (just for info) = 90.21 %
[65]	validation_0-merror:0.091585	validation_1-merror:0.097964	validation_0-target_score:-0.767039	validation_1-target_score:-0.73275
target score = 76.63 %
accuracy (just for info) = 90.88 %
target score = 73.25 %
accuracy (just for info) = 90.23 %
[66]	validation_0-merror:0.091277	validation_1-merror:0.097683	validation_0-target_score:-0.766316	validation_1-target_score:-0.732526
target score = 76.63 %
accuracy (just for info) = 90.89 %
target score = 73.19 %
accuracy (just for info) = 90.24 %
[67]	validation_0-merror:0.091138	validation_1-merror:0.097571	validation_0-target_score:-0.766316	validation_1-target_score:-0.731852
target score = 76.76 %
accuracy (just for info) = 90.90 %
target score = 73.26 %
accuracy (just for info) = 90.22 %
[68]	validation_0-merror:0.091138	validation_1-merror:0.097796	validation_0-target_score:-0.767596	validation_1-target_score:-0.732638
target score = 76.85 %
accuracy (just for info) = 90.91 %
target score = 73.36 %
accuracy (just for info) = 90.24 %
[69]	validation_0-merror:0.09097	validation_1-merror:0.097683	validation_0-target_score:-0.768486	validation_1-target_score:-0.733647
target score = 76.97 %
accuracy (just for info) = 90.93 %
target score = 73.42 %
accuracy (just for info) = 90.25 %
[70]	validation_0-merror:0.090802	validation_1-merror:0.097571	validation_0-target_score:-0.769654	validation_1-target_score:-0.734208
target score = 77.20 %
accuracy (just for info) = 90.94 %
target score = 73.48 %
accuracy (just for info) = 90.28 %
[71]	validation_0-merror:0.090691	validation_1-merror:0.097346	validation_0-target_score:-0.772047	validation_1-target_score:-0.734769
target score = 77.23 %
accuracy (just for info) = 90.96 %
target score = 73.56 %
accuracy (just for info) = 90.31 %
[72]	validation_0-merror:0.090495	validation_1-merror:0.097121	validation_0-target_score:-0.772325	validation_1-target_score:-0.735555
target score = 77.29 %
accuracy (just for info) = 90.99 %
target score = 73.59 %
accuracy (just for info) = 90.31 %
[73]	validation_0-merror:0.090243	validation_1-merror:0.097233	validation_0-target_score:-0.772882	validation_1-target_score:-0.735891
target score = 77.33 %
accuracy (just for info) = 91.00 %
target score = 73.60 %
accuracy (just for info) = 90.30 %
[74]	validation_0-merror:0.090188	validation_1-merror:0.097064	validation_0-target_score:-0.773327	validation_1-target_score:-0.736004
target score = 77.34 %
accuracy (just for info) = 91.02 %
target score = 73.61 %
accuracy (just for info) = 90.32 %
[75]	validation_0-merror:0.089936	validation_1-merror:0.097346	validation_0-target_score:-0.773382	validation_1-target_score:-0.736116
target score = 77.38 %
accuracy (just for info) = 91.02 %
target score = 73.60 %
accuracy (just for info) = 90.30 %
[76]	validation_0-merror:0.089992	validation_1-merror:0.097121	validation_0-target_score:-0.773827	validation_1-target_score:-0.736004
target score = 77.46 %
accuracy (just for info) = 91.04 %
target score = 73.66 %
accuracy (just for info) = 90.34 %
[77]	validation_0-merror:0.089796	validation_1-merror:0.096615	validation_0-target_score:-0.774551	validation_1-target_score:-0.736565
target score = 77.46 %
accuracy (just for info) = 91.05 %
target score = 73.60 %
accuracy (just for info) = 90.36 %
[78]	validation_0-merror:0.089657	validation_1-merror:0.096615	validation_0-target_score:-0.774606	validation_1-target_score:-0.736004
target score = 77.58 %
accuracy (just for info) = 91.10 %
target score = 73.72 %
accuracy (just for info) = 90.35 %
[79]	validation_0-merror:0.089181	validation_1-merror:0.096783	validation_0-target_score:-0.77583	validation_1-target_score:-0.737238
target score = 77.62 %
accuracy (just for info) = 91.13 %
target score = 73.79 %
accuracy (just for info) = 90.37 %
[80]	validation_0-merror:0.088986	validation_1-merror:0.096615	validation_0-target_score:-0.776164	validation_1-target_score:-0.737911
target score = 77.65 %
accuracy (just for info) = 91.13 %
target score = 73.85 %
accuracy (just for info) = 90.36 %
[81]	validation_0-merror:0.089014	validation_1-merror:0.096615	validation_0-target_score:-0.776498	validation_1-target_score:-0.738472
target score = 77.79 %
accuracy (just for info) = 91.15 %
target score = 73.81 %
accuracy (just for info) = 90.33 %
[82]	validation_0-merror:0.08865	validation_1-merror:0.096727	validation_0-target_score:-0.777945	validation_1-target_score:-0.738135
target score = 77.82 %
accuracy (just for info) = 91.17 %
target score = 73.77 %
accuracy (just for info) = 90.35 %
[83]	validation_0-merror:0.088427	validation_1-merror:0.096502	validation_0-target_score:-0.778167	validation_1-target_score:-0.737687
target score = 77.90 %
accuracy (just for info) = 91.19 %
target score = 73.65 %
accuracy (just for info) = 90.36 %
[84]	validation_0-merror:0.088259	validation_1-merror:0.096558	validation_0-target_score:-0.779002	validation_1-target_score:-0.736452
target score = 77.89 %
accuracy (just for info) = 91.19 %
target score = 73.60 %
accuracy (just for info) = 90.36 %
[85]	validation_0-merror:0.088175	validation_1-merror:0.096558	validation_0-target_score:-0.778946	validation_1-target_score:-0.736004
target score = 77.97 %
accuracy (just for info) = 91.21 %
target score = 73.63 %
accuracy (just for info) = 90.37 %
[86]	validation_0-merror:0.088036	validation_1-merror:0.096277	validation_0-target_score:-0.77967	validation_1-target_score:-0.73634
target score = 78.16 %
accuracy (just for info) = 91.23 %
target score = 73.78 %
accuracy (just for info) = 90.40 %
[87]	validation_0-merror:0.08784	validation_1-merror:0.095996	validation_0-target_score:-0.781561	validation_1-target_score:-0.737799
target score = 78.19 %
accuracy (just for info) = 91.25 %
target score = 73.77 %
accuracy (just for info) = 90.43 %
[88]	validation_0-merror:0.087644	validation_1-merror:0.095715	validation_0-target_score:-0.781895	validation_1-target_score:-0.737687
target score = 78.25 %
accuracy (just for info) = 91.27 %
target score = 73.86 %
accuracy (just for info) = 90.43 %
[89]	validation_0-merror:0.087421	validation_1-merror:0.095715	validation_0-target_score:-0.782507	validation_1-target_score:-0.738584
target score = 78.42 %
accuracy (just for info) = 91.29 %
target score = 73.80 %
accuracy (just for info) = 90.38 %
[90]	validation_0-merror:0.087337	validation_1-merror:0.096277	validation_0-target_score:-0.784176	validation_1-target_score:-0.738023
target score = 78.47 %
accuracy (just for info) = 91.32 %
target score = 73.84 %
accuracy (just for info) = 90.40 %
[91]	validation_0-merror:0.086918	validation_1-merror:0.095996	validation_0-target_score:-0.784677	validation_1-target_score:-0.73836
target score = 78.48 %
accuracy (just for info) = 91.34 %
target score = 73.87 %
accuracy (just for info) = 90.40 %
[92]	validation_0-merror:0.086722	validation_1-merror:0.096108	validation_0-target_score:-0.784788	validation_1-target_score:-0.738696
target score = 78.47 %
accuracy (just for info) = 91.35 %
target score = 73.94 %
accuracy (just for info) = 90.39 %
[93]	validation_0-merror:0.08661	validation_1-merror:0.096108	validation_0-target_score:-0.784677	validation_1-target_score:-0.739369
target score = 78.59 %
accuracy (just for info) = 91.36 %
target score = 73.84 %
accuracy (just for info) = 90.41 %
[94]	validation_0-merror:0.086554	validation_1-merror:0.095883	validation_0-target_score:-0.785901	validation_1-target_score:-0.73836
target score = 78.65 %
accuracy (just for info) = 91.39 %
target score = 73.93 %
accuracy (just for info) = 90.41 %
[95]	validation_0-merror:0.086247	validation_1-merror:0.095996	validation_0-target_score:-0.786513	validation_1-target_score:-0.739257
target score = 78.90 %
accuracy (just for info) = 91.39 %
target score = 73.82 %
accuracy (just for info) = 90.41 %
[96]	validation_0-merror:0.086303	validation_1-merror:0.096108	validation_0-target_score:-0.788961	validation_1-target_score:-0.738248
target score = 78.97 %
accuracy (just for info) = 91.42 %
target score = 73.88 %
accuracy (just for info) = 90.42 %
[97]	validation_0-merror:0.086051	validation_1-merror:0.095771	validation_0-target_score:-0.789685	validation_1-target_score:-0.738808
target score = 79.05 %
accuracy (just for info) = 91.42 %
target score = 73.89 %
accuracy (just for info) = 90.43 %
[98]	validation_0-merror:0.085939	validation_1-merror:0.09594	validation_0-target_score:-0.790463	validation_1-target_score:-0.738921
target score = 79.06 %
accuracy (just for info) = 91.44 %
target score = 73.84 %
accuracy (just for info) = 90.43 %
[99]	validation_0-merror:0.085828	validation_1-merror:0.095659	validation_0-target_score:-0.79063	validation_1-target_score:-0.73836
target score = 79.10 %
accuracy (just for info) = 91.45 %
target score = 73.95 %
accuracy (just for info) = 90.45 %
[100]	validation_0-merror:0.085632	validation_1-merror:0.095546	validation_0-target_score:-0.790964	validation_1-target_score:-0.739482
target score = 79.19 %
accuracy (just for info) = 91.46 %
target score = 73.97 %
accuracy (just for info) = 90.44 %
[101]	validation_0-merror:0.085492	validation_1-merror:0.095602	validation_0-target_score:-0.79191	validation_1-target_score:-0.739706
target score = 79.24 %
accuracy (just for info) = 91.46 %
target score = 74.05 %
accuracy (just for info) = 90.41 %
[102]	validation_0-merror:0.085492	validation_1-merror:0.095883	validation_0-target_score:-0.792411	validation_1-target_score:-0.740491
target score = 79.35 %
accuracy (just for info) = 91.48 %
target score = 73.98 %
accuracy (just for info) = 90.40 %
[103]	validation_0-merror:0.085325	validation_1-merror:0.095996	validation_0-target_score:-0.793468	validation_1-target_score:-0.739818
target score = 79.40 %
accuracy (just for info) = 91.49 %
target score = 73.97 %
accuracy (just for info) = 90.42 %
[104]	validation_0-merror:0.085185	validation_1-merror:0.095827	validation_0-target_score:-0.793969	validation_1-target_score:-0.739706
target score = 79.41 %
accuracy (just for info) = 91.51 %
target score = 73.94 %
accuracy (just for info) = 90.43 %
[105]	validation_0-merror:0.084989	validation_1-merror:0.095715	validation_0-target_score:-0.79408	validation_1-target_score:-0.739369
target score = 79.44 %
accuracy (just for info) = 91.52 %
target score = 73.97 %
accuracy (just for info) = 90.46 %
[106]	validation_0-merror:0.084849	validation_1-merror:0.095377	validation_0-target_score:-0.794358	validation_1-target_score:-0.739706
target score = 79.56 %
accuracy (just for info) = 91.53 %
target score = 74.05 %
accuracy (just for info) = 90.47 %
[107]	validation_0-merror:0.084794	validation_1-merror:0.095434	validation_0-target_score:-0.795582	validation_1-target_score:-0.740491
target score = 79.68 %
accuracy (just for info) = 91.54 %
target score = 73.95 %
accuracy (just for info) = 90.48 %
[108]	validation_0-merror:0.084682	validation_1-merror:0.095209	validation_0-target_score:-0.796806	validation_1-target_score:-0.739482
target score = 79.68 %
accuracy (just for info) = 91.54 %
target score = 73.95 %
accuracy (just for info) = 90.49 %
[109]	validation_0-merror:0.084682	validation_1-merror:0.095096	validation_0-target_score:-0.796806	validation_1-target_score:-0.739482
target score = 79.69 %
accuracy (just for info) = 91.55 %
target score = 74.03 %
accuracy (just for info) = 90.47 %
[110]	validation_0-merror:0.084598	validation_1-merror:0.095265	validation_0-target_score:-0.796862	validation_1-target_score:-0.740267
target score = 79.79 %
accuracy (just for info) = 91.59 %
target score = 74.12 %
accuracy (just for info) = 90.47 %
[111]	validation_0-merror:0.084207	validation_1-merror:0.095265	validation_0-target_score:-0.797863	validation_1-target_score:-0.741165
target score = 79.84 %
accuracy (just for info) = 91.60 %
target score = 74.14 %
accuracy (just for info) = 90.46 %
[112]	validation_0-merror:0.084039	validation_1-merror:0.095377	validation_0-target_score:-0.798364	validation_1-target_score:-0.741389
target score = 79.91 %
accuracy (just for info) = 91.63 %
target score = 74.13 %
accuracy (just for info) = 90.46 %
[113]	validation_0-merror:0.08376	validation_1-merror:0.095377	validation_0-target_score:-0.799143	validation_1-target_score:-0.741277
target score = 79.99 %
accuracy (just for info) = 91.64 %
target score = 74.12 %
accuracy (just for info) = 90.47 %
[114]	validation_0-merror:0.083648	validation_1-merror:0.095265	validation_0-target_score:-0.799866	validation_1-target_score:-0.741165
target score = 80.01 %
accuracy (just for info) = 91.65 %
target score = 74.18 %
accuracy (just for info) = 90.47 %
[115]	validation_0-merror:0.08362	validation_1-merror:0.095321	validation_0-target_score:-0.800145	validation_1-target_score:-0.741838
target score = 80.16 %
accuracy (just for info) = 91.68 %
target score = 74.22 %
accuracy (just for info) = 90.48 %
[116]	validation_0-merror:0.083312	validation_1-merror:0.095152	validation_0-target_score:-0.801591	validation_1-target_score:-0.742174
target score = 80.19 %
accuracy (just for info) = 91.69 %
target score = 74.33 %
accuracy (just for info) = 90.47 %
[117]	validation_0-merror:0.083173	validation_1-merror:0.095321	validation_0-target_score:-0.801925	validation_1-target_score:-0.743296
target score = 80.24 %
accuracy (just for info) = 91.71 %
target score = 74.28 %
accuracy (just for info) = 90.45 %
[118]	validation_0-merror:0.083061	validation_1-merror:0.095546	validation_0-target_score:-0.802426	validation_1-target_score:-0.742848
target score = 80.22 %
accuracy (just for info) = 91.74 %
target score = 74.24 %
accuracy (just for info) = 90.48 %
[119]	validation_0-merror:0.082698	validation_1-merror:0.095209	validation_0-target_score:-0.802203	validation_1-target_score:-0.742399
target score = 80.33 %
accuracy (just for info) = 91.75 %
target score = 74.19 %
accuracy (just for info) = 90.47 %
[120]	validation_0-merror:0.08267	validation_1-merror:0.095321	validation_0-target_score:-0.803316	validation_1-target_score:-0.74195
target score = 80.41 %
accuracy (just for info) = 91.76 %
target score = 74.25 %
accuracy (just for info) = 90.47 %
[121]	validation_0-merror:0.082558	validation_1-merror:0.095377	validation_0-target_score:-0.804095	validation_1-target_score:-0.742511
target score = 80.46 %
accuracy (just for info) = 91.79 %
target score = 74.19 %
accuracy (just for info) = 90.51 %
[122]	validation_0-merror:0.082194	validation_1-merror:0.095152	validation_0-target_score:-0.804596	validation_1-target_score:-0.74195
target score = 80.53 %
accuracy (just for info) = 91.81 %
target score = 74.32 %
accuracy (just for info) = 90.55 %
[123]	validation_0-merror:0.082027	validation_1-merror:0.095152	validation_0-target_score:-0.805319	validation_1-target_score:-0.743184
target score = 80.60 %
accuracy (just for info) = 91.80 %
target score = 74.42 %
accuracy (just for info) = 90.54 %
[124]	validation_0-merror:0.082111	validation_1-merror:0.095209	validation_0-target_score:-0.805987	validation_1-target_score:-0.744194
target score = 80.67 %
accuracy (just for info) = 91.80 %
target score = 74.43 %
accuracy (just for info) = 90.55 %
[125]	validation_0-merror:0.082055	validation_1-merror:0.094703	validation_0-target_score:-0.806654	validation_1-target_score:-0.744306
target score = 80.74 %
accuracy (just for info) = 91.83 %
target score = 74.17 %
accuracy (just for info) = 90.57 %
[126]	validation_0-merror:0.081775	validation_1-merror:0.094984	validation_0-target_score:-0.807433	validation_1-target_score:-0.741726
target score = 80.79 %
accuracy (just for info) = 91.84 %
target score = 74.37 %
accuracy (just for info) = 90.56 %
[127]	validation_0-merror:0.081636	validation_1-merror:0.094984	validation_0-target_score:-0.807878	validation_1-target_score:-0.743745
target score = 80.97 %
accuracy (just for info) = 91.86 %
target score = 74.48 %
accuracy (just for info) = 90.56 %
[128]	validation_0-merror:0.081496	validation_1-merror:0.094759	validation_0-target_score:-0.809715	validation_1-target_score:-0.744755
target score = 81.02 %
accuracy (just for info) = 91.88 %
target score = 74.42 %
accuracy (just for info) = 90.54 %
[129]	validation_0-merror:0.081328	validation_1-merror:0.094871	validation_0-target_score:-0.810215	validation_1-target_score:-0.744194
target score = 81.08 %
accuracy (just for info) = 91.88 %
target score = 74.45 %
accuracy (just for info) = 90.56 %
[130]	validation_0-merror:0.0813	validation_1-merror:0.094759	validation_0-target_score:-0.810827	validation_1-target_score:-0.74453
target score = 81.16 %
accuracy (just for info) = 91.89 %
target score = 74.48 %
accuracy (just for info) = 90.57 %
[131]	validation_0-merror:0.081272	validation_1-merror:0.094815	validation_0-target_score:-0.811551	validation_1-target_score:-0.744755
target score = 81.16 %
accuracy (just for info) = 91.91 %
target score = 74.19 %
accuracy (just for info) = 90.56 %
[132]	validation_0-merror:0.081021	validation_1-merror:0.094815	validation_0-target_score:-0.811606	validation_1-target_score:-0.74195
target score = 81.21 %
accuracy (just for info) = 91.91 %
target score = 74.28 %
accuracy (just for info) = 90.54 %
[133]	validation_0-merror:0.081077	validation_1-merror:0.094646	validation_0-target_score:-0.812051	validation_1-target_score:-0.742848
target score = 81.20 %
accuracy (just for info) = 91.91 %
target score = 74.32 %
accuracy (just for info) = 90.55 %
[134]	validation_0-merror:0.080993	validation_1-merror:0.094703	validation_0-target_score:-0.811996	validation_1-target_score:-0.743184
target score = 81.32 %
accuracy (just for info) = 91.93 %
target score = 74.23 %
accuracy (just for info) = 90.55 %
[135]	validation_0-merror:0.080881	validation_1-merror:0.094534	validation_0-target_score:-0.813164	validation_1-target_score:-0.742287
target score = 81.34 %
accuracy (just for info) = 91.93 %
target score = 74.23 %
accuracy (just for info) = 90.56 %
[136]	validation_0-merror:0.080937	validation_1-merror:0.094646	validation_0-target_score:-0.813442	validation_1-target_score:-0.742287
target score = 81.39 %
accuracy (just for info) = 91.95 %
target score = 74.23 %
accuracy (just for info) = 90.54 %
[137]	validation_0-merror:0.080629	validation_1-merror:0.09459	validation_0-target_score:-0.813887	validation_1-target_score:-0.742287
target score = 81.41 %
accuracy (just for info) = 91.97 %
target score = 74.45 %
accuracy (just for info) = 90.56 %
[138]	validation_0-merror:0.080378	validation_1-merror:0.094421	validation_0-target_score:-0.814054	validation_1-target_score:-0.74453
target score = 81.56 %
accuracy (just for info) = 92.00 %
target score = 74.52 %
accuracy (just for info) = 90.54 %
[139]	validation_0-merror:0.080126	validation_1-merror:0.094646	validation_0-target_score:-0.815557	validation_1-target_score:-0.745204
target score = 81.57 %
accuracy (just for info) = 92.02 %
target score = 74.53 %
accuracy (just for info) = 90.55 %
[140]	validation_0-merror:0.079903	validation_1-merror:0.094646	validation_0-target_score:-0.815724	validation_1-target_score:-0.745316
target score = 81.62 %
accuracy (just for info) = 92.03 %
target score = 74.50 %
accuracy (just for info) = 90.56 %
[141]	validation_0-merror:0.079847	validation_1-merror:0.094421	validation_0-target_score:-0.816169	validation_1-target_score:-0.744979
target score = 81.71 %
accuracy (just for info) = 92.05 %
target score = 74.55 %
accuracy (just for info) = 90.57 %
[142]	validation_0-merror:0.079595	validation_1-merror:0.094365	validation_0-target_score:-0.817059	validation_1-target_score:-0.74554
target score = 81.79 %
accuracy (just for info) = 92.06 %
target score = 74.51 %
accuracy (just for info) = 90.55 %
[143]	validation_0-merror:0.079511	validation_1-merror:0.094478	validation_0-target_score:-0.817894	validation_1-target_score:-0.745091
target score = 81.84 %
accuracy (just for info) = 92.07 %
target score = 74.45 %
accuracy (just for info) = 90.55 %
[144]	validation_0-merror:0.0794	validation_1-merror:0.094478	validation_0-target_score:-0.81845	validation_1-target_score:-0.74453
target score = 81.82 %
accuracy (just for info) = 92.08 %
target score = 74.39 %
accuracy (just for info) = 90.54 %
[145]	validation_0-merror:0.07926	validation_1-merror:0.094646	validation_0-target_score:-0.818172	validation_1-target_score:-0.743857
target score = 81.95 %
accuracy (just for info) = 92.09 %
target score = 74.50 %
accuracy (just for info) = 90.54 %
[146]	validation_0-merror:0.079232	validation_1-merror:0.094646	validation_0-target_score:-0.819451	validation_1-target_score:-0.744979
target score = 81.98 %
accuracy (just for info) = 92.11 %
target score = 74.53 %
accuracy (just for info) = 90.55 %
[147]	validation_0-merror:0.07898	validation_1-merror:0.094534	validation_0-target_score:-0.819841	validation_1-target_score:-0.745316
target score = 82.03 %
accuracy (just for info) = 92.12 %
target score = 74.65 %
accuracy (just for info) = 90.56 %
[148]	validation_0-merror:0.078897	validation_1-merror:0.094365	validation_0-target_score:-0.820342	validation_1-target_score:-0.74655
target score = 82.05 %
accuracy (just for info) = 92.14 %
target score = 74.55 %
accuracy (just for info) = 90.55 %
[149]	validation_0-merror:0.078673	validation_1-merror:0.094478	validation_0-target_score:-0.820453	validation_1-target_score:-0.74554
target score = 82.11 %
accuracy (just for info) = 92.19 %
target score = 74.65 %
accuracy (just for info) = 90.61 %
[150]	validation_0-merror:0.078114	validation_1-merror:0.093859	validation_0-target_score:-0.821121	validation_1-target_score:-0.74655
target score = 82.19 %
accuracy (just for info) = 92.21 %
target score = 74.61 %
accuracy (just for info) = 90.60 %
[151]	validation_0-merror:0.077974	validation_1-merror:0.094028	validation_0-target_score:-0.8219	validation_1-target_score:-0.746101
target score = 82.38 %
accuracy (just for info) = 92.24 %
target score = 74.54 %
accuracy (just for info) = 90.63 %
[152]	validation_0-merror:0.077667	validation_1-merror:0.09369	validation_0-target_score:-0.823791	validation_1-target_score:-0.745428
target score = 82.43 %
accuracy (just for info) = 92.25 %
target score = 74.54 %
accuracy (just for info) = 90.62 %
[153]	validation_0-merror:0.077527	validation_1-merror:0.093803	validation_0-target_score:-0.824348	validation_1-target_score:-0.745428
target score = 82.43 %
accuracy (just for info) = 92.27 %
target score = 74.53 %
accuracy (just for info) = 90.63 %
[154]	validation_0-merror:0.077359	validation_1-merror:0.09369	validation_0-target_score:-0.824348	validation_1-target_score:-0.745316
target score = 82.48 %
accuracy (just for info) = 92.27 %
target score = 74.58 %
accuracy (just for info) = 90.63 %
[155]	validation_0-merror:0.077332	validation_1-merror:0.09369	validation_0-target_score:-0.824848	validation_1-target_score:-0.745765
target score = 82.55 %
accuracy (just for info) = 92.28 %
target score = 74.53 %
accuracy (just for info) = 90.61 %
[156]	validation_0-merror:0.077276	validation_1-merror:0.093859	validation_0-target_score:-0.82546	validation_1-target_score:-0.745316
target score = 82.55 %
accuracy (just for info) = 92.29 %
target score = 74.51 %
accuracy (just for info) = 90.61 %
[157]	validation_0-merror:0.077164	validation_1-merror:0.093915	validation_0-target_score:-0.825516	validation_1-target_score:-0.745091
target score = 82.56 %
accuracy (just for info) = 92.29 %
target score = 74.53 %
accuracy (just for info) = 90.60 %
[158]	validation_0-merror:0.07708	validation_1-merror:0.093971	validation_0-target_score:-0.825627	validation_1-target_score:-0.745316
target score = 82.70 %
accuracy (just for info) = 92.34 %
target score = 74.55 %
accuracy (just for info) = 90.60 %
[159]	validation_0-merror:0.076633	validation_1-merror:0.094028	validation_0-target_score:-0.827018	validation_1-target_score:-0.74554
target score = 82.80 %
accuracy (just for info) = 92.36 %
target score = 74.55 %
accuracy (just for info) = 90.59 %
[160]	validation_0-merror:0.076437	validation_1-merror:0.094084	validation_0-target_score:-0.82802	validation_1-target_score:-0.74554
target score = 82.91 %
accuracy (just for info) = 92.36 %
target score = 74.50 %
accuracy (just for info) = 90.62 %
[161]	validation_0-merror:0.076353	validation_1-merror:0.093803	validation_0-target_score:-0.829077	validation_1-target_score:-0.744979
target score = 83.00 %
accuracy (just for info) = 92.39 %
target score = 74.52 %
accuracy (just for info) = 90.64 %
[162]	validation_0-merror:0.07613	validation_1-merror:0.093634	validation_0-target_score:-0.830023	validation_1-target_score:-0.745204
target score = 83.04 %
accuracy (just for info) = 92.41 %
target score = 74.61 %
accuracy (just for info) = 90.64 %
[163]	validation_0-merror:0.07585	validation_1-merror:0.093634	validation_0-target_score:-0.830412	validation_1-target_score:-0.746101
target score = 83.06 %
accuracy (just for info) = 92.44 %
target score = 74.44 %
accuracy (just for info) = 90.64 %
[164]	validation_0-merror:0.075655	validation_1-merror:0.093634	validation_0-target_score:-0.830635	validation_1-target_score:-0.744418
target score = 83.12 %
accuracy (just for info) = 92.47 %
target score = 74.58 %
accuracy (just for info) = 90.63 %
[165]	validation_0-merror:0.075431	validation_1-merror:0.093746	validation_0-target_score:-0.831191	validation_1-target_score:-0.745765
target score = 83.10 %
accuracy (just for info) = 92.47 %
target score = 74.54 %
accuracy (just for info) = 90.62 %
[166]	validation_0-merror:0.075291	validation_1-merror:0.093803	validation_0-target_score:-0.830969	validation_1-target_score:-0.745428
target score = 83.25 %
accuracy (just for info) = 92.48 %
target score = 74.58 %
accuracy (just for info) = 90.61 %
[167]	validation_0-merror:0.075235	validation_1-merror:0.093859	validation_0-target_score:-0.832527	validation_1-target_score:-0.745765
target score = 83.26 %
accuracy (just for info) = 92.50 %
target score = 74.50 %
accuracy (just for info) = 90.63 %
[168]	validation_0-merror:0.075068	validation_1-merror:0.09369	validation_0-target_score:-0.832582	validation_1-target_score:-0.744979
target score = 83.28 %
accuracy (just for info) = 92.50 %
target score = 74.43 %
accuracy (just for info) = 90.63 %
[169]	validation_0-merror:0.075012	validation_1-merror:0.093915	validation_0-target_score:-0.832805	validation_1-target_score:-0.744306
target score = 83.30 %
accuracy (just for info) = 92.52 %
target score = 74.48 %
accuracy (just for info) = 90.61 %
[170]	validation_0-merror:0.074872	validation_1-merror:0.093971	validation_0-target_score:-0.832972	validation_1-target_score:-0.744755
target score = 83.32 %
accuracy (just for info) = 92.55 %
target score = 74.49 %
accuracy (just for info) = 90.61 %
[171]	validation_0-merror:0.074649	validation_1-merror:0.093859	validation_0-target_score:-0.83325	validation_1-target_score:-0.744867
target score = 83.43 %
accuracy (just for info) = 92.55 %
target score = 74.49 %
accuracy (just for info) = 90.61 %
[172]	validation_0-merror:0.074649	validation_1-merror:0.093859	validation_0-target_score:-0.834251	validation_1-target_score:-0.744867
target score = 83.46 %
accuracy (just for info) = 92.57 %
target score = 74.58 %
accuracy (just for info) = 90.64 %
[173]	validation_0-merror:0.074313	validation_1-merror:0.093634	validation_0-target_score:-0.834585	validation_1-target_score:-0.745765
target score = 83.50 %
accuracy (just for info) = 92.60 %
target score = 74.60 %
accuracy (just for info) = 90.63 %
[174]	validation_0-merror:0.074062	validation_1-merror:0.09369	validation_0-target_score:-0.834975	validation_1-target_score:-0.745989
target score = 83.53 %
accuracy (just for info) = 92.60 %
target score = 74.61 %
accuracy (just for info) = 90.59 %
[175]	validation_0-merror:0.074006	validation_1-merror:0.09414	validation_0-target_score:-0.835309	validation_1-target_score:-0.746101
target score = 83.65 %
accuracy (just for info) = 92.63 %
target score = 74.51 %
accuracy (just for info) = 90.63 %
[176]	validation_0-merror:0.073726	validation_1-merror:0.09369	validation_0-target_score:-0.836477	validation_1-target_score:-0.745091
target score = 83.75 %
accuracy (just for info) = 92.66 %
target score = 74.49 %
accuracy (just for info) = 90.64 %
[177]	validation_0-merror:0.073503	validation_1-merror:0.093578	validation_0-target_score:-0.837478	validation_1-target_score:-0.744867
target score = 83.83 %
accuracy (just for info) = 92.67 %
target score = 74.51 %
accuracy (just for info) = 90.64 %
[178]	validation_0-merror:0.073335	validation_1-merror:0.093578	validation_0-target_score:-0.838313	validation_1-target_score:-0.745091
target score = 83.91 %
accuracy (just for info) = 92.68 %
target score = 74.52 %
accuracy (just for info) = 90.61 %
[179]	validation_0-merror:0.073251	validation_1-merror:0.093859	validation_0-target_score:-0.839092	validation_1-target_score:-0.745204
target score = 83.96 %
accuracy (just for info) = 92.69 %
target score = 74.45 %
accuracy (just for info) = 90.63 %
[180]	validation_0-merror:0.073167	validation_1-merror:0.09369	validation_0-target_score:-0.839648	validation_1-target_score:-0.74453
target score = 83.95 %
accuracy (just for info) = 92.69 %
target score = 74.48 %
accuracy (just for info) = 90.64 %
[181]	validation_0-merror:0.073167	validation_1-merror:0.093634	validation_0-target_score:-0.839537	validation_1-target_score:-0.744755
target score = 84.01 %
accuracy (just for info) = 92.73 %
target score = 74.42 %
accuracy (just for info) = 90.65 %
[182]	validation_0-merror:0.072692	validation_1-merror:0.093522	validation_0-target_score:-0.840093	validation_1-target_score:-0.744194
target score = 84.05 %
accuracy (just for info) = 92.73 %
target score = 74.39 %
accuracy (just for info) = 90.65 %
[183]	validation_0-merror:0.07272	validation_1-merror:0.093522	validation_0-target_score:-0.840539	validation_1-target_score:-0.743857
target score = 84.10 %
accuracy (just for info) = 92.76 %
target score = 74.40 %
accuracy (just for info) = 90.66 %
[184]	validation_0-merror:0.072413	validation_1-merror:0.093353	validation_0-target_score:-0.841039	validation_1-target_score:-0.743969
target score = 84.17 %
accuracy (just for info) = 92.78 %
target score = 74.43 %
accuracy (just for info) = 90.65 %
[185]	validation_0-merror:0.072189	validation_1-merror:0.093522	validation_0-target_score:-0.841651	validation_1-target_score:-0.744306
target score = 84.23 %
accuracy (just for info) = 92.81 %
target score = 74.40 %
accuracy (just for info) = 90.63 %
[186]	validation_0-merror:0.071966	validation_1-merror:0.09369	validation_0-target_score:-0.842319	validation_1-target_score:-0.743969
target score = 84.26 %
accuracy (just for info) = 92.83 %
target score = 74.31 %
accuracy (just for info) = 90.61 %
[187]	validation_0-merror:0.071826	validation_1-merror:0.093859	validation_0-target_score:-0.842597	validation_1-target_score:-0.743072
target score = 84.29 %
accuracy (just for info) = 92.85 %
target score = 74.58 %
accuracy (just for info) = 90.58 %
[188]	validation_0-merror:0.071574	validation_1-merror:0.094196	validation_0-target_score:-0.842875	validation_1-target_score:-0.745765
target score = 84.33 %
accuracy (just for info) = 92.87 %
target score = 74.53 %
accuracy (just for info) = 90.61 %
[189]	validation_0-merror:0.071407	validation_1-merror:0.093915	validation_0-target_score:-0.843321	validation_1-target_score:-0.745316
target score = 84.33 %
accuracy (just for info) = 92.88 %
target score = 74.49 %
accuracy (just for info) = 90.65 %
[190]	validation_0-merror:0.071351	validation_1-merror:0.093522	validation_0-target_score:-0.843265	validation_1-target_score:-0.744867
target score = 84.35 %
accuracy (just for info) = 92.90 %
target score = 74.52 %
accuracy (just for info) = 90.65 %
[191]	validation_0-merror:0.071183	validation_1-merror:0.093522	validation_0-target_score:-0.843543	validation_1-target_score:-0.745204
target score = 84.43 %
accuracy (just for info) = 92.91 %
target score = 74.43 %
accuracy (just for info) = 90.64 %
[192]	validation_0-merror:0.071015	validation_1-merror:0.093578	validation_0-target_score:-0.844266	validation_1-target_score:-0.744306
target score = 84.50 %
accuracy (just for info) = 92.93 %
target score = 74.31 %
accuracy (just for info) = 90.65 %
[193]	validation_0-merror:0.070792	validation_1-merror:0.093465	validation_0-target_score:-0.845045	validation_1-target_score:-0.743072
target score = 84.52 %
accuracy (just for info) = 92.95 %
target score = 74.32 %
accuracy (just for info) = 90.65 %
[194]	validation_0-merror:0.070624	validation_1-merror:0.093465	validation_0-target_score:-0.845157	validation_1-target_score:-0.743184
target score = 84.62 %
accuracy (just for info) = 92.98 %
target score = 74.40 %
accuracy (just for info) = 90.65 %
[195]	validation_0-merror:0.070261	validation_1-merror:0.093465	validation_0-target_score:-0.846158	validation_1-target_score:-0.743969
target score = 84.70 %
accuracy (just for info) = 92.99 %
target score = 74.39 %
accuracy (just for info) = 90.65 %
[196]	validation_0-merror:0.070205	validation_1-merror:0.093578	validation_0-target_score:-0.846993	validation_1-target_score:-0.743857
target score = 84.77 %
accuracy (just for info) = 93.03 %
target score = 74.58 %
accuracy (just for info) = 90.65 %
[197]	validation_0-merror:0.069786	validation_1-merror:0.093465	validation_0-target_score:-0.847716	validation_1-target_score:-0.745765
target score = 84.77 %
accuracy (just for info) = 93.02 %
target score = 74.50 %
accuracy (just for info) = 90.68 %
[198]	validation_0-merror:0.069814	validation_1-merror:0.09324	validation_0-target_score:-0.84766	validation_1-target_score:-0.744979
Stopping. Best iteration:
[148]	validation_0-merror:0.078897	validation_1-merror:0.094365	validation_0-target_score:-0.820342	validation_1-target_score:-0.74655
[0, 1, 2]
0.232348206764
