# Guidelines

 - while solving the task, you are free to ask the interviewer for any additional information or any specific data you need to solve the task via email kostiantyn.omelianchuk@grammarly.com; note that you will receive an answer during the working time only (10 a.m. - 7 p.m., Monday-Friday)
 - use any programming language, framework and approach you like

# Description

Train a model that detects and corrects an incorrect usage of the English article (“a”, “an” or “the”). You need to output the confidence of each correction. This way we can measure recall at a specific precision level by leaving only the most confident replacements.

# Input data

Each of the files in the lists below is optional. A good enough solution may use only two files out of these, so don't overthink the task!

The input data is split into three parts: train, test and private test. The data for each part include:
 - sentences: sentence_train.txt, sentence_test.txt, sentence_private_test.txt
 - corrections: corrections_train.txt, corrections_test.txt
 - constituency parse trees: parse_train.txt, parse_test.txt, parse_private_test.txt
 - dependency parse trees: dependencies_train.txt, dependencies_test.txt, dependencies_private_test.txt
 - part of speech tags: pos_tags_train.txt, pos_tags_test.txt, pos_tags_private_test.txt
 - GloVe vectors indexes for list of vectors in glove_vectors.txt file: glove_train.txt, glove_test.txt, glove_private_test.txt

The rest of the data is not split:
 - big collection of almost error-free texts (20M sentences): text.txt.gz
 - GloVe vectors - word embeddings built on a giant text corpus: glove_vectors.txt
 - idioms dictionary: idioms.txt
 - word n-grams frequencies (useful resource) collected on a giant text corpus: ngrams.txt
 - dependency n-grams frequencies collected on a giant text corpus: syntactic_ngrams.txt
 - singular nouns (note that nouns lists may intersect): singular_nouns.txt
 - plural nouns: plural_nouns.txt
 - uncountable nouns: uncountable_nouns.txt
 - transcriptions: transcriptions.txt
 - example solution in Python: sample_solution.py
 - example submission file for test data: submission_test.txt
 - a Python script for testing the performance of your model: evaluate.py

NOTE: n-grams are useful for this task

Feel free to ask for any consultation or additional data you might need.

All data is in the JSON format. You don't need to correct redundant or missing articles. Only confused articles should be corrected in this task.

# Output data

You need to run your model on “sentence_private_test.txt” and submit corrections. You need to output corrections in the same format as in ‘submission_test.txt’: a list of lists of corrections. Elements of the main list correspond to sentences. Elements of the inner lists correspond to words. If a word should not be corrected, the corresponding element should be null; otherwise, it should be a list consisting of the replacement and the confidence level (the higher, the more confident).

For example:  
`[[null, null, ["the", 0.55], null], [null, null, ["the", 100], null, ["a", 0.3], null, null]]`

# Evaluation

If the solution creates a mess of FPs, it's useless. That's why the target metric is not accuracy but recall level at a specified false positive rate level.

 - only article corrections are considered during the evaluation
 - all corrections are divided into four categories: TP, TN, FP and FN (see https://en.wikipedia.org/wiki/Precision_and_recall)
 - an incorrect replacement is considered as both FP and FN
 - only the most confident replacements are left, so that `FP / all_articles <= 0.02`. After that, the recall is measured: `TP / all_mistakes`
 - you can take a look at the sample submission data/submission_test.txt generated by the sample dummy solution sample_solution.py; its score is the baseline: 5.4%

If you wonder what score level is good enough, 50% is a rather good score. Best known current score is around 80%.
